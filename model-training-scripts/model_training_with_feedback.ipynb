{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed50409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "from __future__ import annotations\n",
    "import os, random, warnings, json\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463f3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∫¬†Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Paths & device\n",
    "SCRIPT_DIR = Path(os.getcwd()).resolve()\n",
    "ROOT       = SCRIPT_DIR / \"Data-REHAB24-6\"\n",
    "META_FILE  = ROOT / \"Segmentation.xlsx\"        # original Excel\n",
    "JOINT_FILE = ROOT / \"joints_names.txt\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"‚ñ∫¬†Using device:\", DEVICE)\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# joint names\n",
    "JOINT_NAMES = [\n",
    "    ln.split(\":\", 1)[1].strip()\n",
    "    for ln in JOINT_FILE.read_text().splitlines()\n",
    "]\n",
    "N_JOINTS = len(JOINT_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "068a19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Natural‚Äëlanguage feedback\n",
    "def english_feedback(err_vec: torch.Tensor | np.ndarray, tol: float = 5.0):\n",
    "    tips: list[str] = []\n",
    "    for j, deg in enumerate(err_vec):\n",
    "        deg = float(deg)\n",
    "        if abs(deg) <= tol:\n",
    "            continue\n",
    "        act = \"straighten\" if deg > 0 else \"bend\"\n",
    "        tips.append(f\"{act} your {JOINT_NAMES[j]} by ‚âà{abs(deg):.0f}¬∞\")\n",
    "    return tips or [\"Great form ‚úÖ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a911cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset\n",
    "class RehabDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str | Path,\n",
    "        meta: str | Path,\n",
    "        frames: int = 16,\n",
    "        camera: str = \"Camera17\",\n",
    "        transform: transforms.Compose | None = None,\n",
    "        split: str = \"train\"\n",
    "    ):\n",
    "        self.root   = Path(root)\n",
    "        self.frames = frames\n",
    "        self.camera = camera\n",
    "\n",
    "        # load metadata\n",
    "        ext = Path(meta).suffix.lower()\n",
    "        if ext in [\".xlsx\", \".xls\"]:\n",
    "            df = pd.read_excel(meta, engine=\"openpyxl\")\n",
    "        else:\n",
    "            df = pd.read_csv(meta, skipinitialspace=True, encoding=\"utf-8-sig\")\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # ensure err_0‚Ä¶err_25 exist (zero‚Äëfill if missing)\n",
    "        err_cols = [f\"err_{i}\" for i in range(N_JOINTS)]\n",
    "        if not all(c in df.columns for c in err_cols):\n",
    "            warnings.warn(\"err_0‚Ä¶err_25 not found in metadata ‚Üí auto‚Äëfilling zeros\")\n",
    "            for c in err_cols:\n",
    "                df[c] = 0.0\n",
    "        self.err_cols = err_cols\n",
    "\n",
    "        # subject‚Äëwise split\n",
    "        vids = sorted(df[\"video_id\"].unique())\n",
    "        random.shuffle(vids)\n",
    "        a, b = int(0.7 * len(vids)), int(0.85 * len(vids))\n",
    "        keep = vids[:a] if split==\"train\" else vids[a:b] if split==\"val\" else vids[b:]\n",
    "        self.meta = df[df.video_id.isin(keep)].reset_index(drop=True)\n",
    "\n",
    "        # transforms\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([.485,.456,.406],[.229,.224,.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.meta.iloc[idx]\n",
    "        mp4 = (\n",
    "            self.root / f\"videos/Ex{row.exercise_id}\"\n",
    "            / f\"{row.video_id}-Camera17-30fps.mp4\"\n",
    "        )\n",
    "        cap = cv2.VideoCapture(str(mp4))\n",
    "        if not cap.isOpened():\n",
    "            warnings.warn(f\"Cannot open {mp4}; skipping to next\")\n",
    "            return self.__getitem__((idx+1) % len(self))\n",
    "\n",
    "        tot = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames = torch.linspace(row.first_frame, min(row.last_frame, tot-1), self.frames).long()\n",
    "        imgs = []\n",
    "        for f in frames:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(f))\n",
    "            ok, img = cap.read()\n",
    "            if not ok:\n",
    "                warnings.warn(f\"Bad frame {f} in {mp4}; skipping\")\n",
    "                return self.__getitem__((idx+1) % len(self))\n",
    "            imgs.append(self.transform(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "        cap.release()\n",
    "\n",
    "        clip  = torch.stack(imgs)  # (T,C,H,W)\n",
    "        label = torch.tensor(row.correctness, dtype=torch.long)\n",
    "\n",
    "        # ‚ùóÔ∏è Force float32 array to avoid object dtype\n",
    "        err_vals = row[self.err_cols].to_numpy(dtype=np.float32)\n",
    "        err      = torch.tensor(err_vals, dtype=torch.float32)\n",
    "\n",
    "        return clip, label, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a49b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Model\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed: int = 512):\n",
    "        super().__init__()\n",
    "        base = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.proj     = nn.Linear(512, embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "        feats = self.backbone(x.view(B*T, *x.shape[2:])).flatten(1)\n",
    "        return self.proj(feats).view(B, T, -1)\n",
    "\n",
    "class PoseQualityNet(nn.Module):\n",
    "    def __init__(self, embed: int = 512, hidden: int = 256):\n",
    "        super().__init__()\n",
    "        self.cnn   = CNNEncoder(embed)\n",
    "        self.lstm  = nn.LSTM(embed, hidden, 2, batch_first=True, bidirectional=True)\n",
    "        dim = hidden * 2\n",
    "        self.cls_head = nn.Linear(dim, 2)\n",
    "        self.err_head = nn.Linear(dim, N_JOINTS)\n",
    "\n",
    "    def forward(self, clip):\n",
    "        feats, _ = self.lstm(self.cnn(clip))\n",
    "        g = feats.mean(1)\n",
    "        return self.cls_head(g), self.err_head(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa0f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training routine\n",
    "def train_epochs(\n",
    "    epochs: int = 30,\n",
    "    batch: int = 4,\n",
    "    lr: float = 1e-4,\n",
    "    ckpt_file: str | Path = \"pose_quality_best.pt\"\n",
    "):\n",
    "    train_ds = RehabDataset(ROOT, META_FILE, split=\"train\")\n",
    "    val_ds   = RehabDataset(ROOT, META_FILE, split=\"val\")\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=batch, shuffle=False)\n",
    "\n",
    "    model    = PoseQualityNet().to(DEVICE)\n",
    "    loss_cls = nn.CrossEntropyLoss()\n",
    "    loss_err = nn.SmoothL1Loss()\n",
    "    opt      = Adam(model.parameters(), lr)\n",
    "    best_f1  = 0.0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for clip, y, err in tqdm(train_dl, desc=f\"Epoch {epoch:02d}\"):\n",
    "            clip, y, err = clip.to(DEVICE), y.to(DEVICE), err.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits, err_hat = model(clip)\n",
    "            loss = loss_cls(logits, y) + 0.1 * loss_err(err_hat, err)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * y.size(0)\n",
    "        print(f\"  ‚Ü≥ train loss: {total_loss/len(train_ds):.4f}\")\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        y_true, y_pred, errs = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for clip, y, err in val_dl:\n",
    "                logits, err_hat = model(clip.to(DEVICE))\n",
    "                y_true += y.tolist()\n",
    "                y_pred += logits.argmax(1).cpu().tolist()\n",
    "                errs    += [torch.abs(err_hat.cpu() - err).mean(1)]\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1  = f1_score(y_true, y_pred)\n",
    "        mae = torch.cat(errs).mean().item()\n",
    "        print(f\"  ‚Ü≥ val acc {acc:.3f}, F1 {f1:.3f}, MAE¬∞ {mae:.2f}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save({\"state\": model.state_dict()}, ckpt_file)\n",
    "            print(\"  ‚úì saved new best model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c78881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_17229/2280425479.py:27: UserWarning: err_0‚Ä¶err_25 not found in metadata ‚Üí auto‚Äëfilling zeros\n",
      "  warnings.warn(\"err_0‚Ä¶err_25 not found in metadata ‚Üí auto‚Äëfilling zeros\")\n",
      "/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_17229/2280425479.py:27: UserWarning: err_0‚Ä¶err_25 not found in metadata ‚Üí auto‚Äëfilling zeros\n",
      "  warnings.warn(\"err_0‚Ä¶err_25 not found in metadata ‚Üí auto‚Äëfilling zeros\")\n",
      "Epoch 01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [14:03<00:00, 18.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Ü≥ train loss: 0.6563\n",
      "  ‚Ü≥ val acc 0.651, F1 0.538, MAE¬∞ 0.02\n",
      "  ‚úì saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [14:46<00:00, 19.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Ü≥ train loss: 0.4827\n",
      "  ‚Ü≥ val acc 0.709, F1 0.769, MAE¬∞ 0.01\n",
      "  ‚úì saved new best model\n"
     ]
    }
   ],
   "source": [
    "# 6. run training\n",
    "train_epochs(epochs=2, batch=16, lr=1e-4, ckpt_file=\"pose_quality_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27cd7748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_17229/2365228642.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(model_pt, map_location=DEVICE)\n",
      "OpenCV: Couldn't read video stream from file \"Data-REHAB24-6/videos/Ex1/sample.mp4\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot open Data-REHAB24-6/videos/Ex1/sample.mp4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# run inference\u001b[39;00m\n\u001b[32m     55\u001b[39m clip_path = Path(\u001b[33m\"\u001b[39m\u001b[33mData-REHAB24-6/videos/Ex1/sample.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m out = \u001b[43mpredict_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_pt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpose_quality_best.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(out, indent=\u001b[32m2\u001b[39m))\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Correctness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout[\u001b[33m'\u001b[39m\u001b[33mcorrectness_prob\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mpredict_clip\u001b[39m\u001b[34m(clip_path, model_pt, frames)\u001b[39m\n\u001b[32m     27\u001b[39m cap = cv2.VideoCapture(\u001b[38;5;28mstr\u001b[39m(clip_path))\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap.isOpened():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot open \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclip_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m tot = \u001b[38;5;28mint\u001b[39m(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\u001b[32m     31\u001b[39m idxs = torch.linspace(\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m(tot-\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m), frames).long()\n",
      "\u001b[31mOSError\u001b[39m: Cannot open Data-REHAB24-6/videos/Ex1/sample.mp4"
     ]
    }
   ],
   "source": [
    "# 7. Inference & Feedback \n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "# make sure PoseQualityNet & english_feedback are still in scope from Cell¬†1\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_clip(\n",
    "    clip_path: str | Path,\n",
    "    model_pt: str | Path = \"pose_quality_best.pt\",\n",
    "    frames: int = 16\n",
    "):\n",
    "    ck = torch.load(model_pt, map_location=DEVICE)\n",
    "    net = PoseQualityNet().to(DEVICE).eval()\n",
    "    net.load_state_dict(ck[\"state\"])\n",
    "\n",
    "    cap = cv2.VideoCapture(str(clip_path))\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open {clip_path}\")\n",
    "    tot = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    idxs = torch.linspace(0, max(tot-1,0), frames).long()\n",
    "\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([.485,.456,.406],[.229,.224,.225])\n",
    "    ])\n",
    "    frames_tensor = []\n",
    "    for f in idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(f))\n",
    "        ok, img = cap.read()\n",
    "        if not ok:\n",
    "            raise RuntimeError(f\"Bad frame {f}\")\n",
    "        frames_tensor.append(tfm(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "    cap.release()\n",
    "\n",
    "    clip = torch.stack(frames_tensor).unsqueeze(0).to(DEVICE)\n",
    "    logits, err_hat = net(clip)\n",
    "    prob = torch.softmax(logits,1)[0,1].item()\n",
    "    tips = english_feedback(err_hat.squeeze().cpu())\n",
    "    return {\"correctness_prob\": prob, \"feedback\": tips}\n",
    "\n",
    "# run inference\n",
    "clip_path = Path(\"Data-REHAB24-6/videos/Ex1/sample.mp4\")\n",
    "out = predict_clip(clip_path, model_pt=\"pose_quality_best.pt\", frames=16)\n",
    "\n",
    "print(json.dumps(out, indent=2))\n",
    "print(f\"\\n‚úÖ Correctness: {out['correctness_prob']:.2%}\")\n",
    "print(\"üìù Feedback:\")\n",
    "for t in out[\"feedback\"]:\n",
    "    print(\"  -\", t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rehabtrainingpy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
