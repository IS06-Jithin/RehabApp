{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d02677",
   "metadata": {},
   "source": [
    "# 🧍‍♂️ Pose Correction System for Physical Therapy and Rehabilitation Using Computer Vision\n",
    "\n",
    "**Author**: *Jithin Krishnan (A0249481W)*  \n",
    "**Institution**: *NUS-ISS*\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Requirements\n",
    "\n",
    "- **Python Version**: 3.12  \n",
    "- **Dependencies**: All required packages are listed in the `requirements.txt` file located in the `model-training-and-evaluation-scripts` directory.\n",
    "\n",
    "To install them, open your terminal, navigate to the specified folder, and run:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab585d8f",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a70e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset \n",
    "import argparse, json, sys, time\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns \n",
    "from tabulate import tabulate\n",
    "import threading\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cea14",
   "metadata": {},
   "source": [
    "# 2. Original Dataset Analysis\n",
    "\n",
    "This step performs a summary analysis of the original REHAB24-6 dataset (`Segmentation_original.xlsx`) before applying any custom augmentations or modifications.  \n",
    "\n",
    "For each exercise, it calculates:\n",
    "- The number of repetitions\n",
    "- The count of correct and incorrect labels\n",
    "- Total frames and approximate duration (in seconds)\n",
    "- Number of unique camera orientations\n",
    "\n",
    "A final \"Total\" row aggregates these values across all exercises for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2322a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data-REHAB24-6_New/Segmentation_original.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m FPS = \u001b[32m30\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Read Excel file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Calculate the frame difference for each repetition (row)\u001b[39;00m\n\u001b[32m      9\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mframe_diff\u001b[39m\u001b[33m\"\u001b[39m] = (df[\u001b[33m\"\u001b[39m\u001b[33mlast_frame\u001b[39m\u001b[33m\"\u001b[39m] - df[\u001b[33m\"\u001b[39m\u001b[33mfirst_frame\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/pandas/io/excel/_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Data-REHAB24-6_New/Segmentation_original.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT = Path(\"Data-REHAB24-6_New\")           \n",
    "segmentation = ROOT / \"Segmentation_original.xlsx\"  # Excel file from authors\n",
    "FPS = 30\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(segmentation)\n",
    "\n",
    "# Calculate the frame difference for each repetition (row)\n",
    "df[\"frame_diff\"] = (df[\"last_frame\"] - df[\"first_frame\"] + 1)\n",
    "\n",
    "# One row = one repetition\n",
    "summary = (\n",
    "    df.groupby(\"exercise_id\")\n",
    "      .agg(\n",
    "          Reps=(\"exercise_id\", \"size\"),\n",
    "          Correct=(\"correctness\", lambda x: (x == 1).sum()),\n",
    "          Wrong=(\"correctness\", lambda x: (x == 0).sum()),\n",
    "          Frames=(\"frame_diff\", \"sum\"),  # Sum the precomputed frame differences\n",
    "          Dir=(\"cam17_orientation\", \"nunique\")  # Already correct\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate totals\n",
    "total_row = pd.DataFrame({\n",
    "    \"exercise_id\": [\"Total\"],\n",
    "    \"Reps\": [summary[\"Reps\"].sum()],\n",
    "    \"Correct\": [summary[\"Correct\"].sum()],\n",
    "    \"Wrong\": [summary[\"Wrong\"].sum()],\n",
    "    \"Frames\": [summary[\"Frames\"].sum()],\n",
    "    \"Dir\": [\"–\"],  # Direction doesn't apply to total\n",
    "    \"Seconds\": [\"–\"]  # Seconds doesn't apply to total\n",
    "})\n",
    "\n",
    "# Add Seconds column to the summary (before appending the total row)\n",
    "summary[\"Seconds\"] = (summary.Frames / FPS).round(1)\n",
    "\n",
    "# Append the total row to the summary DataFrame\n",
    "summary = pd.concat([summary, total_row], ignore_index=True)\n",
    "\n",
    "# Pretty-print\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc37936",
   "metadata": {},
   "source": [
    "The table presents a summary of exercise performance metrics from the REHAB24-6 dataset, which includes 1,072 repetitions of six physical rehabilitation exercises recorded at 30 FPS across 65 videos, focusing on Camera 17 data. The results are grouped by exercise_id (1 to 6, corresponding to Arm Abduction, Arm VW, Push-ups, Leg Abduction, Leg Lunge, and Squats, respectively). Exercise 1 (Arm Abduction) has 178 repetitions, with 90 correct and 88 incorrect, totaling 27,442 frames (914.7 seconds) and performed in 2 directions. Exercise 2 (Arm VW) shows 208 repetitions, with 94 correct and 114 incorrect, spanning 33,641 frames (1,121.4 seconds) across 2 directions. Exercise 3 (Push-ups) has 107 repetitions, with 52 correct and 55 incorrect, covering 12,054 frames (401.8 seconds) in 1 direction, reflecting its table-based setup. Exercise 4 (Leg Abduction) includes 210 repetitions, with 120 correct and 90 incorrect, totaling 18,329 frames (611.0 seconds) across 2 directions. Exercise 5 (Leg Lunge) has 174 repetitions, with 78 correct and 96 incorrect, spanning 17,608 frames (586.9 seconds) in 2 directions. Finally, Exercise 6 (Squats) shows 195 repetitions, with 134 correct and 61 incorrect, covering 19,373 frames (645.8 seconds) across 2 directions. Overall, the dataset totals 1,072 repetitions, with 568 correct and 504 incorrect, amounting to 128,447 frames, with direction and seconds not aggregated for the total. These results highlight the dataset's diversity in exercise types, correctness, and directional views, supporting its use for evaluating human pose estimation and exercise feedback systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0b559",
   "metadata": {},
   "source": [
    "# 3. Keypoint Extraction from Training Videos with MediaPipe  \n",
    "Before training a model for tasks like pose estimation, exercise recognition, or movement analysis, the first step is to extract relevant features from the data. In this case, the data consists of training videos, and the relevant features are pose keypoints—3D coordinates representing specific body parts (like shoulders, elbows, knees, etc.).  \n",
    "\n",
    "This code uses MediaPipe Pose to extract keypoints from training videos. These keypoints are critical for model training as they represent the underlying body movements that the model will need to learn. Below is a breakdown of the process:\n",
    "\n",
    "<div style=\"color: red; font-weight: bold;\">\n",
    "⚠️ Warning: This process may take several hours to complete. Uncomment and run the code only if you wish to perform the segmentation again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae42d0f",
   "metadata": {},
   "source": [
    "## 3.1 Keypoint extraction and saving as numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keypoint extraction from Training Videos With Mediapipe\n",
    "\n",
    "# # 0. Quiet TensorFlow/absl\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# # 1. MediaPipe Pose setup\n",
    "# pose = mp.solutions.pose.Pose(\n",
    "#     static_image_mode=False,\n",
    "#     model_complexity=2,\n",
    "#     enable_segmentation=False,\n",
    "#     min_detection_confidence=0.5,\n",
    "#     min_tracking_confidence=0.5\n",
    "# )\n",
    "\n",
    "# # 2. Paths\n",
    "# VIDEO_ROOT = Path(\"Data-REHAB24-6_New/videos\")\n",
    "# OUT_ROOT   = Path(\"Data-REHAB24-6_New/mp_keypoints\")\n",
    "# OUT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# # 3. Worker\n",
    "# def process_video(vid_path: Path):\n",
    "#     rel     = vid_path.parent.name            # e.g. \"Ex1\"\n",
    "#     out_dir = OUT_ROOT / rel\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     out_file = out_dir / f\"{vid_path.stem}-mp.npy\"\n",
    "\n",
    "#     print(f\"\\n→ Processing: {vid_path.name}\")\n",
    "#     print(f\"   From:      {vid_path.parent}\")\n",
    "#     print(f\"   To folder: {out_dir}\")\n",
    "\n",
    "#     cap    = cv2.VideoCapture(str(vid_path))\n",
    "#     frames = []\n",
    "#     count  = 0\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         count += 1\n",
    "\n",
    "#         img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         res = pose.process(img)\n",
    "#         lm  = res.pose_world_landmarks.landmark if res.pose_world_landmarks else []\n",
    "\n",
    "#         if lm:\n",
    "#             pts = [(p.x, p.y, p.z) for p in lm]\n",
    "#         else:\n",
    "#             pts = [(0.0, 0.0, 0.0)] * 33\n",
    "\n",
    "#         frames.append(pts)\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "#     arr = np.array(frames, dtype=np.float32)\n",
    "#     np.save(out_file, arr)\n",
    "\n",
    "#     print(f\"✔ Saved: {out_file}  (frames={count}, shape={arr.shape})\")\n",
    "\n",
    "# # 4. Run — only Ex1 through Ex5\n",
    "# for i in range(1, 6):\n",
    "#     ex_dir = VIDEO_ROOT / f\"Ex{i}\"\n",
    "#     if not ex_dir.is_dir():\n",
    "#         print(f\"⚠️  Skipping missing folder: {ex_dir}\")\n",
    "#         continue\n",
    "\n",
    "#     for vid in sorted(ex_dir.glob(\"*.mp4\")):\n",
    "#         try:\n",
    "#             process_video(vid)\n",
    "#         except Exception as e:\n",
    "#             print(f\"✘ Failed processing {vid.name}: {e}\")\n",
    "\n",
    "# print(\"\\nAll requested videos processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390d6ed",
   "metadata": {},
   "source": [
    "## 3.2 Inspecting at a numpy file containing 3D keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86099ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Data-REHAB24-6_New/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\n",
      " dtype: float32\n",
      " shape: (5191, 33, 3)  (frames × landmarks × coords)\n",
      "\n",
      "Frame #000 (33×3):\n",
      "[[-0.03379065 -0.6112996  -0.22619084]\n",
      " [-0.02455677 -0.628223   -0.2275483 ]\n",
      " [-0.0257254  -0.6304051  -0.21732828]\n",
      " [-0.02547118 -0.6302204  -0.21820049]\n",
      " [-0.02778288 -0.6363151  -0.2358914 ]\n",
      " [-0.02677054 -0.63463634 -0.24706481]\n",
      " [-0.02263773 -0.61979294 -0.2281486 ]\n",
      " [ 0.02648694 -0.6184615  -0.16859515]\n",
      " [-0.03559308 -0.56201506 -0.14808732]\n",
      " [ 0.0030987  -0.59678274 -0.19447449]\n",
      " [-0.01712019 -0.55977213 -0.21580447]\n",
      " [ 0.1250833  -0.49333623 -0.08702794]\n",
      " [-0.05722423 -0.53108674 -0.02576461]\n",
      " [ 0.15178505 -0.51440114 -0.09251688]\n",
      " [-0.17247145 -0.4994753  -0.05377672]\n",
      " [ 0.14430666 -0.54461473 -0.03090633]\n",
      " [-0.32217076 -0.5845331  -0.08351779]\n",
      " [ 0.1352469  -0.5789426  -0.02217976]\n",
      " [-0.33411983 -0.62724733 -0.13406767]\n",
      " [ 0.11857966 -0.58959824 -0.04421883]\n",
      " [-0.30619952 -0.65674794 -0.14033641]\n",
      " [ 0.13962431 -0.533594   -0.04002995]\n",
      " [-0.32007053 -0.60172904 -0.09692235]\n",
      " [ 0.05049127 -0.0018556   0.00790542]\n",
      " [-0.0485956  -0.00113658 -0.00433628]\n",
      " [ 0.11502459  0.2840918   0.04747445]\n",
      " [-0.05322852  0.24029574  0.08932951]\n",
      " [ 0.14734772  0.5199347   0.23229995]\n",
      " [-0.07118332  0.5179724   0.2826942 ]\n",
      " [ 0.13367084  0.54242045  0.2601168 ]\n",
      " [-0.07182921  0.541233    0.24959905]\n",
      " [ 0.12196927  0.57154816  0.2570243 ]\n",
      " [-0.07216683  0.572454    0.23920183]]\n",
      "  → first landmark: (-0.033790648, -0.6112996, -0.22619084)\n",
      "\n",
      "Frame #001 (33×3):\n",
      "[[-0.01134037 -0.5697082  -0.339561  ]\n",
      " [ 0.00353809 -0.5859319  -0.34309617]\n",
      " [ 0.00270209 -0.5884564  -0.33378896]\n",
      " [ 0.00201175 -0.5885314  -0.33455902]\n",
      " [-0.01271034 -0.59487945 -0.34993634]\n",
      " [-0.01165643 -0.593975   -0.36170712]\n",
      " [-0.01075809 -0.57910186 -0.3406161 ]\n",
      " [ 0.07479228 -0.5783957  -0.28210232]\n",
      " [-0.02705817 -0.537724   -0.2474346 ]\n",
      " [ 0.03136193 -0.5524706  -0.30781525]\n",
      " [-0.00508686 -0.5230155  -0.32598764]\n",
      " [ 0.1856992  -0.48319736 -0.18188453]\n",
      " [-0.05337023 -0.5305265  -0.10725395]\n",
      " [ 0.30533957 -0.5076612  -0.24926643]\n",
      " [-0.18747927 -0.47846928 -0.1686121 ]\n",
      " [ 0.40025973 -0.51844925 -0.28818202]\n",
      " [-0.3567456  -0.5400336  -0.23759778]\n",
      " [ 0.4252214  -0.5436983  -0.29650414]\n",
      " [-0.37466687 -0.5707025  -0.28731832]\n",
      " [ 0.4044413  -0.5539673  -0.32024327]\n",
      " [-0.34815052 -0.60103    -0.30733863]\n",
      " [ 0.39781106 -0.5031106  -0.30399245]\n",
      " [-0.35498336 -0.55496377 -0.2580629 ]\n",
      " [ 0.07222402 -0.00160784  0.0128601 ]\n",
      " [-0.06938923 -0.00155623 -0.00999647]\n",
      " [ 0.08928943  0.31117028  0.06024915]\n",
      " [-0.10052951  0.25218678  0.05140337]\n",
      " [ 0.1335875   0.57052726  0.22892243]\n",
      " [-0.15915288  0.5619467   0.21901147]\n",
      " [ 0.12346359  0.590925    0.25226566]\n",
      " [-0.16072083  0.5869104   0.19796352]\n",
      " [ 0.13690576  0.60883754  0.23850642]\n",
      " [-0.17555799  0.603654    0.1471466 ]]\n",
      "  → first landmark: (-0.0113403695, -0.5697082, -0.339561)\n",
      "\n",
      "Frame #002 (33×3):\n",
      "[[-0.0168165  -0.57375824 -0.36536366]\n",
      " [-0.00349796 -0.58968115 -0.36685658]\n",
      " [-0.00404231 -0.5920712  -0.3569905 ]\n",
      " [-0.00467073 -0.5923622  -0.35750297]\n",
      " [-0.01410367 -0.5981765  -0.3782469 ]\n",
      " [-0.01414973 -0.5977909  -0.3914376 ]\n",
      " [-0.01312186 -0.5821221  -0.37337747]\n",
      " [ 0.05679294 -0.572806   -0.29834253]\n",
      " [-0.0476275  -0.5310251  -0.2819756 ]\n",
      " [ 0.02337127 -0.55599713 -0.3263316 ]\n",
      " [-0.0054874  -0.5257816  -0.3534069 ]\n",
      " [ 0.17624158 -0.4816474  -0.20698948]\n",
      " [-0.09889796 -0.52954847 -0.11612497]\n",
      " [ 0.32004458 -0.51815015 -0.27747568]\n",
      " [-0.2538115  -0.42914388 -0.22348595]\n",
      " [ 0.479071   -0.5206693  -0.31443927]\n",
      " [-0.4472741  -0.45803884 -0.3754344 ]\n",
      " [ 0.5139305  -0.5360094  -0.3153151 ]\n",
      " [-0.47330606 -0.47658482 -0.43610725]\n",
      " [ 0.490114   -0.54833627 -0.3450243 ]\n",
      " [-0.44490942 -0.51193917 -0.46239042]\n",
      " [ 0.4791889  -0.50126463 -0.33063355]\n",
      " [-0.44496435 -0.465993   -0.40077347]\n",
      " [ 0.08651342 -0.00139352  0.02232434]\n",
      " [-0.08341713 -0.00188619 -0.01804559]\n",
      " [ 0.14964634  0.36578354  0.05097246]\n",
      " [-0.11824293  0.26294428 -0.02509112]\n",
      " [ 0.20363103  0.61495334  0.19673198]\n",
      " [-0.19085191  0.5981499   0.14407939]\n",
      " [ 0.19493678  0.6042932   0.21519588]\n",
      " [-0.19582     0.62702686  0.12640719]\n",
      " [ 0.23405242  0.69470036  0.18596604]\n",
      " [-0.22132283  0.64795923  0.03823205]]\n",
      "  → first landmark: (-0.0168165, -0.57375824, -0.36536366)\n",
      "\n",
      "Overall coordinate stats:\n",
      "  x: min=-0.633, max=0.785, mean=0.061\n",
      "  y: min=-0.686, max=0.848, mean=-0.109\n",
      "  z: min=-0.742, max=0.672, mean=-0.146\n"
     ]
    }
   ],
   "source": [
    "# ─── edit this to your target file ────────────────────────────────────────────\n",
    "file_path = Path(\"Data-REHAB24-6_New/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# load\n",
    "arr = np.load(file_path)\n",
    "\n",
    "# basic info\n",
    "print(f\"Loaded: {file_path}\")\n",
    "print(f\" dtype: {arr.dtype}\")\n",
    "print(f\" shape: {arr.shape}  (frames × landmarks × coords)\\n\")\n",
    "\n",
    "# show first few frames\n",
    "n_show = min(3, arr.shape[0])\n",
    "for i in range(n_show):\n",
    "    print(f\"Frame #{i:03d} (33×3):\")\n",
    "    print(arr[i])\n",
    "    print(f\"  → first landmark: {tuple(arr[i,0])}\\n\")\n",
    "\n",
    "# overall statistics\n",
    "print(\"Overall coordinate stats:\")\n",
    "for idx, name in enumerate((\"x\", \"y\", \"z\")):\n",
    "    data = arr[..., idx]\n",
    "    print(f\"  {name}: min={data.min():.3f}, max={data.max():.3f}, mean={data.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a70d4",
   "metadata": {},
   "source": [
    "# 4. Data Preparation for Pose Error Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522482d6",
   "metadata": {},
   "source": [
    "## 4.1 Change the correctness of half profile vidoes to incorrect \n",
    "\n",
    "This change is being made because the half-profile camera angle is considered insufficient or unreliable for accurately determining if the exercise repetition was performed correctly according to the established standards. Furthermore, for model inference, the intention is to use only the front-facing view. Therefore, this action serves as a data cleaning and preparation step to improve the quality and consistency of the data that will be used for subsequent analysis or modeling, ensuring that only reliably assessed repetitions are marked as 'correct'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac9350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original file: Data-REHAB24-6_New/Segmentation_original.xlsx\n",
      "Original file loaded successfully.\n",
      "Original shape: (1072, 13)\n",
      "Found 466 rows where 'cam17_orientation' is 'half-profile'.\n",
      "Updating 'correctness' column to 0 for matched rows...\n",
      "Update complete.\n",
      "Saving modified data to: Data-REHAB24-6_New/Segmentation_new.xlsx\n",
      "Successfully saved modified data to '/Users/jithinkrishnan/Documents/Study/IS06 /MVP/RehabApp/model-training-scripts/Data-REHAB24-6_New/Segmentation_new.xlsx'\n",
      "\n",
      "Verifying the saved file...\n",
      "Loaded new file shape: (1072, 13)\n",
      "Verification successful: All 'half-profile' rows have 'correctness' set to 0.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATA_ROOT = Path(\"Data-REHAB24-6_New\") # Adjust if your data folder has a different relative path\n",
    "ORIGINAL_FILENAME = \"Segmentation_original.xlsx\"\n",
    "# --- CHANGE HERE: Update the output filename extension ---\n",
    "NEW_FILENAME = \"Segmentation_new.xlsx\" # Saving as .xlsx\n",
    "\n",
    "FILE_ORIG = DATA_ROOT / ORIGINAL_FILENAME\n",
    "FILE_DEST = DATA_ROOT / NEW_FILENAME\n",
    "\n",
    "# --- Processing ---\n",
    "if not DATA_ROOT.is_dir():\n",
    "    print(f\"Error: Data directory not found at '{DATA_ROOT.resolve()}'\")\n",
    "elif not FILE_ORIG.is_file():\n",
    "    print(f\"Error: Original file not found at '{FILE_ORIG.resolve()}'\")\n",
    "else:\n",
    "    print(f\"Loading original file: {FILE_ORIG}\")\n",
    "    try:\n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(FILE_ORIG)\n",
    "        print(\"Original file loaded successfully.\")\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "        # Identify rows where 'cam17_orientation' is 'half-profile'\n",
    "        condition = df['cam17_orientation'] == 'half-profile'\n",
    "        num_rows_to_change = condition.sum()\n",
    "        print(f\"Found {num_rows_to_change} rows where 'cam17_orientation' is 'half-profile'.\")\n",
    "\n",
    "        if num_rows_to_change > 0:\n",
    "            # Change 'correctness' to 0 for matched rows\n",
    "            print(\"Updating 'correctness' column to 0 for matched rows...\")\n",
    "            df.loc[condition, 'correctness'] = 0\n",
    "            print(\"Update complete.\")\n",
    "        else:\n",
    "            print(\"No rows matched the condition. 'correctness' column remains unchanged.\")\n",
    "\n",
    "        # Save the modified DataFrame to a new Excel file (.xlsx)\n",
    "        print(f\"Saving modified data to: {FILE_DEST}\")\n",
    "        try:\n",
    "            # Saving to .xlsx uses openpyxl engine by default (pip install openpyxl if needed)\n",
    "            df.to_excel(FILE_DEST, index=False)\n",
    "            print(f\"Successfully saved modified data to '{FILE_DEST.resolve()}'\")\n",
    "        except Exception as save_error:\n",
    "            # --- CHANGE HERE: Updated error message for .xlsx ---\n",
    "            print(f\"Error saving file to {FILE_DEST}: {save_error}\")\n",
    "            print(\"Saving to .xlsx format typically requires the 'openpyxl' package. Try: pip install openpyxl\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Error: Make sure the file exists at {FILE_ORIG.resolve()}\")\n",
    "    except KeyError as e:\n",
    "         print(f\"Error: Column not found - {e}. Please check column names in '{ORIGINAL_FILENAME}'.\")\n",
    "         print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during processing: {e}\")\n",
    "\n",
    "# --- Verification ---\n",
    "if FILE_DEST.is_file():\n",
    "    print(\"\\nVerifying the saved file...\")\n",
    "    try:\n",
    "        # --- CHANGE HERE: Reading the .xlsx file for verification ---\n",
    "        df_new = pd.read_excel(FILE_DEST)\n",
    "        print(f\"Loaded new file shape: {df_new.shape}\")\n",
    "        check_condition = df_new['cam17_orientation'] == 'half-profile'\n",
    "        incorrect_rows = df_new.loc[check_condition & (df_new['correctness'] != 0)]\n",
    "        if incorrect_rows.empty:\n",
    "            print(\"Verification successful: All 'half-profile' rows have 'correctness' set to 0.\")\n",
    "        else:\n",
    "            print(\"Verification FAILED: Some 'half-profile' rows still have 'correctness' != 0.\")\n",
    "            print(incorrect_rows)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "else:\n",
    "     print(f\"\\nCould not verify as the destination file '{FILE_DEST}' was not found or not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed5455",
   "metadata": {},
   "source": [
    "## 4.2 Sliding Window-Based Pose Error Calculation for Video segments  \n",
    "This script processes keypoint data from videos to compute errors in body joint angles relative to \"ideal\" angles, and then generates sliding windows of these errors. This is done to prepare features that will later be used for model training in tasks like exercise recognition or pose correction.\n",
    "\n",
    "What is done: For each exercise in the dataset, the script calculates the \"ideal\" joint angles by selecting the middle frame of each correct repetition (based on the correctness label). It calculates the angles between the joints defined in JOINT_TRIPLETS for each of these frames.  We use 17 out of the 33 MediaPipe landmarks for the ideal joint angle calculation. The used mediapipe landmarks are: NOSE, LEFT_SHOULDER, RIGHT_SHOULDER, LEFT_ELBOW, RIGHT_ELBOW, LEFT_WRIST, RIGHT_WRIST, LEFT_INDEX, RIGHT_INDEX, LEFT_HIP, RIGHT_HIP, LEFT_KNEE, RIGHT_KNEE, LEFT_ANKLE, RIGHT_ANKLE, LEFT_FOOT_INDEX, and RIGHT_FOOT_INDEX. These cover the major joints and end‑effectors (shoulders through wrists and hips through ankles, plus the spine/head via the nose) needed to compute all our angle‑based error metrics for the six rehab exercises. The 16 unused landmarks are all the fine‑grain facial points (inner/outer eyes, ears, mouth corners), the pinky and thumb tips, and the heel points. Since our focus is on gross limb alignment (arm and leg joint planes) rather than facial expression, finger articulation, or detailed foot posture, those landmarks don’t contribute to correcting the targeted movements and so are omitted.\n",
    "\n",
    "The median angle is then computed for each joint across the correct repetitions of the exercise. These median values represent the \"ideal\" angles the model should aim for in perfect executions of the exercise.  \n",
    "\n",
    "In the next step, the script generates sliding windows of angular errors, calculated as the difference between the observed angles in the video and the ideal angles. These windows contain temporal sequences of error data that are used as features for model training. The windowed data is then saved in a CSV file, which includes additional metadata such as exercise ID, repetition number, and frame indices.  \n",
    "\n",
    "Why it’s done: Calculating the ideal angles for each exercise provides a reference for identifying errors during subsequent video frames. These ideal angles will serve as the baseline for assessing whether a movement is performed correctly or incorrectly. By generating and saving the sliding window data, the script prepares the dataset for supervised learning, allowing the model to analyze temporal error patterns over a series of frames. This windowed approach is crucial for the model to learn dynamic movements and classify whether exercises are performed correctly based on the computed joint angles. The segmentation data in the CSV file offers a structured and labeled dataset, which helps in efficient training and evaluation of the model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa0f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 14375 windows to Segmentation_windows.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. helpers --------------------------------------------------\n",
    "def angle_between(a,b,c):\n",
    "    BA = a-b; BC = c-b\n",
    "    cosθ = np.dot(BA,BC)/(np.linalg.norm(BA)*np.linalg.norm(BC))\n",
    "    return math.degrees(math.acos(np.clip(cosθ,-1,1)))\n",
    "\n",
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "JOINT_TRIPLETS = {\n",
    "    \"LEFT_ELBOW\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "                     PoseLandmark.LEFT_ELBOW.value,\n",
    "                     PoseLandmark.LEFT_WRIST.value),\n",
    "    \"RIGHT_ELBOW\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                     PoseLandmark.RIGHT_ELBOW.value,\n",
    "                     PoseLandmark.RIGHT_WRIST.value),\n",
    "    \"LEFT_SHOULDER\":  (PoseLandmark.LEFT_ELBOW.value,\n",
    "                       PoseLandmark.LEFT_SHOULDER.value,\n",
    "                       PoseLandmark.LEFT_HIP.value),\n",
    "    \"RIGHT_SHOULDER\": (PoseLandmark.RIGHT_ELBOW.value,\n",
    "                       PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                       PoseLandmark.RIGHT_HIP.value),\n",
    "    \"LEFT_HIP\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "                   PoseLandmark.LEFT_HIP.value,\n",
    "                   PoseLandmark.LEFT_KNEE.value),\n",
    "    \"RIGHT_HIP\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                   PoseLandmark.RIGHT_HIP.value,\n",
    "                   PoseLandmark.RIGHT_KNEE.value),\n",
    "    \"LEFT_KNEE\":  (PoseLandmark.LEFT_HIP.value,\n",
    "                  PoseLandmark.LEFT_KNEE.value,\n",
    "                  PoseLandmark.LEFT_ANKLE.value),\n",
    "    \"RIGHT_KNEE\": (PoseLandmark.RIGHT_HIP.value,\n",
    "                  PoseLandmark.RIGHT_KNEE.value,\n",
    "                  PoseLandmark.RIGHT_ANKLE.value),\n",
    "    \"SPINE\": (\n",
    "       PoseLandmark.LEFT_HIP.value,       \n",
    "       PoseLandmark.LEFT_SHOULDER.value,   \n",
    "       PoseLandmark.RIGHT_SHOULDER.value   \n",
    "    ),\n",
    "    \"HEAD\": (\n",
    "       PoseLandmark.LEFT_SHOULDER.value,\n",
    "       PoseLandmark.NOSE.value,\n",
    "       PoseLandmark.RIGHT_SHOULDER.value\n",
    "    ),\n",
    "     \"LEFT_WRIST\":  (\n",
    "        PoseLandmark.LEFT_ELBOW.value,\n",
    "        PoseLandmark.LEFT_WRIST.value,\n",
    "        PoseLandmark.LEFT_INDEX.value\n",
    "    ),\n",
    "    \"RIGHT_WRIST\": (\n",
    "        PoseLandmark.RIGHT_ELBOW.value,\n",
    "        PoseLandmark.RIGHT_WRIST.value,\n",
    "        PoseLandmark.RIGHT_INDEX.value\n",
    "    ),\n",
    "    \"LEFT_ANKLE\":  (\n",
    "        PoseLandmark.LEFT_KNEE.value,\n",
    "        PoseLandmark.LEFT_ANKLE.value,\n",
    "        PoseLandmark.LEFT_FOOT_INDEX.value\n",
    "    ),\n",
    "    \"RIGHT_ANKLE\": (\n",
    "        PoseLandmark.RIGHT_KNEE.value,\n",
    "        PoseLandmark.RIGHT_ANKLE.value,\n",
    "        PoseLandmark.RIGHT_FOOT_INDEX.value\n",
    "    ),\n",
    "    \n",
    "}\n",
    "ERR_JOINTS = list(JOINT_TRIPLETS.keys())\n",
    "N_ERR = len(ERR_JOINTS)  # 14\n",
    "\n",
    "# 2. load original metadata & keypoints -----------------------\n",
    "DATA_ROOT    = Path(\"Data-REHAB24-6_New\")\n",
    "KEYPT_ROOT   = DATA_ROOT/\"mp_keypoints\"\n",
    "META_ORIG    = DATA_ROOT/\"Segmentation_original.xlsx\"\n",
    "df           = pd.read_excel(META_ORIG, engine=\"openpyxl\")\n",
    "df.columns   = df.columns.str.strip()\n",
    "\n",
    "# 3. compute ideal_angles on correct reps ----------------------\n",
    "ideal_angles = {}\n",
    "correct = df[df.correctness==1]\n",
    "for ex in correct.exercise_id.unique():\n",
    "    all_ang = {jn:[] for jn in ERR_JOINTS}\n",
    "    for _,r in correct[correct.exercise_id==ex].iterrows():\n",
    "        vid, f0, f1 = r.video_id, int(r.first_frame), int(r.last_frame)\n",
    "        files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "        if not files: continue\n",
    "        arr = np.load(files[0])\n",
    "        seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "        if len(seg)==0: continue\n",
    "        mid = len(seg)//2\n",
    "        frm = seg[mid]\n",
    "        for jn in ERR_JOINTS:\n",
    "            ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "            ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "            all_ang[jn].append(ang)\n",
    "    # median\n",
    "    ideal_angles[ex] = {jn:float(np.median(all_ang[jn])) for jn in all_ang if all_ang[jn]}\n",
    "\n",
    "# 4. slide windows & write rows --------------------------------\n",
    "WINDOW, STRIDE = 16, 8\n",
    "rows = []\n",
    "for _,r in df.iterrows():\n",
    "    vid, ex, f0, f1 = r.video_id, int(r.exercise_id), int(r.first_frame), int(r.last_frame)\n",
    "    files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "    if not files: continue\n",
    "    arr = np.load(files[0])                # (F,33,3)\n",
    "    seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "    if len(seg)<WINDOW: continue\n",
    "\n",
    "    # per-frame errors\n",
    "    pf_err = {jn:[] for jn in ERR_JOINTS}\n",
    "    for frm in seg:\n",
    "        for jn in ERR_JOINTS:\n",
    "            ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "            ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "            pf_err[jn].append(ang - ideal_angles[ex].get(jn,ang))\n",
    "\n",
    "    # slide\n",
    "    for start in range(0, len(seg)-WINDOW+1, STRIDE):\n",
    "        w = np.array([ pf_err[jn][start:start+WINDOW] for jn in ERR_JOINTS ])  # (10,WINDOW)\n",
    "        mean_err = w.mean(axis=1)\n",
    "        row = {\n",
    "            \"video_id\":vid,\n",
    "            \"exercise_id\":ex,\n",
    "            \"repetition_number\":r.repetition_number,\n",
    "            \"window_start\": f0+start,\n",
    "            \"window_end\":   f0+start+WINDOW,\n",
    "            \"correctness\":  r.correctness\n",
    "        }\n",
    "        for i,jn in enumerate(ERR_JOINTS):\n",
    "            row[f\"err_{i}\"] = float(mean_err[i])\n",
    "        rows.append(row)\n",
    "\n",
    "win_df = pd.DataFrame(rows)\n",
    "win_df.to_csv(DATA_ROOT/\"Segmentation_windows.csv\", index=False)\n",
    "print(\"Wrote\", len(win_df), \"windows to Segmentation_windows.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ea133",
   "metadata": {},
   "source": [
    "# 5. Traning Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa56f5d",
   "metadata": {},
   "source": [
    "## 5.1 Paths & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SCRIPT_DIR    = Path().resolve()\n",
    "DATA_ROOT     = SCRIPT_DIR/\"Data-REHAB24-6_New\"\n",
    "WIN_CSV       = DATA_ROOT/\"Segmentation_windows.csv\"\n",
    "KEYPT_ROOT    = DATA_ROOT/\"mp_keypoints\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"► Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580cd2b",
   "metadata": {},
   "source": [
    "## 5.2 Analysing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2629d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: /Users/jithinkrishnan/Documents/Study/IS06 /MVP/RehabApp/model-training-scripts/Data-REHAB24-6_New/Segmentation_original.xlsx\n",
      "Successfully loaded 'Segmentation_original.xlsx'. Shape: (1072, 13)\n",
      "\n",
      "Analyzing 'correctness' column distribution...\n",
      "\n",
      "Class Counts:\n",
      "correctness\n",
      "1    568\n",
      "0    504\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of 'Correct' (1) instances: 568\n",
      "Number of 'Wrong' (0) instances:   504\n",
      "Total analyzed instances:        1072\n",
      "\n",
      "Percentage 'Correct': 52.99%\n",
      "Percentage 'Wrong':   47.01%\n",
      "\n",
      "The dataset appears relatively balanced based on this threshold.\n"
     ]
    }
   ],
   "source": [
    "# --- Specify the file to analyze ---\n",
    "# Use the file that was generated in the previous step (after modification)\n",
    "DATA_FILENAME = \"Segmentation_original.xlsx\"\n",
    "FILE_TO_ANALYZE = DATA_ROOT / DATA_FILENAME\n",
    "\n",
    "print(f\"Attempting to load data from: {FILE_TO_ANALYZE}\")\n",
    "\n",
    "# --- Check if file exists and process ---\n",
    "if not FILE_TO_ANALYZE.is_file():\n",
    "    print(f\"Error: The file '{FILE_TO_ANALYZE.name}' was not found in '{DATA_ROOT}'.\")\n",
    "    print(\"Please ensure the previous script ran successfully and saved the file.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the dataframe\n",
    "        df = pd.read_excel(FILE_TO_ANALYZE)\n",
    "        print(f\"Successfully loaded '{FILE_TO_ANALYZE.name}'. Shape: {df.shape}\")\n",
    "\n",
    "        # Check if the 'correctness' column exists\n",
    "        if 'correctness' not in df.columns:\n",
    "            print(f\"Error: 'correctness' column not found in the dataframe.\")\n",
    "            print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "        else:\n",
    "            print(\"\\nAnalyzing 'correctness' column distribution...\")\n",
    "\n",
    "            # Get the counts for each value in the 'correctness' column\n",
    "            class_counts = df['correctness'].value_counts()\n",
    "\n",
    "            # --- Report the counts ---\n",
    "            print(\"\\nClass Counts:\")\n",
    "            print(class_counts)\n",
    "\n",
    "            # Provide a more descriptive output\n",
    "            correct_count = class_counts.get(1, 0) # Get count for value 1, default to 0 if not present\n",
    "            wrong_count = class_counts.get(0, 0)   # Get count for value 0, default to 0 if not present\n",
    "            total_count = correct_count + wrong_count # Or use len(df) if there are only 0s and 1s\n",
    "\n",
    "            print(f\"\\nNumber of 'Correct' (1) instances: {correct_count}\")\n",
    "            print(f\"Number of 'Wrong' (0) instances:   {wrong_count}\")\n",
    "            print(f\"Total analyzed instances:        {total_count}\") # Good sanity check\n",
    "\n",
    "            # --- Assess Balance ---\n",
    "            if total_count > 0:\n",
    "                correct_percentage = (correct_count / total_count) * 100\n",
    "                wrong_percentage = (wrong_count / total_count) * 100\n",
    "                print(f\"\\nPercentage 'Correct': {correct_percentage:.2f}%\")\n",
    "                print(f\"Percentage 'Wrong':   {wrong_percentage:.2f}%\")\n",
    "\n",
    "                # Basic balance assessment (you can adjust the threshold)\n",
    "                if abs(correct_percentage - wrong_percentage) < 20: # e.g., less than 20% difference (60/40 split)\n",
    "                    print(\"\\nThe dataset appears relatively balanced based on this threshold.\")\n",
    "                elif abs(correct_percentage - wrong_percentage) < 40: # e.g., less than 40% difference (70/30 split)\n",
    "                     print(\"\\nThe dataset shows moderate imbalance.\")\n",
    "                else:\n",
    "                    print(\"\\nWARNING: The dataset appears significantly imbalanced.\")\n",
    "            else:\n",
    "                print(\"\\nCannot assess balance: No instances found in the 'correctness' column.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during file loading or analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cb40c",
   "metadata": {},
   "source": [
    "### Note: Balacing of the dataset will be done duting model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bfd1e",
   "metadata": {},
   "source": [
    "## 5.3 Joint names setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97531384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOINT_NAMES: ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
      "N_JOINTS: 33\n"
     ]
    }
   ],
   "source": [
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "\n",
    "# Then:\n",
    "JOINT_NAMES = [lm.name for lm in PoseLandmark]\n",
    "N_JOINTS    = len(JOINT_NAMES)  # should be 33\n",
    "\n",
    "print(f\"JOINT_NAMES: {JOINT_NAMES}\")\n",
    "print(f\"N_JOINTS: {N_JOINTS}\")\n",
    "\n",
    "#  Exerciseses (Ex1…Ex6)\n",
    "NUM_EXERCISES = 6\n",
    "CKPT_FILE     = \"kp_pose_quality_windows_ex.pt\"  \n",
    "\n",
    "ERR_JOINTS   = [\n",
    "  \"LEFT_ELBOW\",\"RIGHT_ELBOW\",\n",
    "  \"LEFT_SHOULDER\",\"RIGHT_SHOULDER\",\n",
    "  \"LEFT_HIP\",\"RIGHT_HIP\",\n",
    "  \"LEFT_KNEE\",\"RIGHT_KNEE\",\n",
    "  \"SPINE\",\"HEAD\",\n",
    "  \"LEFT_WRIST\", \"RIGHT_WRIST\",\n",
    "  \"LEFT_ANKLE\", \"RIGHT_ANKLE\"\n",
    "]\n",
    "N_ERR = len(ERR_JOINTS)   # 14\n",
    "ERR_COLS = [f\"err_{i}\" for i in range(N_ERR)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583456b",
   "metadata": {},
   "source": [
    "## 5.4 Dataset class definition\n",
    "\n",
    "The KeypointWindowDataset class loads and processes pose keypoint data from videos for model training. It reads a CSV file containing metadata, including video IDs, exercise IDs, frame indices, and pre-calculated joint angle errors (ranging from 0 to 9). The data is sorted based on video ID, repetition number, and window start. For each sample, it loads the corresponding keypoint data (in .npy format), extracts a segment of frames based on the start and end indices, reshapes the keypoints into a 2D array, and converts them into a PyTorch tensor. It also retrieves the correctness label and the pre-calculated error values, which are stored in tensors. This class efficiently loads and processes the data in batches for training tasks like exercise recognition, where both pose keypoints and error features are used for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KeypointWindowDataset(Dataset):\n",
    "    def __init__(self, csv_file: Path, keypt_root: Path):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df.sort_values([\"video_id\",\"repetition_number\",\"window_start\"])\n",
    "        self.rows = df.to_dict(\"records\")\n",
    "        self.keypt_root = keypt_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        r   = self.rows[i]\n",
    "        ex  = int(r[\"exercise_id\"]) - 1      # zero‐based [0..NUM_EXERCISES-1]\n",
    "        vid = r[\"video_id\"]\n",
    "        f0, f1 = int(r[\"window_start\"]), int(r[\"window_end\"])\n",
    "\n",
    "        # load keypoints\n",
    "        arr = np.load(\n",
    "            next((self.keypt_root/f\"Ex{ex+1}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "        )  # shape (F,33,3)\n",
    "\n",
    "        seg = arr[f0:f1]            # (T, 33, 3)\n",
    "        seg = seg.reshape(len(seg), -1)  # (T, 99)\n",
    "        seq = torch.from_numpy(seg).float()\n",
    "\n",
    "        label = torch.tensor(r[\"correctness\"], dtype=torch.long)\n",
    "        err   = torch.tensor([r[f\"err_{j}\"] for j in range(N_ERR)],\n",
    "                             dtype=torch.float32)\n",
    "\n",
    "        return seq, label, err, ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81848f",
   "metadata": {},
   "source": [
    "### 5.5 Model definition – PoseQualityNet-KP (v3, **three concurrent tasks**)\n",
    "\n",
    "The network converts a short window of Mediapipe key-points (*T = 16 frames, D = 99 features/frame*) into:\n",
    "\n",
    "* **repetition quality**       → *Correct / Wrong*  \n",
    "* **joint-angle corrections**    → 14 absolute error degrees  \n",
    "* **exercise identity**       → 6-class label (used to catch “wrong exercise” at runtime)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "| Step | Block | I/O (batch **B**) | Purpose / comment |\n",
    "|:---:|-------|-------------------|-------------------|\n",
    "| **①** | **Keypoint Encoder**<br/>`Conv1d 99→128 → Conv1d 128→512 → GAP` | **in** (B,D) → **out** (B,512) | Frame-wise spatial features |\n",
    "| **②** | **Bi-LSTM × 2** (256 hidden, bidirectional) | **in** (B,T,512) → **out** (B,512) | Adds temporal context; mean-pool along *T* |\n",
    "| **③** | **Exercise embedding MLP**<br/>`Linear 6→64 → ReLU → Linear 64→64` | **in** one-hot (B,6) → **out** (B,64) | Injects user-selected exercise prior |\n",
    "| **④** | **Heads** | see below | Multi-task predictions |\n",
    "\n",
    "#### Output heads\n",
    "\n",
    "| Head | Input | Output shape | Target |\n",
    "|------|-------|--------------|--------|\n",
    "| `cls_head` | `[g ‖ e]` (B, 576) | (B,2) | Repetition quality |\n",
    "| `err_head` | `[g ‖ e]` (B, 576) | (B,14) | Per-joint angle error (°) |\n",
    "| `ex_head`  | *g* (B, 512)   | (B,6) | Exercise ID **(no prior fed)** |\n",
    "\n",
    "> *Design choice*: the exercise prior *e* **is not fed** to `ex_head`, forcing the model to genuinely recognise the exercise so it can raise a *“wrong-exercise”* alert.\n",
    "\n",
    "#### Parameter & speed footprint  \n",
    "\n",
    "* **≈ 3.4 M parameters** with all blocks active (variant **FULL**)  \n",
    "* **≈ 30 FPS** on an Apple-M-series GPU with *T = 16*\n",
    "\n",
    "This footprint keeps the model mobile-friendly while providing:\n",
    "\n",
    "* Dual classification (rep-quality **and** exercise ID)  \n",
    "* Fine-grained joint-angle guidance for actionable rehab feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ed8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────── Feature encoder (unchanged) ──────────────────\n",
    "class KeypointEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, embed: int = 512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, embed, 3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):                                # x: (B, D)\n",
    "        x = torch.relu(self.conv1(x.unsqueeze(2)))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return self.pool(x).squeeze(-1)                  # (B, embed = 512)\n",
    "\n",
    "# ───────────────── Pose-Quality–Exercise network ─────────────────\n",
    "class PoseQualityNetKP(nn.Module):\n",
    "    \"\"\"\n",
    "    Outputs:\n",
    "        logits_q  : (B, 2)   – Correct / Wrong\n",
    "        err_hat   : (B, N_ERR) – joint-angle errors\n",
    "        logits_ex : (B, num_ex) – predicted exercise ID\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int,\n",
    "                 num_ex: int,\n",
    "                 hidden: int = 256,\n",
    "                 ex_emb: int = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) per-frame feature extractor\n",
    "        self.encoder = KeypointEncoder(in_dim)           # → 512-d frame vec\n",
    "\n",
    "        # 2) sequence model\n",
    "        self.lstm = nn.LSTM(\n",
    "            512, hidden, num_layers=2,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "        feat_dim = hidden * 2                            # 256×2 = 512\n",
    "\n",
    "        # 3) exercise embedding ( fed with *user-selected* one-hot )\n",
    "        self.ex_emb = nn.Sequential(\n",
    "            nn.Linear(num_ex, ex_emb), nn.ReLU(),\n",
    "            nn.Linear(ex_emb, ex_emb)\n",
    "        )\n",
    "\n",
    "        # 4) heads\n",
    "        self.cls_head = nn.Linear(feat_dim + ex_emb, 2)      # quality\n",
    "        self.err_head = nn.Linear(feat_dim + ex_emb, N_ERR)  # MAE\n",
    "        self.ex_head  = nn.Linear(feat_dim,          num_ex) # exercise ✔︎\n",
    "\n",
    "    def forward(self,\n",
    "                seq: torch.Tensor,      # (B, T, D)\n",
    "                ex_1hot: torch.Tensor   # (B, num_ex)  user-selected\n",
    "    ):\n",
    "        B, T, _ = seq.shape\n",
    "\n",
    "        # frame-wise encoding\n",
    "        feats = torch.stack(\n",
    "            [self.encoder(seq[:, t]) for t in range(T)], dim=1\n",
    "        )                                               # (B, T, 512)\n",
    "\n",
    "        out, _ = self.lstm(feats)                       # (B, T, 512)\n",
    "        g = out.mean(1)                                 # (B, 512)\n",
    "\n",
    "        # exercise embedding (context for quality / error heads only)\n",
    "        ex_e = self.ex_emb(ex_1hot)                     # (B, 64)\n",
    "\n",
    "        h = torch.cat([g, ex_e], dim=1)                 # (B, 576)\n",
    "\n",
    "        logits_q  = self.cls_head(h)                    # (B, 2)\n",
    "        err_hat   = self.err_head(h)                    # (B, N_ERR)\n",
    "        logits_ex = self.ex_head(g)                     # (B, num_ex) ← uses g\n",
    "\n",
    "        return logits_q, err_hat, logits_ex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc70cc",
   "metadata": {},
   "source": [
    "# 6  Model Training — *Re-Revised Pipeline (3-head)*  \n",
    "\n",
    "The latest **PoseQualityNet-KP** now predicts **three** things at once:\n",
    "\n",
    "| Head | Output | Purpose |\n",
    "|------|--------|---------|\n",
    "| **`rep_head`** | *Correct / Wrong* (2) | Quality flag for each 0.5 s window |\n",
    "| **`ex_head`**  | *Exercise-ID* (6)     | Detects if the patient performs the prescribed drill |\n",
    "| **`err_head`** | 14 joint-angle errors | Pin-points how to fix a “Wrong” repetition |\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Recap\n",
    "\n",
    "| Block | Role | Key Details |\n",
    "|-------|------|-------------|\n",
    "| **Keypoint Encoder** | Per-frame spatial features | Conv1d (99→128) → Conv1d (128→512) → GAP |\n",
    "| **Bi-LSTM × 2** | Temporal context | 256 hidden · bidirectional → **512-D** |\n",
    "| **Exercise MLP** | Exercise bias | One-hot (6) → 64-D embedding |\n",
    "| **Heads** | Predictions | `rep_head`, `ex_head`, `err_head` (see above) |\n",
    "\n",
    "---\n",
    "\n",
    "## Data Splits & Sampling  \n",
    "\n",
    "* **70 % Train – 15 % Val – 15 % Test** (patient-stratified)  \n",
    "* **WeightedRandomSampler** on **Train** only (balances *Correct* vs *Wrong*).\n",
    "\n",
    "---\n",
    "\n",
    "## Optimisation Settings\n",
    "\n",
    "| Component | Setting | Why |\n",
    "|-----------|---------|-----|\n",
    "| **Rep-loss** | Cross-Entropy *(class-weighted)* | emphasise minority *Correct* |\n",
    "| **Ex-loss** | Cross-Entropy · **0.2 ×** | important but not dominant |\n",
    "| **Err-loss** | Smooth-L1 · **0.1 ×** | regression is auxiliary |\n",
    "| **Optimiser** | AdamW (3 × 10⁻⁴, wd 1 × 10⁻²) | decoupled weight-decay |\n",
    "| **Scheduler** | ReduceLROnPlateau on **val rep-F1** | auto LR decay |\n",
    "| **Grad clip** | 1.0 | tame LSTM spikes |\n",
    "| **Early-stop** | patience 6 on val rep-F1 | avoid over-fit |\n",
    "\n",
    "---\n",
    "\n",
    "## Metric Suite (tracked every epoch)\n",
    "\n",
    "| Metric | Meaning | Why it matters |\n",
    "|--------|---------|----------------|\n",
    "| **Rep-Acc / Rep-F1** | Quality-classification skill | headline numbers for therapists |\n",
    "| **Ex-Acc / Ex-F1** | Drill-identification skill | flags wrong exercise immediately |\n",
    "| **MAE (°)** | Avg. joint-angle error | clinical tolerance ±7 ° |\n",
    "| **Confusion Mats** | *Rep* **and** *Exercise* | exposes dominant error modes |\n",
    "| **Train-loss plots** | Total + Rep + Ex + Err | makes weighting effects visible |\n",
    "| **FPS & Params (M)** | Runtime feasibility | ≥25 FPS, ≤4 M params ✔ |\n",
    "\n",
    "All curves & matrices are auto-saved:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Check-pointing\n",
    "\n",
    "* **Best val Rep-F1** → `pose_quality_best.pt / .pth`  \n",
    "* Test-set numbers reported **after** restoring this snapshot.\n",
    "\n",
    "---\n",
    "\n",
    "## Outcome Highlights\n",
    "\n",
    "1. **Dual-task resilience** – quality and exercise heads learn **together** without hurting each other.  \n",
    "2. **Safety net** – model now refuses feedback if the patient performs the *wrong exercise*.  \n",
    "3. **Deploy-ready** – ~30 FPS on an Apple-M GPU, **3.4 M** parameters.  \n",
    "4. **Actionable analytics** – side-by-side Rep & Ex confusion matrices pinpoint next optimisation targets.\n",
    "\n",
    "> **PoseQualityNet-KP (3-head)** delivers trustworthy, real-time rehab feedback while automatically guarding against exercise mix-ups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6652654",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WIN_CSV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 226\u001b[39m\n\u001b[32m    223\u001b[39m     pd.DataFrame(history).to_json(\u001b[33m\"\u001b[39m\u001b[33mmetrics.json\u001b[39m\u001b[33m\"\u001b[39m, indent=\u001b[32m2\u001b[39m)\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# ╭───────────────────────────── RUN ──────────────────────────────╮\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m train(\u001b[43mWIN_CSV\u001b[49m, KEYPT_ROOT, epochs=\u001b[32m60\u001b[39m, batch=\u001b[32m32\u001b[39m, lr=\u001b[32m3e-4\u001b[39m)\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# ╰────────────────────────────────────────────────────────────────╯\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'WIN_CSV' is not defined"
     ]
    }
   ],
   "source": [
    "# ╭───────────────────────── CONSTANTS & HELPERS ─────────────────────────╮\n",
    "PATIENCE         = 6\n",
    "SAVE_PREFIX      = \"pose_quality_best\"\n",
    "FPS_DUMMY_FRAMES = 64    # for FPS timing\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad) / 1e6\n",
    "\n",
    "@torch.no_grad()\n",
    "def timed_forward(m, in_dim, t=FPS_DUMMY_FRAMES):\n",
    "    m.eval()\n",
    "    dummy_seq = torch.zeros(1, t, in_dim, device=DEVICE)\n",
    "    dummy_ex  = torch.zeros(1, NUM_EXERCISES, device=DEVICE)\n",
    "    _ = m(dummy_seq, dummy_ex)             # warm-up\n",
    "    t0 = time.time(); _ = m(dummy_seq, dummy_ex)\n",
    "    return t / (time.time() - t0)\n",
    "# ╰────────────────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ─────────────────────────── evaluation helper ──────────────────────────\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    rep_y, rep_p, ex_y, ex_p, err = [], [], [], [], []\n",
    "    for seq, y, e, ex in loader:\n",
    "        seq, y, e, ex = [t.to(DEVICE) for t in (seq, y, e, ex)]\n",
    "        log_rep, ê, log_ex = model(seq, F.one_hot(ex, NUM_EXERCISES).float())\n",
    "        rep_y += y.cpu().tolist()\n",
    "        rep_p += log_rep.argmax(1).cpu().tolist()\n",
    "        ex_y  += ex.cpu().tolist()\n",
    "        ex_p  += log_ex.argmax(1).cpu().tolist()\n",
    "        err   += (ê - e).abs().mean(1).cpu().tolist()\n",
    "\n",
    "    return dict(\n",
    "        rep_acc = accuracy_score(rep_y, rep_p),\n",
    "        rep_f1  = f1_score(rep_y, rep_p, average=\"weighted\"),\n",
    "        ex_acc  = accuracy_score(ex_y, ex_p),\n",
    "        ex_f1   = f1_score(ex_y, ex_p, average=\"weighted\"),\n",
    "        mae     = float(np.mean(err)),\n",
    "        rep_cm  = confusion_matrix(rep_y, rep_p),\n",
    "        ex_cm   = confusion_matrix(ex_y, ex_p)\n",
    "    )\n",
    "# ╰────────────────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "def train(csv_file, keypt_root, *, epochs=60, batch=32, lr=3e-4):\n",
    "    # ───── prepare data splits ─────────────────────────────────────────────────\n",
    "    ds  = KeypointWindowDataset(csv_file, keypt_root)\n",
    "    idx = np.random.permutation(len(ds))\n",
    "    c1, c2 = int(.7*len(ds)), int(.85*len(ds))\n",
    "    tr_idx, val_idx, tst_idx = idx[:c1], idx[c1:c2], idx[c2:]\n",
    "\n",
    "    # class weights + sampler for 'correctness'\n",
    "    y_tr = [ds.rows[i][\"correctness\"] for i in tr_idx]\n",
    "    freq = np.bincount(y_tr)\n",
    "    cls_w = torch.tensor([1., freq[0]/max(freq[1],1)],\n",
    "                         dtype=torch.float32, device=DEVICE)\n",
    "    samp_w = [freq.sum()/(len(freq)*freq[l]) for l in y_tr]\n",
    "\n",
    "    mk = lambda ids, **kw: DataLoader(Subset(ds, ids), batch_size=batch, **kw)\n",
    "    tr_dl  = mk(tr_idx, sampler=WeightedRandomSampler(samp_w, len(samp_w)))\n",
    "    val_dl = mk(val_idx, shuffle=False)\n",
    "    tst_dl = mk(tst_idx, shuffle=False)\n",
    "    # ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ───── build model & optimiser ───────────────────────────────────────────────\n",
    "    in_dim = ds[0][0].shape[-1]\n",
    "    model  = PoseQualityNetKP(in_dim, NUM_EXERCISES).to(DEVICE)\n",
    "    print(f\"❚ params {count_params(model):.2f} M  •  device {DEVICE}\")\n",
    "\n",
    "    crit_rep = nn.CrossEntropyLoss(weight=cls_w)   # rep-quality\n",
    "    crit_err = nn.SmoothL1Loss()                   # joint-angle MAE\n",
    "    crit_ex  = nn.CrossEntropyLoss()               # exercise head\n",
    "    opt   = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    sched = ReduceLROnPlateau(opt, mode=\"max\", factor=.5, patience=3)\n",
    "    # ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ───── live dashboard setup ─────────────────────────────────────────────────\n",
    "    from IPython.display import clear_output\n",
    "    fig,(axLoss, axF1, axAcc) = plt.subplots(1,3, figsize=(20,4))\n",
    "    axAcc2 = axAcc.twinx()\n",
    "\n",
    "    lnTot, = axLoss.plot([], [], \"o-\", c=\"tab:blue\",   label=\"Total\")\n",
    "    lnRep, = axLoss.plot([], [], \"^-\", c=\"tab:red\",    label=\"Rep\")\n",
    "    lnEx , = axLoss.plot([], [], \"s-\", c=\"tab:orange\", label=\"Ex\")\n",
    "    lnErr, = axLoss.plot([], [], \"d-\", c=\"tab:green\",  label=\"Err×0.1\")\n",
    "\n",
    "    lnF1rep_v, = axF1.plot([], [], \"o-\", c=\"tab:purple\", label=\"Val Rep-F1\")\n",
    "    lnF1ex_v , = axF1.plot([], [], \"s-\", c=\"tab:cyan\",   label=\"Val Ex-F1\")\n",
    "\n",
    "    lnAccrep_v, = axAcc.plot([], [], \"o-\",  c=\"tab:olive\", label=\"Val Rep-Acc\")\n",
    "    lnAccex_v , = axAcc.plot([], [], \"^-\",  c=\"tab:blue\",  label=\"Val Ex-Acc\")\n",
    "    lnMAE_v,    = axAcc2.plot([], [], \"x-\", c=\"tab:orange\", label=\"Val MAE\")\n",
    "\n",
    "    for a in (axLoss, axF1, axAcc):\n",
    "        a.set_xlabel(\"epoch\")\n",
    "    axLoss.set_ylabel(\"loss\");            axLoss.legend()\n",
    "    axF1  .set_ylabel(\"weighted F1\");     axF1.legend()\n",
    "    axAcc .set_ylabel(\"accuracy\");        axAcc2.set_ylabel(\"MAE (°)\")\n",
    "    axAcc.set_title(\"Accuracies & MAE\")\n",
    "    axAcc.legend(loc=\"upper left\");      axAcc2.legend(loc=\"upper right\")\n",
    "    display(fig)\n",
    "    plt.tight_layout()\n",
    "    # ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ───── training + early-stopping on validation ──────────────────────────────\n",
    "    history, best_f1, best_state, stall = [], 0., None, 0\n",
    "    for ep in range(1, epochs+1):\n",
    "        # — train epoch —\n",
    "        model.train()\n",
    "        totL = repL = exL = errL = 0.0\n",
    "        for seq,y,e,ex in tr_dl:\n",
    "            seq,y,e,ex = [t.to(DEVICE) for t in (seq,y,e,ex)]\n",
    "            opt.zero_grad()\n",
    "            log_r, ê, log_ex = model(seq, F.one_hot(ex,NUM_EXERCISES).float())\n",
    "            Lr = crit_rep(log_r,  y)\n",
    "            Le = crit_err( ê,    e)\n",
    "            Lx = crit_ex (log_ex,ex)\n",
    "            loss = Lr + 0.1*Le + 0.2*Lx\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "            opt.step()\n",
    "\n",
    "            bs = y.size(0)\n",
    "            totL += loss.item()*bs\n",
    "            repL += Lr.item()*bs\n",
    "            errL += Le.item()*bs\n",
    "            exL  += Lx.item()*bs\n",
    "\n",
    "        # — validation —\n",
    "        val = evaluate(model, val_dl)\n",
    "        sched.step(val[\"rep_f1\"])\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\":       ep,\n",
    "            \"train_tot\":   totL/len(tr_idx),\n",
    "            \"train_rep\":   repL/len(tr_idx),\n",
    "            \"train_err\":   errL/len(tr_idx),\n",
    "            \"train_ex\":    exL/len(tr_idx),\n",
    "            \"val_rep_f1\":  val[\"rep_f1\"],\n",
    "            \"val_ex_f1\":   val[\"ex_f1\"],\n",
    "            \"val_rep_acc\": val[\"rep_acc\"],\n",
    "            \"val_ex_acc\":  val[\"ex_acc\"],\n",
    "            \"val_mae\":     val[\"mae\"]\n",
    "        })\n",
    "\n",
    "        print(f\"E{ep:02d} | train_tot {history[-1]['train_tot']:.3f}  \"\n",
    "              f\"Val-Rep-F1 {val['rep_f1']:.3f}  Val-Ex-F1 {val['ex_f1']:.3f}  Val-MAE {val['mae']:.2f}\")\n",
    "\n",
    "        # early-stop on val_rep_f1\n",
    "        if val[\"rep_f1\"] > best_f1:\n",
    "            best_f1, best_state, stall = val[\"rep_f1\"], model.state_dict(), 0\n",
    "        else:\n",
    "            stall += 1\n",
    "            if stall >= PATIENCE:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "        # — update live dashboard —\n",
    "        xs = [h[\"epoch\"] for h in history]\n",
    "        lnTot.set_data(xs,[h[\"train_tot\"]   for h in history])\n",
    "        lnRep.set_data(xs,[h[\"train_rep\"]   for h in history])\n",
    "        lnErr.set_data(xs,[h[\"train_err\"]   for h in history])\n",
    "        lnEx .set_data(xs,[h[\"train_ex\"]    for h in history])\n",
    "\n",
    "        lnF1rep_v.set_data(xs,[h[\"val_rep_f1\"] for h in history])\n",
    "        lnF1ex_v .set_data(xs,[h[\"val_ex_f1\"]  for h in history])\n",
    "\n",
    "        lnAccrep_v.set_data(xs,[h[\"val_rep_acc\"] for h in history])\n",
    "        lnAccex_v .set_data(xs,[h[\"val_ex_acc\"]  for h in history])\n",
    "        lnMAE_v   .set_data(xs,[h[\"val_mae\"]     for h in history])\n",
    "\n",
    "        # ← NEW: rescale each axis to fit\n",
    "        for ax in (axLoss, axF1, axAcc, axAcc2):\n",
    "            ax.relim()\n",
    "            ax.autoscale_view()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "    # ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ───── print per-epoch summary table ───────────────────────────────────────\n",
    "    from tabulate import tabulate\n",
    "    df = pd.DataFrame(history)\n",
    "    cols = [\n",
    "        \"epoch\",\n",
    "        \"train_tot\",\"train_rep\",\"train_ex\",\"train_err\",\n",
    "        \"val_rep_f1\",\"val_ex_f1\",\"val_mae\",\n",
    "        \"val_rep_acc\",\"val_ex_acc\"\n",
    "    ]\n",
    "    print(\"\\n### Per-epoch summary\\n\")\n",
    "    print(tabulate(df[cols], headers=\"keys\",\n",
    "                   floatfmt=\".3f\", tablefmt=\"github\"))\n",
    "    # ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ───── final-test evaluation & heatmaps ────────────────────────────────────\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Test set size: {len(tst_idx)}\")\n",
    "    test = evaluate(model, tst_dl)\n",
    "    print(\"\\n◆ Final test metrics ◆\")\n",
    "    print(f\"Rep-Acc: {test['rep_acc']:.3f}, Rep-F1: {test['rep_f1']:.3f}\")\n",
    "    print(f\"Ex-Acc:  {test['ex_acc']:.3f}, Ex-F1:  {test['ex_f1']:.3f}\")\n",
    "    print(f\"MAE:     {test['mae']:.2f}°\")\n",
    "    print(f\"FPS:     {timed_forward(model,in_dim):.1f}\")\n",
    "    print(f\"Params:  {count_params(model):.2f} M\\n\")\n",
    "\n",
    "    if test[\"rep_cm\"].size > 0:\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        sns.heatmap(test[\"rep_cm\"], annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Repetition-Quality CM\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ rep_cm is empty — skipping heatmap.\")\n",
    "\n",
    "    if test[\"ex_cm\"].size > 0:\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        sns.heatmap(test[\"ex_cm\"], annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "        plt.title(\"Exercise-ID CM\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ ex_cm is empty — skipping heatmap.\")\n",
    "    # dump metrics.json\n",
    "    pd.DataFrame(history).to_json(\"metrics.json\", indent=2)\n",
    "\n",
    "# ╭───────────────────────────── RUN ──────────────────────────────╮\n",
    "train(WIN_CSV, KEYPT_ROOT, epochs=60, batch=32, lr=3e-4)\n",
    "# ╰────────────────────────────────────────────────────────────────╯\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03401151",
   "metadata": {},
   "source": [
    "# 7. Ablation study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2154a",
   "metadata": {},
   "source": [
    "### Ablation-study score card  – *3-head PoseQualityNet-KP*\n",
    "\n",
    "We now compare four architectural variants and **seven key metrics per **task**:  \n",
    "repetition-quality (**Rep**) *and* exercise-ID (**Ex**).\n",
    "\n",
    "| Metric | Scope | Definition (updated) | Why it matters |\n",
    "|--------|-------|----------------------|----------------|\n",
    "| **Rep-accuracy** | Repetition-quality | fraction of windows classified *Correct / Wrong* correctly | First sanity check that the rep-quality head is doing its job |\n",
    "| **Rep-weighted F1** | Repetition-quality | harmonic mean of precision & recall, weighted by class frequency | Headline score for rep-quality under the 70 : 30 imbalance |\n",
    "| **Ex-accuracy** | Exercise ID | fraction of windows for which the exercise class (1-6) is predicted correctly | Verifies the network can detect wrong-exercise scenarios at inference time |\n",
    "| **Ex-weighted F1** | Exercise ID | weighted F1 over the 6-class exercise classifier | More informative than raw accuracy when some exercises are rarer |\n",
    "| **MAE (°)** | Joint-error head | mean absolute error over the 14 predicted joint-angle deviations | Directly maps to clinical tolerance (±7°); lower = better feedback granularity |\n",
    "| **Throughput (FPS)** | All heads | forward passes per second on a laptop-GPU / M-series Mac | ≥25 fps needed for real-time coaching |\n",
    "| **Params (M)** | All heads | millions of trainable parameters | Proxy for memory / mobile compute cost; tells us if a block is “worth” its gains |\n",
    "\n",
    "---\n",
    "\n",
    "#### How the table answers the big questions\n",
    "\n",
    "1. **Does it judge reps *and* exercise type well?**  \n",
    "   → `Rep-accuracy / F1` **and** `Ex-accuracy / F1`\n",
    "\n",
    "2. **Is the joint-level feedback useful?**  \n",
    "   → `MAE (°)` (must stay close to clinical ±7° tolerance)\n",
    "\n",
    "3. **Can we ship it on-device?**  \n",
    "   → `FPS` ≥ 25 and `Params (M)` ≲ 4 M  \n",
    "\n",
    "Blocks that improve (1) and/or (2) **without** breaking (3) survive the ablation.\n",
    "\n",
    "> After the loop the script prints a styled summary table (one row per variant) and stores per-epoch logs in **`metrics.json`** for deeper dives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34eae92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeypointWindowDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    144\u001b[39m     display(df.style.format({\u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mparams_M\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfps\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33mrep_acc\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mex_acc\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    146\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33mrep_f1\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mex_f1\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[33m\"\u001b[39m}))\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# ╰───────────────────────────────────────────────────────────────╯\u001b[39;00m\n\u001b[32m    148\u001b[39m \n\u001b[32m    149\u001b[39m \n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# ╭────────────────────────── RUN IT ─────────────────────────────╮\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mrun_ablation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# ╰───────────────────────────────────────────────────────────────╯\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mrun_ablation\u001b[39m\u001b[34m(epochs, batch, lr)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_ablation\u001b[39m(epochs=\u001b[32m60\u001b[39m, batch=\u001b[32m32\u001b[39m, lr=\u001b[32m3e-4\u001b[39m):\n\u001b[32m     88\u001b[39m     torch.manual_seed(\u001b[32m0\u001b[39m); np.random.seed(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     ds   = \u001b[43mKeypointWindowDataset\u001b[49m(CSV_FILE, KEYPT_ROOT)\n\u001b[32m     91\u001b[39m     in_d = ds[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].shape[-\u001b[32m1\u001b[39m]\n\u001b[32m     93\u001b[39m     idx = np.random.permutation(\u001b[38;5;28mlen\u001b[39m(ds)); c1,c2 = \u001b[38;5;28mint\u001b[39m(\u001b[32m.7\u001b[39m*\u001b[38;5;28mlen\u001b[39m(ds)), \u001b[38;5;28mint\u001b[39m(\u001b[32m.85\u001b[39m*\u001b[38;5;28mlen\u001b[39m(ds))\n",
      "\u001b[31mNameError\u001b[39m: name 'KeypointWindowDataset' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ablation driver for PoseQualityNetKP (3-head)\n",
    "\n",
    "Variants  \n",
    "  A   – CNN only  \n",
    "  B   – CNN + ex-embedding  \n",
    "  C   – CNN + Bi-LSTM  \n",
    "  FULL– CNN + Bi-LSTM + ex-embedding\n",
    "\"\"\"\n",
    "# PoseQualityNetKP & KeypointWindowDataset must already be in scope\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "SCRIPT_DIR = Path().resolve()\n",
    "DATA_ROOT  = SCRIPT_DIR / \"Data-REHAB24-6_New\"\n",
    "CSV_FILE   = DATA_ROOT / \"Segmentation_windows.csv\"\n",
    "KEYPT_ROOT = DATA_ROOT / \"mp_keypoints\"\n",
    "\n",
    "DEVICE = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "          else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "          else torch.device(\"cpu\"))\n",
    "\n",
    "NUM_EXERCISES = 6\n",
    "PATIENCE      = 6\n",
    "# ╰─────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ╭──────────── MODEL FACTORY (toggle blocks) ────────────╮\n",
    "class DummyLSTM(nn.Module):           # keeps interface → (out, h)\n",
    "    def forward(self, x): return x, None\n",
    "\n",
    "class Zero64(nn.Module):              # keeps 64-dim embedding\n",
    "    def forward(self, x):\n",
    "        return torch.zeros(x.size(0), 64, device=x.device, dtype=x.dtype)\n",
    "\n",
    "def build_model(variant: str, in_dim: int) -> nn.Module:\n",
    "    use_lstm = variant in (\"C\", \"FULL\")\n",
    "    use_emb  = variant in (\"B\", \"FULL\")\n",
    "\n",
    "    class Net(PoseQualityNetKP):\n",
    "        def __init__(self):\n",
    "            super().__init__(in_dim, NUM_EXERCISES, hidden=256)\n",
    "            if not use_lstm: self.lstm   = DummyLSTM()\n",
    "            if not use_emb : self.ex_emb = Zero64()\n",
    "    return Net().to(DEVICE)\n",
    "# ╰────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ╭──────────────────── HELPER FUNCTIONS ────────────────────╮\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    rep_y, rep_p, ex_y, ex_p, err = [], [], [], [], []\n",
    "    for seq,y,e,ex in loader:\n",
    "        seq,y,e,ex = [t.to(DEVICE) for t in (seq,y,e,ex)]\n",
    "        log_rep, ê, log_ex = model(seq, F.one_hot(ex, NUM_EXERCISES).float())\n",
    "        rep_y += y.cpu().tolist()\n",
    "        rep_p += log_rep.argmax(1).cpu().tolist()\n",
    "        ex_y  += ex.cpu().tolist()\n",
    "        ex_p  += log_ex.argmax(1).cpu().tolist()\n",
    "        err   += (ê-e).abs().mean(1).cpu().tolist()\n",
    "\n",
    "    return dict(\n",
    "        rep_acc = accuracy_score(rep_y, rep_p),\n",
    "        rep_f1  = f1_score(rep_y, rep_p, average=\"weighted\"),\n",
    "        ex_acc  = accuracy_score(ex_y , ex_p),\n",
    "        ex_f1   = f1_score(ex_y , ex_p , average=\"weighted\"),\n",
    "        mae     = float(np.mean(err)),\n",
    "        rep_cm  = confusion_matrix(rep_y, rep_p),\n",
    "        ex_cm   = confusion_matrix(ex_y , ex_p )\n",
    "    )\n",
    "\n",
    "def timed_fps(model, in_dim, t=64):\n",
    "    s = torch.zeros(1,t,in_dim,device=DEVICE)\n",
    "    e = torch.zeros(1,NUM_EXERCISES,device=DEVICE)\n",
    "    _ = model(s,e)                                  # warm-up\n",
    "    if DEVICE.type==\"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time(); _ = model(s,e)\n",
    "    if DEVICE.type==\"cuda\": torch.cuda.synchronize()\n",
    "    return round(t/(time.time()-t0),1)\n",
    "\n",
    "def count_params(m):\n",
    "    return round(sum(p.numel() for p in m.parameters() if p.requires_grad)/1e6,2)\n",
    "# ╰──────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ╭────────────────────── ABLATION DRIVER ─────────────────────────╮\n",
    "def run_ablation(epochs=60, batch=32, lr=3e-4):\n",
    "    torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "    ds   = KeypointWindowDataset(CSV_FILE, KEYPT_ROOT)\n",
    "    in_d = ds[0][0].shape[-1]\n",
    "\n",
    "    idx = np.random.permutation(len(ds)); c1,c2 = int(.7*len(ds)), int(.85*len(ds))\n",
    "    split = {\"train\":idx[:c1], \"val\":idx[c1:c2], \"test\":idx[c2:]}\n",
    "\n",
    "    y_tr  = [ds.rows[i][\"correctness\"] for i in split[\"train\"]]\n",
    "    freq  = np.bincount(y_tr)\n",
    "    cls_w = torch.tensor([freq.sum()/(2*freq[0]), freq.sum()/(2*freq[1])],\n",
    "                         dtype=torch.float32, device=DEVICE)\n",
    "    samp_w= [freq.sum()/(len(freq)*freq[l]) for l in y_tr]\n",
    "\n",
    "    mk = lambda ids,**kw: DataLoader(Subset(ds, ids), batch_size=batch, **kw)\n",
    "    dl = {k: mk(v, sampler=WeightedRandomSampler(samp_w,len(samp_w))\n",
    "                 if k==\"train\" else None, shuffle=False)\n",
    "          for k,v in split.items()}\n",
    "\n",
    "    rows=[]\n",
    "    for var in [\"A\",\"B\",\"C\",\"FULL\"]:\n",
    "        print(f\"\\n── Variant {var}\")\n",
    "        model = build_model(var, in_d)\n",
    "        opt   = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "        best, stall = 0., 0\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            model.train()\n",
    "            for seq,y,e,ex in dl[\"train\"]:\n",
    "                seq,y,e,ex=[t.to(DEVICE) for t in (seq,y,e,ex)]\n",
    "                opt.zero_grad()\n",
    "                log_r, ê, log_ex = model(seq, F.one_hot(ex,NUM_EXERCISES).float())\n",
    "                loss = (F.cross_entropy(log_r ,y ,weight=cls_w) +\n",
    "                        0.1*F.smooth_l1_loss(ê,e) +\n",
    "                        0.2*F.cross_entropy(log_ex,ex))\n",
    "                loss.backward(); nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "                opt.step()\n",
    "\n",
    "            val = evaluate(model, dl[\"val\"])\n",
    "            if val[\"rep_f1\"]>best: best,stall=val[\"rep_f1\"],0\n",
    "            else: stall+=1\n",
    "            if stall>=PATIENCE: break\n",
    "\n",
    "        test = evaluate(model, dl[\"test\"])\n",
    "        rows.append(dict(\n",
    "            variant   = var,\n",
    "            rep_acc   = test[\"rep_acc\"],   rep_f1 = test[\"rep_f1\"],\n",
    "            ex_acc    = test[\"ex_acc\"],    ex_f1  = test[\"ex_f1\"],\n",
    "            mae       = test[\"mae\"],\n",
    "            fps       = timed_fps(model,in_d),\n",
    "            params_M  = count_params(model)\n",
    "        ))\n",
    "        print(rows[-1])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"\\nAblation summary\")\n",
    "    display(df.style.format({\"mae\":\"{:.2f}\",\"params_M\":\"{:.2f}\",\"fps\":\"{:.0f}\",\n",
    "                             \"rep_acc\":\"{:.3f}\",\"ex_acc\":\"{:.3f}\",\n",
    "                             \"rep_f1\":\"{:.3f}\",\"ex_f1\":\"{:.3f}\"}))\n",
    "# ╰───────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ╭────────────────────────── RUN IT ─────────────────────────────╮\n",
    "run_ablation(epochs=60, batch=32, lr=3e-4)\n",
    "# ╰───────────────────────────────────────────────────────────────╯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b4529",
   "metadata": {},
   "source": [
    "# 6. Inference Tesing - Correctness and feedback on Live Videos / Recorded Videos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cd790",
   "metadata": {},
   "source": [
    "## 7 Inference & Real-Time Feedback\n",
    "\n",
    "This notebook cell launches a simple CLI + OpenCV/MediaPipe based inference script that:\n",
    "\n",
    "1. **Loads** a pretrained `PoseQualityNetKP` checkpoint.  \n",
    "2. **Initializes** MediaPipe Pose to track 33 landmarks and draws them on each frame.  \n",
    "3. **Buffers** a sliding window of 16 frames of world‐space keypoints.  \n",
    "4. **Checks** that all required landmarks for the chosen exercise are visible (visibility ≥ 0.8).  \n",
    "5. **Runs** the buffer through `PoseQualityNetKP` to obtain:  \n",
    "   - **`logits_q`** → binary pose‐quality (“Correct” vs “Form issue”)  \n",
    "   - **`err_hat`**  → 14 joint‐angle deviations in degrees  \n",
    "   - **`logits_ex`** → predicted exercise ID (for mismatch detection)  \n",
    "6. **Displays**:\n",
    "   - A green “You’re on the right track!” if classified correct.  \n",
    "   - A red “Form issue” plus up to 3 worst joint warnings (e.g. `LEFT_ELBOW +12°`) if incorrect, shown for 90 frames.  \n",
    "   - Guidance messages (“No pose”, “Adjust posture”, “World landmarks missing”) when detection fails.  \n",
    "7. **Throttles** live capture to ≈30 FPS (by skipping frames if webcam FPS > 30).  \n",
    "8. **Cleans up** OpenCV windows and MediaPipe resources on exit.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Key Constants & Parameters\n",
    "\n",
    "| Name                       | Value            | Purpose                                                         |\n",
    "|----------------------------|------------------|-----------------------------------------------------------------|\n",
    "| `VIS_THRESH`               | 0.8              | Minimum landmark visibility to include in error-warning checks. |\n",
    "| `SEQ_LEN`                  | 16               | Number of frames in the temporal buffer.                        |\n",
    "| `TH_ERR_DEG`               | 10               | Joint deviation threshold (°) for issuing a warning.            |\n",
    "| `SUGGESTION_DURATION_FRAMES` | 3×30          | How long (in frames) to display each joint suggestion.          |\n",
    "| `FPS_DUMMY_FRAMES`         | 64               | Number of dummy frames for FPS measurement at the end.          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 How It Works\n",
    "\n",
    "1. **Video Capture**  \n",
    "   - Opens either a file path (e.g. `“Videos/Ex1/PM_000-Camera17-30fps.mp4”`) or webcam (`0`).  \n",
    "   - If webcam, reads `CAP_PROP_FPS` and skips frames to maintain a 30 FPS processing rate.  \n",
    "\n",
    "2. **MediaPipe Pose**  \n",
    "   - Detects 33 landmarks each frame; converts to world‐space 3D coordinates.  \n",
    "   - Draws landmarks with OpenCV on the original BGR frame.  \n",
    "\n",
    "3. **Buffer & Visibility Check**  \n",
    "   - Maintains a `collections.deque(maxlen=16)` of flattened `(x,y,z)` world‐space keypoints.  \n",
    "   - If any required triplet (e.g. `LEFT_SHOULDER–LEFT_ELBOW–LEFT_WRIST`) has visibility < 0.8, resets the buffer and prompts “Adjust posture”.\n",
    "\n",
    "4. **Model Inference**  \n",
    "   - Once 16 frames collected:\n",
    "     - Constructs a `(1,16,99)` tensor (`batch, time, 33 joints×3 coords`).  \n",
    "     - One‐hot encodes the user‐selected exercise ID (shape `(1,6)`).  \n",
    "     - Feeds both into `model(seq, ex_1hot)` → `(logits_q, err_hat, logits_ex)`.  \n",
    "\n",
    "5. **Feedback Logic**  \n",
    "   - **Exercise mismatch**: if `argmax(logits_ex)+1 ≠ user_ex_id`, displays “Wrong exercise – doing Ex X”.  \n",
    "   - **Quality classification**:\n",
    "     - `argmax(logits_q)==1` → “You’re on the right track!” (green).  \n",
    "     - `argmax(logits_q)==0` → “Form issue” (red), then:\n",
    "       - Sorts `err_hat` descending by absolute value, picks top 3 joints exceeding `TH_ERR_DEG`.  \n",
    "       - Formats suggestions (e.g. `“Check LEFT_ELBOW (Dev: +12°)”`) and displays in yellow.  \n",
    "\n",
    "6. **Overlay & Display**  \n",
    "   - Uses `cv2.putText` to render exercise name, feedback, and suggestions in color on each frame.  \n",
    "   - Window title: **“Exercise-coach”**.  \n",
    "   - Exit on pressing **q**.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ How to Run\n",
    "\n",
    "1. Install dependencies\n",
    "2. Place files:\n",
    "    - this script in your working directory\n",
    "    - Data-REHAB24-6/Segmentation_windows.csv & mp_keypoints/\n",
    "    - pose_quality_best.pt checkpoint alongside the script\n",
    "\n",
    "3. Launch inference\n",
    "python pose_inference.py\n",
    "\n",
    "4. Input prompts:\n",
    "Select exercise 1–6:  1  \n",
    "Video source (0=webcam or path):  0      # for webcam  \n",
    "or for file:\n",
    "Video source (0=webcam or path):  Videos/Ex1/PM_000-Camera17-30fps.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37096455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(CKPT_PATH,map_location=DEVICE)\n",
      "I0000 00:00:1745630563.599409  895948 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► device = mps\n",
      "Loading model …\n",
      "✓ model ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745630563.636967  897029 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745630563.660157  897029 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 = webcam, or drop a video path)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 09:23:02.606 python[26678:895948] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-26 09:23:02.606 python[26678:895948] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "W0000 00:00:1745630584.287485  897031 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-5 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-8 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-6 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-7 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-9 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-10 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-11 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-15 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-13 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-14 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-16 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-17 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-19 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n",
      "Error: AudioFileOpen failed ('wht?')\n",
      "Exception in thread Thread-20 (speak):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26678/1712933293.py\", line 30, in speak\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'temp.mp3'\n"
     ]
    }
   ],
   "source": [
    "# ───────────────── Imports ─────────────────\n",
    "import os\n",
    "import math\n",
    "import threading\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from gtts import gTTS\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "SCRIPT_DIR  = Path().resolve()\n",
    "DATA_ROOT   = SCRIPT_DIR / \"Data-REHAB24-6_New\"\n",
    "CKPT_PATH   = SCRIPT_DIR / \"pose_quality_best.pt\"      # <─ name from trainer\n",
    "\n",
    "DEVICE = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "          else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "          else torch.device(\"cpu\"))\n",
    "print(\"► device =\", DEVICE)\n",
    "\n",
    "# ─── TTS SETUP (FOR VOICE)────────────────────────────────────\n",
    "def speak(text: str):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(\"temp.mp3\")\n",
    "    os.system(\"afplay temp.mp3\")  # Use afplay on macOS to play the audio\n",
    "    os.remove(\"temp.mp3\")\n",
    "\n",
    "def speak_async(text: str):\n",
    "    threading.Thread(target=speak, args=(text,), daemon=True).start()\n",
    "\n",
    "# ╭───────────────────────── CONSTANTS ─────────────────────────╮\n",
    "mp_pose = mp.solutions.pose\n",
    "PoseLM  = mp_pose.PoseLandmark\n",
    "JOINT_NAMES  = [lm.name for lm in PoseLM];   N_JOINTS = len(JOINT_NAMES)\n",
    "\n",
    "EXERCISE_MAP = {1:\"Arm-abduction\", 2:\"Arm-VW\", 3:\"Push-ups\",\n",
    "                4:\"Leg-abduction\", 5:\"Lunge\", 6:\"Squat\"}\n",
    "NUM_EX = len(EXERCISE_MAP)\n",
    "\n",
    "# joints whose angle MAE the model predicts (same order as training)\n",
    "ERR_JOINTS = [\n",
    "    \"LEFT_ELBOW\",\"RIGHT_ELBOW\",\"LEFT_SHOULDER\",\"RIGHT_SHOULDER\",\n",
    "    \"LEFT_HIP\",\"RIGHT_HIP\",\"LEFT_KNEE\",\"RIGHT_KNEE\",\n",
    "    \"SPINE\",\"HEAD\",\"LEFT_WRIST\",\"RIGHT_WRIST\",\"LEFT_ANKLE\",\"RIGHT_ANKLE\"\n",
    "];   N_ERR = len(ERR_JOINTS)\n",
    "\n",
    "JOINT_LABELS = [j.replace('_', ' ').lower() for j in ERR_JOINTS] # → [\"left elbow\", \"right elbow\", ..., \"right ankle\"]\n",
    "\n",
    "# index triplets for live angle warnings (unchanged)\n",
    "JOINT_TRIPLETS = {\n",
    "    \"LEFT_ELBOW\": (PoseLM.LEFT_SHOULDER, PoseLM.LEFT_ELBOW, PoseLM.LEFT_WRIST),\n",
    "    \"RIGHT_ELBOW\":(PoseLM.RIGHT_SHOULDER,PoseLM.RIGHT_ELBOW,PoseLM.RIGHT_WRIST),\n",
    "    \"LEFT_SHOULDER\": (PoseLM.LEFT_ELBOW, PoseLM.LEFT_SHOULDER, PoseLM.LEFT_HIP),\n",
    "    \"RIGHT_SHOULDER\":(PoseLM.RIGHT_ELBOW,PoseLM.RIGHT_SHOULDER,PoseLM.RIGHT_HIP),\n",
    "    \"LEFT_HIP\": (PoseLM.LEFT_SHOULDER,PoseLM.LEFT_HIP,PoseLM.LEFT_KNEE),\n",
    "    \"RIGHT_HIP\":(PoseLM.RIGHT_SHOULDER,PoseLM.RIGHT_HIP,PoseLM.RIGHT_KNEE),\n",
    "    \"LEFT_KNEE\": (PoseLM.LEFT_HIP,PoseLM.LEFT_KNEE,PoseLM.LEFT_ANKLE),\n",
    "    \"RIGHT_KNEE\":(PoseLM.RIGHT_HIP,PoseLM.RIGHT_KNEE,PoseLM.RIGHT_ANKLE),\n",
    "    \"SPINE\": (PoseLM.LEFT_HIP,PoseLM.LEFT_SHOULDER,PoseLM.RIGHT_SHOULDER),\n",
    "    \"HEAD\":  (PoseLM.LEFT_SHOULDER,PoseLM.NOSE,PoseLM.RIGHT_SHOULDER),\n",
    "    \"LEFT_WRIST\": (PoseLM.LEFT_ELBOW,PoseLM.LEFT_WRIST,PoseLM.LEFT_INDEX),\n",
    "    \"RIGHT_WRIST\":(PoseLM.RIGHT_ELBOW,PoseLM.RIGHT_WRIST,PoseLM.RIGHT_INDEX),\n",
    "    \"LEFT_ANKLE\": (PoseLM.LEFT_KNEE,PoseLM.LEFT_ANKLE,PoseLM.LEFT_FOOT_INDEX),\n",
    "    \"RIGHT_ANKLE\":(PoseLM.RIGHT_KNEE,PoseLM.RIGHT_ANKLE,PoseLM.RIGHT_FOOT_INDEX)\n",
    "}\n",
    "\n",
    "VIS_THRESH  = 0.8\n",
    "SEQ_LEN     = 16\n",
    "IN_DIM      = N_JOINTS * 3\n",
    "TH_ERR_DEG  = 10      # show joint warning if |err| ≥ 10°\n",
    "\n",
    "# ╭─────────────────────── MODEL DEFINITION ───────────────────────╮\n",
    "# ────────────────── Feature encoder (unchanged) ──────────────────\n",
    "class KeypointEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, embed: int = 512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, embed, 3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):                                # x: (B, D)\n",
    "        x = torch.relu(self.conv1(x.unsqueeze(2)))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return self.pool(x).squeeze(-1)                  # (B, embed = 512)\n",
    "\n",
    "# ───────────────── Pose-Quality–Exercise network ─────────────────\n",
    "class PoseQualityNetKP(nn.Module):\n",
    "    \"\"\"\n",
    "    Outputs:\n",
    "        logits_q  : (B, 2)   – Correct / Wrong\n",
    "        err_hat   : (B, N_ERR) – joint-angle errors\n",
    "        logits_ex : (B, num_ex) – predicted exercise ID\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int,\n",
    "                 num_ex: int,\n",
    "                 hidden: int = 256,\n",
    "                 ex_emb: int = 64):\n",
    "        super().__init__()\n",
    "        # 1) per-frame feature extractor\n",
    "        self.encoder = KeypointEncoder(in_dim)           # → 512-d frame vec\n",
    "        # 2) sequence model\n",
    "        self.lstm = nn.LSTM(\n",
    "            512, hidden, num_layers=2,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "        feat_dim = hidden * 2                            # 256×2 = 512\n",
    "        # 3) exercise embedding ( fed with *user-selected* one-hot )\n",
    "        self.ex_emb = nn.Sequential(\n",
    "            nn.Linear(num_ex, ex_emb), nn.ReLU(),\n",
    "            nn.Linear(ex_emb, ex_emb)\n",
    "        )\n",
    "        # 4) heads\n",
    "        self.cls_head = nn.Linear(feat_dim + ex_emb, 2)      # quality\n",
    "        self.err_head = nn.Linear(feat_dim + ex_emb, N_ERR)  # MAE\n",
    "        self.ex_head  = nn.Linear(feat_dim,          num_ex) # exercise ✔︎\n",
    "\n",
    "    def forward(self,\n",
    "                seq: torch.Tensor,      # (B, T, D)\n",
    "                ex_1hot: torch.Tensor   # (B, num_ex)  user-selected\n",
    "    ):\n",
    "        B, T, _ = seq.shape\n",
    "\n",
    "        # frame-wise encoding\n",
    "        feats = torch.stack(\n",
    "            [self.encoder(seq[:, t]) for t in range(T)], dim=1\n",
    "        )                                               # (B, T, 512)\n",
    "        out, _ = self.lstm(feats)                       # (B, T, 512)\n",
    "        g = out.mean(1)                                 # (B, 512)\n",
    "        # exercise embedding (context for quality / error heads only)\n",
    "        ex_e = self.ex_emb(ex_1hot)                     # (B, 64)\n",
    "        h = torch.cat([g, ex_e], dim=1)                 # (B, 576)\n",
    "        logits_q  = self.cls_head(h)                    # (B, 2)\n",
    "        err_hat   = self.err_head(h)                    # (B, N_ERR)\n",
    "        logits_ex = self.ex_head(g)                     # (B, num_ex) ← uses g\n",
    "\n",
    "        return logits_q, err_hat, logits_ex\n",
    "\n",
    "# ╰─────────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ╭───────────────────── LOAD TRAINED MODEL ───────────────────────╮\n",
    "if not CKPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"{CKPT_PATH} not found\")\n",
    "print(\"Loading model …\")\n",
    "model=torch.load(CKPT_PATH,map_location=DEVICE)\n",
    "if isinstance(model,dict):        # state_dict case\n",
    "    m=PoseQualityNetKP(IN_DIM,NUM_EX).to(DEVICE)\n",
    "    m.load_state_dict(model); model=m\n",
    "model.eval();  print(\"✓ model ready\")\n",
    "\n",
    "# ╭─────────────── MEDIAPIPE INITIALISATION ───────────────────────╮\n",
    "pose = mp_pose.Pose(False,2,False,\n",
    "                    min_detection_confidence=0.8,\n",
    "                    min_tracking_confidence=0.8)\n",
    "drawer= mp.solutions.drawing_utils\n",
    "\n",
    "# ╭─────────────────────── HELPERS ────────────────────────────────╮\n",
    "def euler_angle(p1,p2,p3):\n",
    "    a,b,c=np.array(p1),np.array(p2),np.array(p3)\n",
    "    v1=a-b; v2=c-b\n",
    "    ang= math.degrees(math.acos(\n",
    "        np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)+1e-8)))\n",
    "    return ang\n",
    "\n",
    "def world_kps_from_result(res):\n",
    "    return np.array([(lm.x,lm.y,lm.z) for lm in res.pose_world_landmarks.landmark],\n",
    "                    np.float32)\n",
    "\n",
    "def required_visible(res, ex_id):\n",
    "    req = set(JOINT_TRIPLETS.keys()) if ex_id!=3 else {\"RIGHT_ELBOW\",\"RIGHT_SHOULDER\",\"SPINE\"}\n",
    "    idx=set(); [idx.update(JOINT_TRIPLETS[j]) for j in req]\n",
    "    return all(res.pose_landmarks.landmark[i.value].visibility>=VIS_THRESH for i in idx)\n",
    "\n",
    "# ╭────────────────── CORE INFERENCE LOOP ─────────────────────────╮\n",
    "def run_live(video_src, user_ex_id):\n",
    "    last_message = \"\"\n",
    "    user_ex_name = EXERCISE_MAP[user_ex_id]\n",
    "    cap = cv2.VideoCapture(video_src)\n",
    "    if video_src==0:                       # webcam – resample to 30 fps\n",
    "        src_fps=cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        skip=max(1,round(src_fps/30))\n",
    "    else: skip=1\n",
    "    buf=deque(maxlen=SEQ_LEN)\n",
    "    feedback    = \"Initialising…\"\n",
    "    suggestion  = \"\"\n",
    "    warn_ctr    = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:                      # end-of-stream\n",
    "            break\n",
    "        if video_src==0 and cap.get(cv2.CAP_PROP_POS_FRAMES)%skip: continue\n",
    "        rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        res=pose.process(rgb)\n",
    "        if res.pose_landmarks:   drawer.draw_landmarks(\n",
    "                frame, res.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # ---------- state machine ----------\n",
    "        if not res.pose_landmarks:\n",
    "            feedback=\"No pose detected\"; buf.clear(); suggestion=\"\"\n",
    "        elif not required_visible(res,user_ex_id):\n",
    "            feedback=\"Adjust your posture\"; buf.clear(); suggestion=\"\"\n",
    "        elif not res.pose_world_landmarks:\n",
    "            feedback=\"World landmarks missing\"; buf.clear(); suggestion=\"\"\n",
    "        else:\n",
    "            buf.append(world_kps_from_result(res).flatten())\n",
    "            if len(buf)==SEQ_LEN:\n",
    "                seq=torch.tensor(np.array(buf),dtype=torch.float32,\n",
    "                                 device=DEVICE).unsqueeze(0)\n",
    "                ex_1h=F.one_hot(torch.tensor([user_ex_id-1],device=DEVICE),\n",
    "                                NUM_EX).float()\n",
    "                with torch.no_grad():\n",
    "                    log_q, err_hat, log_ex = model(seq, ex_1h)\n",
    "                q_pred = log_q.argmax(1).item()       # 0 / 1\n",
    "                ex_pred= log_ex.argmax(1).item()+1     # 1…6\n",
    "                errs   = err_hat.squeeze().cpu().numpy()\n",
    "\n",
    "                # mismatch branch ------------------------------------------------\n",
    "                if ex_pred != user_ex_id:\n",
    "                    pred_name = EXERCISE_MAP.get(ex_pred, f\"Ex {ex_pred}\")\n",
    "                    feedback = f\"Wrong exercise! It looks like you're doing {pred_name}.\"\n",
    "                    suggestion=\"\";  warn_ctr=0\n",
    "                else:\n",
    "                    if q_pred==1:\n",
    "                        feedback=\"You're on the right track!\"\n",
    "                        suggestion=\"\"; warn_ctr=0\n",
    "                    else:\n",
    "                        feedback = \"You're doing it wrongly!\"\n",
    "                        bad_idxs = np.argsort(np.abs(errs))[::-1][:3]\n",
    "                        # pick joints above threshold\n",
    "                        joints = [\n",
    "                            JOINT_LABELS[i]\n",
    "                            for i in bad_idxs\n",
    "                            if abs(errs[i]) >= TH_ERR_DEG\n",
    "                        ]\n",
    "\n",
    "                        if joints:\n",
    "                            # build “a, b and c” style list\n",
    "                            if len(joints) == 1:\n",
    "                                joint_str = joints[0]\n",
    "                            elif len(joints) == 2:\n",
    "                                joint_str = f\"{joints[0]} and {joints[1]}\"\n",
    "                            else:\n",
    "                                joint_str = f\"{joints[0]}, {joints[1]} and {joints[2]}\"\n",
    "                            suggestion = f\"Please adjust your {joint_str} properly.\"\n",
    "                        else:\n",
    "                            suggestion = \"Check your form.\"\n",
    "                        warn_ctr = 30                    # show for ~1 s\n",
    "            else:\n",
    "                feedback=f\"Analysing {len(buf)}/{SEQ_LEN}\"\n",
    "                suggestion=\"\"\n",
    "\n",
    "        # ─ speak any change ───────────────────────────────────────\n",
    "        message = (\n",
    "            feedback if not suggestion\n",
    "            else f\"{feedback}. {suggestion}\"\n",
    "        )\n",
    "        if message != last_message:\n",
    "            speak_async(message)\n",
    "            last_message = message\n",
    "            \n",
    "        # ---------------- draw text ----------------------------------\n",
    "        cv2.putText(frame,f\"Exercise {user_ex_id}: {user_ex_name}\",(15,40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "        color={\"No pose detected\":(0,0,255),\"Adjust your posture\":(0,165,255),\n",
    "               \"World landmarks missing\":(0,0,255)}.get(feedback,(0,255,0) if\n",
    "               \"You're on the right track!\" in feedback else (0,0,255))\n",
    "\n",
    "        cv2.putText(frame,feedback,(15,90),cv2.FONT_HERSHEY_SIMPLEX,1.2,\n",
    "                    color,3,cv2.LINE_AA)\n",
    "\n",
    "        if warn_ctr>0 and suggestion:\n",
    "            cv2.putText(frame,suggestion,(15,140),cv2.FONT_HERSHEY_SIMPLEX,0.9,\n",
    "                        (0,255,255),2,cv2.LINE_AA); warn_ctr-=1\n",
    "\n",
    "        cv2.imshow(\"Exercise-coach\",frame)\n",
    "        if cv2.waitKey(1)&0xFF==ord('q'): break\n",
    "\n",
    "    cap.release(); cv2.destroyAllWindows(); pose.close()\n",
    "# ╰───────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ╭────────────────────── USER PROMPTS & RUN ──────────────────────╮\n",
    "while True:\n",
    "    try:\n",
    "        ex_id=int(input(f\"Select exercise 1-{NUM_EX}: \"))\n",
    "        if 1<=ex_id<=NUM_EX: break\n",
    "    except: pass\n",
    "print(\"(0 = webcam, or drop a video path)\"); src=input(\"Video source (0 = webcam, or drop a video path): \")\n",
    "src=0 if src.strip()==\"0\" else src.strip()\n",
    "if src and src!=\"0\" and not Path(src).exists():\n",
    "    print(\"File not found\"); raise SystemExit\n",
    "run_live(src,ex_id)\n",
    "# ╰───────────────────────────────────────────────────────────────╯\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rehabtrainingpy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
