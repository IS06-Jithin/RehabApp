{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab585d8f",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a70e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0b559",
   "metadata": {},
   "source": [
    "# 2. Keypoint Extraction from Training Videos with MediaPipe  \n",
    "Before training a model for tasks like pose estimation, exercise recognition, or movement analysis, the first step is to extract relevant features from the data. In this case, the data consists of training videos, and the relevant features are pose keypoints—3D coordinates representing specific body parts (like shoulders, elbows, knees, etc.).  \n",
    "\n",
    "This code uses MediaPipe Pose to extract keypoints from training videos. These keypoints are critical for model training as they represent the underlying body movements that the model will need to learn. Below is a breakdown of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae42d0f",
   "metadata": {},
   "source": [
    "## 2.1 Keypoint extraction and saving as numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keypoint extraction from Training Videos With Mediapipe\n",
    "\n",
    "# # 0. Quiet TensorFlow/absl\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# # 1. MediaPipe Pose setup\n",
    "# pose = mp.solutions.pose.Pose(\n",
    "#     static_image_mode=False,\n",
    "#     model_complexity=2,\n",
    "#     enable_segmentation=False,\n",
    "#     min_detection_confidence=0.5,\n",
    "#     min_tracking_confidence=0.5\n",
    "# )\n",
    "\n",
    "# # 2. Paths\n",
    "# VIDEO_ROOT = Path(\"Data-REHAB24-6/videos\")\n",
    "# OUT_ROOT   = Path(\"Data-REHAB24-6/mp_keypoints\")\n",
    "# OUT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# # 3. Worker\n",
    "# def process_video(vid_path: Path):\n",
    "#     rel     = vid_path.parent.name            # e.g. \"Ex1\"\n",
    "#     out_dir = OUT_ROOT / rel\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     out_file = out_dir / f\"{vid_path.stem}-mp.npy\"\n",
    "\n",
    "#     print(f\"\\n→ Processing: {vid_path.name}\")\n",
    "#     print(f\"   From:      {vid_path.parent}\")\n",
    "#     print(f\"   To folder: {out_dir}\")\n",
    "\n",
    "#     cap    = cv2.VideoCapture(str(vid_path))\n",
    "#     frames = []\n",
    "#     count  = 0\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         count += 1\n",
    "\n",
    "#         img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         res = pose.process(img)\n",
    "#         lm  = res.pose_world_landmarks.landmark if res.pose_world_landmarks else []\n",
    "\n",
    "#         if lm:\n",
    "#             pts = [(p.x, p.y, p.z) for p in lm]\n",
    "#         else:\n",
    "#             pts = [(0.0, 0.0, 0.0)] * 33\n",
    "\n",
    "#         frames.append(pts)\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "#     arr = np.array(frames, dtype=np.float32)\n",
    "#     np.save(out_file, arr)\n",
    "\n",
    "#     print(f\"✔ Saved: {out_file}  (frames={count}, shape={arr.shape})\")\n",
    "\n",
    "# # 4. Run — only Ex1 through Ex5\n",
    "# for i in range(1, 6):\n",
    "#     ex_dir = VIDEO_ROOT / f\"Ex{i}\"\n",
    "#     if not ex_dir.is_dir():\n",
    "#         print(f\"⚠️  Skipping missing folder: {ex_dir}\")\n",
    "#         continue\n",
    "\n",
    "#     for vid in sorted(ex_dir.glob(\"*.mp4\")):\n",
    "#         try:\n",
    "#             process_video(vid)\n",
    "#         except Exception as e:\n",
    "#             print(f\"✘ Failed processing {vid.name}: {e}\")\n",
    "\n",
    "# print(\"\\nAll requested videos processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390d6ed",
   "metadata": {},
   "source": [
    "## 2.2 Inspecting at a numpy file containing 3D keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86099ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Data-REHAB24-6/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\n",
      " dtype: float32\n",
      " shape: (5191, 33, 3)  (frames × landmarks × coords)\n",
      "\n",
      "Frame #000 (33×3):\n",
      "[[-0.03379065 -0.6112996  -0.22619084]\n",
      " [-0.02455677 -0.628223   -0.2275483 ]\n",
      " [-0.0257254  -0.6304051  -0.21732828]\n",
      " [-0.02547118 -0.6302204  -0.21820049]\n",
      " [-0.02778288 -0.6363151  -0.2358914 ]\n",
      " [-0.02677054 -0.63463634 -0.24706481]\n",
      " [-0.02263773 -0.61979294 -0.2281486 ]\n",
      " [ 0.02648694 -0.6184615  -0.16859515]\n",
      " [-0.03559308 -0.56201506 -0.14808732]\n",
      " [ 0.0030987  -0.59678274 -0.19447449]\n",
      " [-0.01712019 -0.55977213 -0.21580447]\n",
      " [ 0.1250833  -0.49333623 -0.08702794]\n",
      " [-0.05722423 -0.53108674 -0.02576461]\n",
      " [ 0.15178505 -0.51440114 -0.09251688]\n",
      " [-0.17247145 -0.4994753  -0.05377672]\n",
      " [ 0.14430666 -0.54461473 -0.03090633]\n",
      " [-0.32217076 -0.5845331  -0.08351779]\n",
      " [ 0.1352469  -0.5789426  -0.02217976]\n",
      " [-0.33411983 -0.62724733 -0.13406767]\n",
      " [ 0.11857966 -0.58959824 -0.04421883]\n",
      " [-0.30619952 -0.65674794 -0.14033641]\n",
      " [ 0.13962431 -0.533594   -0.04002995]\n",
      " [-0.32007053 -0.60172904 -0.09692235]\n",
      " [ 0.05049127 -0.0018556   0.00790542]\n",
      " [-0.0485956  -0.00113658 -0.00433628]\n",
      " [ 0.11502459  0.2840918   0.04747445]\n",
      " [-0.05322852  0.24029574  0.08932951]\n",
      " [ 0.14734772  0.5199347   0.23229995]\n",
      " [-0.07118332  0.5179724   0.2826942 ]\n",
      " [ 0.13367084  0.54242045  0.2601168 ]\n",
      " [-0.07182921  0.541233    0.24959905]\n",
      " [ 0.12196927  0.57154816  0.2570243 ]\n",
      " [-0.07216683  0.572454    0.23920183]]\n",
      "  → first landmark: (-0.033790648, -0.6112996, -0.22619084)\n",
      "\n",
      "Frame #001 (33×3):\n",
      "[[-0.01134037 -0.5697082  -0.339561  ]\n",
      " [ 0.00353809 -0.5859319  -0.34309617]\n",
      " [ 0.00270209 -0.5884564  -0.33378896]\n",
      " [ 0.00201175 -0.5885314  -0.33455902]\n",
      " [-0.01271034 -0.59487945 -0.34993634]\n",
      " [-0.01165643 -0.593975   -0.36170712]\n",
      " [-0.01075809 -0.57910186 -0.3406161 ]\n",
      " [ 0.07479228 -0.5783957  -0.28210232]\n",
      " [-0.02705817 -0.537724   -0.2474346 ]\n",
      " [ 0.03136193 -0.5524706  -0.30781525]\n",
      " [-0.00508686 -0.5230155  -0.32598764]\n",
      " [ 0.1856992  -0.48319736 -0.18188453]\n",
      " [-0.05337023 -0.5305265  -0.10725395]\n",
      " [ 0.30533957 -0.5076612  -0.24926643]\n",
      " [-0.18747927 -0.47846928 -0.1686121 ]\n",
      " [ 0.40025973 -0.51844925 -0.28818202]\n",
      " [-0.3567456  -0.5400336  -0.23759778]\n",
      " [ 0.4252214  -0.5436983  -0.29650414]\n",
      " [-0.37466687 -0.5707025  -0.28731832]\n",
      " [ 0.4044413  -0.5539673  -0.32024327]\n",
      " [-0.34815052 -0.60103    -0.30733863]\n",
      " [ 0.39781106 -0.5031106  -0.30399245]\n",
      " [-0.35498336 -0.55496377 -0.2580629 ]\n",
      " [ 0.07222402 -0.00160784  0.0128601 ]\n",
      " [-0.06938923 -0.00155623 -0.00999647]\n",
      " [ 0.08928943  0.31117028  0.06024915]\n",
      " [-0.10052951  0.25218678  0.05140337]\n",
      " [ 0.1335875   0.57052726  0.22892243]\n",
      " [-0.15915288  0.5619467   0.21901147]\n",
      " [ 0.12346359  0.590925    0.25226566]\n",
      " [-0.16072083  0.5869104   0.19796352]\n",
      " [ 0.13690576  0.60883754  0.23850642]\n",
      " [-0.17555799  0.603654    0.1471466 ]]\n",
      "  → first landmark: (-0.0113403695, -0.5697082, -0.339561)\n",
      "\n",
      "Frame #002 (33×3):\n",
      "[[-0.0168165  -0.57375824 -0.36536366]\n",
      " [-0.00349796 -0.58968115 -0.36685658]\n",
      " [-0.00404231 -0.5920712  -0.3569905 ]\n",
      " [-0.00467073 -0.5923622  -0.35750297]\n",
      " [-0.01410367 -0.5981765  -0.3782469 ]\n",
      " [-0.01414973 -0.5977909  -0.3914376 ]\n",
      " [-0.01312186 -0.5821221  -0.37337747]\n",
      " [ 0.05679294 -0.572806   -0.29834253]\n",
      " [-0.0476275  -0.5310251  -0.2819756 ]\n",
      " [ 0.02337127 -0.55599713 -0.3263316 ]\n",
      " [-0.0054874  -0.5257816  -0.3534069 ]\n",
      " [ 0.17624158 -0.4816474  -0.20698948]\n",
      " [-0.09889796 -0.52954847 -0.11612497]\n",
      " [ 0.32004458 -0.51815015 -0.27747568]\n",
      " [-0.2538115  -0.42914388 -0.22348595]\n",
      " [ 0.479071   -0.5206693  -0.31443927]\n",
      " [-0.4472741  -0.45803884 -0.3754344 ]\n",
      " [ 0.5139305  -0.5360094  -0.3153151 ]\n",
      " [-0.47330606 -0.47658482 -0.43610725]\n",
      " [ 0.490114   -0.54833627 -0.3450243 ]\n",
      " [-0.44490942 -0.51193917 -0.46239042]\n",
      " [ 0.4791889  -0.50126463 -0.33063355]\n",
      " [-0.44496435 -0.465993   -0.40077347]\n",
      " [ 0.08651342 -0.00139352  0.02232434]\n",
      " [-0.08341713 -0.00188619 -0.01804559]\n",
      " [ 0.14964634  0.36578354  0.05097246]\n",
      " [-0.11824293  0.26294428 -0.02509112]\n",
      " [ 0.20363103  0.61495334  0.19673198]\n",
      " [-0.19085191  0.5981499   0.14407939]\n",
      " [ 0.19493678  0.6042932   0.21519588]\n",
      " [-0.19582     0.62702686  0.12640719]\n",
      " [ 0.23405242  0.69470036  0.18596604]\n",
      " [-0.22132283  0.64795923  0.03823205]]\n",
      "  → first landmark: (-0.0168165, -0.57375824, -0.36536366)\n",
      "\n",
      "Overall coordinate stats:\n",
      "  x: min=-0.633, max=0.785, mean=0.061\n",
      "  y: min=-0.686, max=0.848, mean=-0.109\n",
      "  z: min=-0.742, max=0.672, mean=-0.146\n"
     ]
    }
   ],
   "source": [
    "# ─── edit this to your target file ────────────────────────────────────────────\n",
    "file_path = Path(\"Data-REHAB24-6/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# load\n",
    "arr = np.load(file_path)\n",
    "\n",
    "# basic info\n",
    "print(f\"Loaded: {file_path}\")\n",
    "print(f\" dtype: {arr.dtype}\")\n",
    "print(f\" shape: {arr.shape}  (frames × landmarks × coords)\\n\")\n",
    "\n",
    "# show first few frames\n",
    "n_show = min(3, arr.shape[0])\n",
    "for i in range(n_show):\n",
    "    print(f\"Frame #{i:03d} (33×3):\")\n",
    "    print(arr[i])\n",
    "    print(f\"  → first landmark: {tuple(arr[i,0])}\\n\")\n",
    "\n",
    "# overall statistics\n",
    "print(\"Overall coordinate stats:\")\n",
    "for idx, name in enumerate((\"x\", \"y\", \"z\")):\n",
    "    data = arr[..., idx]\n",
    "    print(f\"  {name}: min={data.min():.3f}, max={data.max():.3f}, mean={data.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a70d4",
   "metadata": {},
   "source": [
    "# 3. Data Preparation for Pose Error Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522482d6",
   "metadata": {},
   "source": [
    "## 3.1 Change the correctness of half profile vidoes to incorrect \n",
    "\n",
    "This change is being made because the half-profile camera angle is considered insufficient or unreliable for accurately determining if the exercise repetition was performed correctly according to the established standards. Furthermore, for model inference, the intention is to use only the front-facing view. Therefore, this action serves as a data cleaning and preparation step to improve the quality and consistency of the data that will be used for subsequent analysis or modeling, ensuring that only reliably assessed repetitions are marked as 'correct'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ac9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration ---\n",
    "# DATA_ROOT = Path(\"Data-REHAB24-6\") # Adjust if your data folder has a different relative path\n",
    "# ORIGINAL_FILENAME = \"Segmentation_original.xlsx\"\n",
    "# # --- CHANGE HERE: Update the output filename extension ---\n",
    "# NEW_FILENAME = \"Segmentation.xlsx\" # Saving as .xlsx\n",
    "\n",
    "# FILE_ORIG = DATA_ROOT / ORIGINAL_FILENAME\n",
    "# FILE_DEST = DATA_ROOT / NEW_FILENAME\n",
    "\n",
    "# # --- Processing ---\n",
    "# if not DATA_ROOT.is_dir():\n",
    "#     print(f\"Error: Data directory not found at '{DATA_ROOT.resolve()}'\")\n",
    "# elif not FILE_ORIG.is_file():\n",
    "#     print(f\"Error: Original file not found at '{FILE_ORIG.resolve()}'\")\n",
    "# else:\n",
    "#     print(f\"Loading original file: {FILE_ORIG}\")\n",
    "#     try:\n",
    "#         # Load the Excel file\n",
    "#         df = pd.read_excel(FILE_ORIG)\n",
    "#         print(\"Original file loaded successfully.\")\n",
    "#         print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "#         # Identify rows where 'cam17_orientation' is 'half-profile'\n",
    "#         condition = df['cam17_orientation'] == 'half-profile'\n",
    "#         num_rows_to_change = condition.sum()\n",
    "#         print(f\"Found {num_rows_to_change} rows where 'cam17_orientation' is 'half-profile'.\")\n",
    "\n",
    "#         if num_rows_to_change > 0:\n",
    "#             # Change 'correctness' to 0 for matched rows\n",
    "#             print(\"Updating 'correctness' column to 0 for matched rows...\")\n",
    "#             df.loc[condition, 'correctness'] = 0\n",
    "#             print(\"Update complete.\")\n",
    "#         else:\n",
    "#             print(\"No rows matched the condition. 'correctness' column remains unchanged.\")\n",
    "\n",
    "#         # Save the modified DataFrame to a new Excel file (.xlsx)\n",
    "#         print(f\"Saving modified data to: {FILE_DEST}\")\n",
    "#         try:\n",
    "#             # Saving to .xlsx uses openpyxl engine by default (pip install openpyxl if needed)\n",
    "#             df.to_excel(FILE_DEST, index=False)\n",
    "#             print(f\"Successfully saved modified data to '{FILE_DEST.resolve()}'\")\n",
    "#         except Exception as save_error:\n",
    "#             # --- CHANGE HERE: Updated error message for .xlsx ---\n",
    "#             print(f\"Error saving file to {FILE_DEST}: {save_error}\")\n",
    "#             print(\"Saving to .xlsx format typically requires the 'openpyxl' package. Try: pip install openpyxl\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#          print(f\"Error: Make sure the file exists at {FILE_ORIG.resolve()}\")\n",
    "#     except KeyError as e:\n",
    "#          print(f\"Error: Column not found - {e}. Please check column names in '{ORIGINAL_FILENAME}'.\")\n",
    "#          print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred during processing: {e}\")\n",
    "\n",
    "# # --- Verification ---\n",
    "# if FILE_DEST.is_file():\n",
    "#     print(\"\\nVerifying the saved file...\")\n",
    "#     try:\n",
    "#         # --- CHANGE HERE: Reading the .xlsx file for verification ---\n",
    "#         df_new = pd.read_excel(FILE_DEST)\n",
    "#         print(f\"Loaded new file shape: {df_new.shape}\")\n",
    "#         check_condition = df_new['cam17_orientation'] == 'half-profile'\n",
    "#         incorrect_rows = df_new.loc[check_condition & (df_new['correctness'] != 0)]\n",
    "#         if incorrect_rows.empty:\n",
    "#             print(\"Verification successful: All 'half-profile' rows have 'correctness' set to 0.\")\n",
    "#         else:\n",
    "#             print(\"Verification FAILED: Some 'half-profile' rows still have 'correctness' != 0.\")\n",
    "#             print(incorrect_rows)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during verification: {e}\")\n",
    "# else:\n",
    "#      print(f\"\\nCould not verify as the destination file '{FILE_DEST}' was not found or not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed5455",
   "metadata": {},
   "source": [
    "## 3.2 Sliding Window-Based Pose Error Calculation for Video segments  \n",
    "This script processes keypoint data from videos to compute errors in body joint angles relative to \"ideal\" angles, and then generates sliding windows of these errors. This is done to prepare features that will later be used for model training in tasks like exercise recognition or pose correction.\n",
    "\n",
    "What is done: For each exercise in the dataset, the script calculates the \"ideal\" joint angles by selecting the middle frame of each correct repetition (based on the correctness label). It calculates the angles between the joints defined in JOINT_TRIPLETS for each of these frames.  We use 17 out of the 33 MediaPipe landmarks for the ideal joint angle calculation. The used mediapipe landmarks are: NOSE, LEFT_SHOULDER, RIGHT_SHOULDER, LEFT_ELBOW, RIGHT_ELBOW, LEFT_WRIST, RIGHT_WRIST, LEFT_INDEX, RIGHT_INDEX, LEFT_HIP, RIGHT_HIP, LEFT_KNEE, RIGHT_KNEE, LEFT_ANKLE, RIGHT_ANKLE, LEFT_FOOT_INDEX, and RIGHT_FOOT_INDEX. These cover the major joints and end‑effectors (shoulders through wrists and hips through ankles, plus the spine/head via the nose) needed to compute all our angle‑based error metrics for the six rehab exercises. The 16 unused landmarks are all the fine‑grain facial points (inner/outer eyes, ears, mouth corners), the pinky and thumb tips, and the heel points. Since our focus is on gross limb alignment (arm and leg joint planes) rather than facial expression, finger articulation, or detailed foot posture, those landmarks don’t contribute to correcting the targeted movements and so are omitted.\n",
    "\n",
    "The median angle is then computed for each joint across the correct repetitions of the exercise. These median values represent the \"ideal\" angles the model should aim for in perfect executions of the exercise.  \n",
    "\n",
    "In the next step, the script generates sliding windows of angular errors, calculated as the difference between the observed angles in the video and the ideal angles. These windows contain temporal sequences of error data that are used as features for model training. The windowed data is then saved in a CSV file, which includes additional metadata such as exercise ID, repetition number, and frame indices.  \n",
    "\n",
    "Why it’s done: Calculating the ideal angles for each exercise provides a reference for identifying errors during subsequent video frames. These ideal angles will serve as the baseline for assessing whether a movement is performed correctly or incorrectly. By generating and saving the sliding window data, the script prepares the dataset for supervised learning, allowing the model to analyze temporal error patterns over a series of frames. This windowed approach is crucial for the model to learn dynamic movements and classify whether exercises are performed correctly based on the computed joint angles. The segmentation data in the CSV file offers a structured and labeled dataset, which helps in efficient training and evaluation of the model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. helpers --------------------------------------------------\n",
    "# def angle_between(a,b,c):\n",
    "#     BA = a-b; BC = c-b\n",
    "#     cosθ = np.dot(BA,BC)/(np.linalg.norm(BA)*np.linalg.norm(BC))\n",
    "#     return math.degrees(math.acos(np.clip(cosθ,-1,1)))\n",
    "\n",
    "# PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "# JOINT_TRIPLETS = {\n",
    "#     \"LEFT_ELBOW\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "#                      PoseLandmark.LEFT_ELBOW.value,\n",
    "#                      PoseLandmark.LEFT_WRIST.value),\n",
    "#     \"RIGHT_ELBOW\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "#                      PoseLandmark.RIGHT_ELBOW.value,\n",
    "#                      PoseLandmark.RIGHT_WRIST.value),\n",
    "#     \"LEFT_SHOULDER\":  (PoseLandmark.LEFT_ELBOW.value,\n",
    "#                        PoseLandmark.LEFT_SHOULDER.value,\n",
    "#                        PoseLandmark.LEFT_HIP.value),\n",
    "#     \"RIGHT_SHOULDER\": (PoseLandmark.RIGHT_ELBOW.value,\n",
    "#                        PoseLandmark.RIGHT_SHOULDER.value,\n",
    "#                        PoseLandmark.RIGHT_HIP.value),\n",
    "#     \"LEFT_HIP\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "#                    PoseLandmark.LEFT_HIP.value,\n",
    "#                    PoseLandmark.LEFT_KNEE.value),\n",
    "#     \"RIGHT_HIP\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "#                    PoseLandmark.RIGHT_HIP.value,\n",
    "#                    PoseLandmark.RIGHT_KNEE.value),\n",
    "#     \"LEFT_KNEE\":  (PoseLandmark.LEFT_HIP.value,\n",
    "#                   PoseLandmark.LEFT_KNEE.value,\n",
    "#                   PoseLandmark.LEFT_ANKLE.value),\n",
    "#     \"RIGHT_KNEE\": (PoseLandmark.RIGHT_HIP.value,\n",
    "#                   PoseLandmark.RIGHT_KNEE.value,\n",
    "#                   PoseLandmark.RIGHT_ANKLE.value),\n",
    "#     \"SPINE\": (\n",
    "#        PoseLandmark.LEFT_HIP.value,       \n",
    "#        PoseLandmark.LEFT_SHOULDER.value,   \n",
    "#        PoseLandmark.RIGHT_SHOULDER.value   \n",
    "#     ),\n",
    "#     \"HEAD\": (\n",
    "#        PoseLandmark.LEFT_SHOULDER.value,\n",
    "#        PoseLandmark.NOSE.value,\n",
    "#        PoseLandmark.RIGHT_SHOULDER.value\n",
    "#     ),\n",
    "#      \"LEFT_WRIST\":  (\n",
    "#         PoseLandmark.LEFT_ELBOW.value,\n",
    "#         PoseLandmark.LEFT_WRIST.value,\n",
    "#         PoseLandmark.LEFT_INDEX.value\n",
    "#     ),\n",
    "#     \"RIGHT_WRIST\": (\n",
    "#         PoseLandmark.RIGHT_ELBOW.value,\n",
    "#         PoseLandmark.RIGHT_WRIST.value,\n",
    "#         PoseLandmark.RIGHT_INDEX.value\n",
    "#     ),\n",
    "#     \"LEFT_ANKLE\":  (\n",
    "#         PoseLandmark.LEFT_KNEE.value,\n",
    "#         PoseLandmark.LEFT_ANKLE.value,\n",
    "#         PoseLandmark.LEFT_FOOT_INDEX.value\n",
    "#     ),\n",
    "#     \"RIGHT_ANKLE\": (\n",
    "#         PoseLandmark.RIGHT_KNEE.value,\n",
    "#         PoseLandmark.RIGHT_ANKLE.value,\n",
    "#         PoseLandmark.RIGHT_FOOT_INDEX.value\n",
    "#     ),\n",
    "    \n",
    "# }\n",
    "# ERR_JOINTS = list(JOINT_TRIPLETS.keys())\n",
    "# N_ERR = len(ERR_JOINTS)  # 14\n",
    "\n",
    "# # 2. load original metadata & keypoints -----------------------\n",
    "# DATA_ROOT    = Path(\"Data-REHAB24-6\")\n",
    "# KEYPT_ROOT   = DATA_ROOT/\"mp_keypoints\"\n",
    "# META_ORIG    = DATA_ROOT/\"Segmentation.xlsx\"\n",
    "# df           = pd.read_excel(META_ORIG, engine=\"openpyxl\")\n",
    "# df.columns   = df.columns.str.strip()\n",
    "\n",
    "# # 3. compute ideal_angles on correct reps ----------------------\n",
    "# ideal_angles = {}\n",
    "# correct = df[df.correctness==1]\n",
    "# for ex in correct.exercise_id.unique():\n",
    "#     all_ang = {jn:[] for jn in ERR_JOINTS}\n",
    "#     for _,r in correct[correct.exercise_id==ex].iterrows():\n",
    "#         vid, f0, f1 = r.video_id, int(r.first_frame), int(r.last_frame)\n",
    "#         files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "#         if not files: continue\n",
    "#         arr = np.load(files[0])\n",
    "#         seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "#         if len(seg)==0: continue\n",
    "#         mid = len(seg)//2\n",
    "#         frm = seg[mid]\n",
    "#         for jn in ERR_JOINTS:\n",
    "#             ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "#             ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "#             all_ang[jn].append(ang)\n",
    "#     # median\n",
    "#     ideal_angles[ex] = {jn:float(np.median(all_ang[jn])) for jn in all_ang if all_ang[jn]}\n",
    "\n",
    "# # 4. slide windows & write rows --------------------------------\n",
    "# WINDOW, STRIDE = 16, 8\n",
    "# rows = []\n",
    "# for _,r in df.iterrows():\n",
    "#     vid, ex, f0, f1 = r.video_id, int(r.exercise_id), int(r.first_frame), int(r.last_frame)\n",
    "#     files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "#     if not files: continue\n",
    "#     arr = np.load(files[0])                # (F,33,3)\n",
    "#     seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "#     if len(seg)<WINDOW: continue\n",
    "\n",
    "#     # per-frame errors\n",
    "#     pf_err = {jn:[] for jn in ERR_JOINTS}\n",
    "#     for frm in seg:\n",
    "#         for jn in ERR_JOINTS:\n",
    "#             ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "#             ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "#             pf_err[jn].append(ang - ideal_angles[ex].get(jn,ang))\n",
    "\n",
    "#     # slide\n",
    "#     for start in range(0, len(seg)-WINDOW+1, STRIDE):\n",
    "#         w = np.array([ pf_err[jn][start:start+WINDOW] for jn in ERR_JOINTS ])  # (10,WINDOW)\n",
    "#         mean_err = w.mean(axis=1)\n",
    "#         row = {\n",
    "#             \"video_id\":vid,\n",
    "#             \"exercise_id\":ex,\n",
    "#             \"repetition_number\":r.repetition_number,\n",
    "#             \"window_start\": f0+start,\n",
    "#             \"window_end\":   f0+start+WINDOW,\n",
    "#             \"correctness\":  r.correctness\n",
    "#         }\n",
    "#         for i,jn in enumerate(ERR_JOINTS):\n",
    "#             row[f\"err_{i}\"] = float(mean_err[i])\n",
    "#         rows.append(row)\n",
    "\n",
    "# win_df = pd.DataFrame(rows)\n",
    "# win_df.to_csv(DATA_ROOT/\"Segmentation_windows.csv\", index=False)\n",
    "# print(\"Wrote\", len(win_df), \"windows to Segmentation_windows.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ea133",
   "metadata": {},
   "source": [
    "# 4. Traning Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa56f5d",
   "metadata": {},
   "source": [
    "## 4.1 Paths & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463f3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SCRIPT_DIR    = Path().resolve()\n",
    "DATA_ROOT     = SCRIPT_DIR/\"Data-REHAB24-6\"\n",
    "WIN_CSV       = DATA_ROOT/\"Segmentation_windows.csv\"\n",
    "KEYPT_ROOT    = DATA_ROOT/\"mp_keypoints\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"► Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580cd2b",
   "metadata": {},
   "source": [
    "## 4.2 Analysing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2629d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: /Users/jithinkrishnan/Documents/Study/IS06 /MVP/RehabApp/model-training-scripts/Data-REHAB24-6/Segmentation.xlsx\n",
      "Successfully loaded 'Segmentation.xlsx'. Shape: (1072, 13)\n",
      "\n",
      "Analyzing 'correctness' column distribution...\n",
      "\n",
      "Class Counts:\n",
      "correctness\n",
      "0    751\n",
      "1    321\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of 'Correct' (1) instances: 321\n",
      "Number of 'Wrong' (0) instances:   751\n",
      "Total analyzed instances:        1072\n",
      "\n",
      "Percentage 'Correct': 29.94%\n",
      "Percentage 'Wrong':   70.06%\n",
      "\n",
      "WARNING: The dataset appears significantly imbalanced.\n"
     ]
    }
   ],
   "source": [
    "# --- Specify the file to analyze ---\n",
    "# Use the file that was generated in the previous step (after modification)\n",
    "DATA_FILENAME = \"Segmentation.xlsx\"\n",
    "FILE_TO_ANALYZE = DATA_ROOT / DATA_FILENAME\n",
    "\n",
    "print(f\"Attempting to load data from: {FILE_TO_ANALYZE}\")\n",
    "\n",
    "# --- Check if file exists and process ---\n",
    "if not FILE_TO_ANALYZE.is_file():\n",
    "    print(f\"Error: The file '{FILE_TO_ANALYZE.name}' was not found in '{DATA_ROOT}'.\")\n",
    "    print(\"Please ensure the previous script ran successfully and saved the file.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the dataframe\n",
    "        df = pd.read_excel(FILE_TO_ANALYZE)\n",
    "        print(f\"Successfully loaded '{FILE_TO_ANALYZE.name}'. Shape: {df.shape}\")\n",
    "\n",
    "        # Check if the 'correctness' column exists\n",
    "        if 'correctness' not in df.columns:\n",
    "            print(f\"Error: 'correctness' column not found in the dataframe.\")\n",
    "            print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "        else:\n",
    "            print(\"\\nAnalyzing 'correctness' column distribution...\")\n",
    "\n",
    "            # Get the counts for each value in the 'correctness' column\n",
    "            class_counts = df['correctness'].value_counts()\n",
    "\n",
    "            # --- Report the counts ---\n",
    "            print(\"\\nClass Counts:\")\n",
    "            print(class_counts)\n",
    "\n",
    "            # Provide a more descriptive output\n",
    "            correct_count = class_counts.get(1, 0) # Get count for value 1, default to 0 if not present\n",
    "            wrong_count = class_counts.get(0, 0)   # Get count for value 0, default to 0 if not present\n",
    "            total_count = correct_count + wrong_count # Or use len(df) if there are only 0s and 1s\n",
    "\n",
    "            print(f\"\\nNumber of 'Correct' (1) instances: {correct_count}\")\n",
    "            print(f\"Number of 'Wrong' (0) instances:   {wrong_count}\")\n",
    "            print(f\"Total analyzed instances:        {total_count}\") # Good sanity check\n",
    "\n",
    "            # --- Assess Balance ---\n",
    "            if total_count > 0:\n",
    "                correct_percentage = (correct_count / total_count) * 100\n",
    "                wrong_percentage = (wrong_count / total_count) * 100\n",
    "                print(f\"\\nPercentage 'Correct': {correct_percentage:.2f}%\")\n",
    "                print(f\"Percentage 'Wrong':   {wrong_percentage:.2f}%\")\n",
    "\n",
    "                # Basic balance assessment (you can adjust the threshold)\n",
    "                if abs(correct_percentage - wrong_percentage) < 20: # e.g., less than 20% difference (60/40 split)\n",
    "                    print(\"\\nThe dataset appears relatively balanced based on this threshold.\")\n",
    "                elif abs(correct_percentage - wrong_percentage) < 40: # e.g., less than 40% difference (70/30 split)\n",
    "                     print(\"\\nThe dataset shows moderate imbalance.\")\n",
    "                else:\n",
    "                    print(\"\\nWARNING: The dataset appears significantly imbalanced.\")\n",
    "            else:\n",
    "                print(\"\\nCannot assess balance: No instances found in the 'correctness' column.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during file loading or analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cb40c",
   "metadata": {},
   "source": [
    "### Note: Balacing of the dataset will be done duting model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bfd1e",
   "metadata": {},
   "source": [
    "## 4.3 Joint names setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97531384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOINT_NAMES: ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
      "N_JOINTS: 33\n"
     ]
    }
   ],
   "source": [
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "\n",
    "# Then:\n",
    "JOINT_NAMES = [lm.name for lm in PoseLandmark]\n",
    "N_JOINTS    = len(JOINT_NAMES)  # should be 33\n",
    "\n",
    "print(f\"JOINT_NAMES: {JOINT_NAMES}\")\n",
    "print(f\"N_JOINTS: {N_JOINTS}\")\n",
    "\n",
    "#  Exerciseses (Ex1…Ex6)\n",
    "NUM_EXERCISES = 6\n",
    "CKPT_FILE     = \"kp_pose_quality_windows_ex.pt\"  \n",
    "\n",
    "ERR_JOINTS   = [\n",
    "  \"LEFT_ELBOW\",\"RIGHT_ELBOW\",\n",
    "  \"LEFT_SHOULDER\",\"RIGHT_SHOULDER\",\n",
    "  \"LEFT_HIP\",\"RIGHT_HIP\",\n",
    "  \"LEFT_KNEE\",\"RIGHT_KNEE\",\n",
    "  \"SPINE\",\"HEAD\",\n",
    "  \"LEFT_WRIST\", \"RIGHT_WRIST\",\n",
    "  \"LEFT_ANKLE\", \"RIGHT_ANKLE\"\n",
    "]\n",
    "N_ERR = len(ERR_JOINTS)   # 14\n",
    "ERR_COLS = [f\"err_{i}\" for i in range(N_ERR)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583456b",
   "metadata": {},
   "source": [
    "## 4.4 Dataset class definition\n",
    "\n",
    "The KeypointWindowDataset class loads and processes pose keypoint data from videos for model training. It reads a CSV file containing metadata, including video IDs, exercise IDs, frame indices, and pre-calculated joint angle errors (ranging from 0 to 9). The data is sorted based on video ID, repetition number, and window start. For each sample, it loads the corresponding keypoint data (in .npy format), extracts a segment of frames based on the start and end indices, reshapes the keypoints into a 2D array, and converts them into a PyTorch tensor. It also retrieves the correctness label and the pre-calculated error values, which are stored in tensors. This class efficiently loads and processes the data in batches for training tasks like exercise recognition, where both pose keypoints and error features are used for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a911cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KeypointWindowDataset(Dataset):\n",
    "    def __init__(self, csv_file: Path, keypt_root: Path):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df.sort_values([\"video_id\",\"repetition_number\",\"window_start\"])\n",
    "        self.rows = df.to_dict(\"records\")\n",
    "        self.keypt_root = keypt_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        r   = self.rows[i]\n",
    "        ex  = int(r[\"exercise_id\"]) - 1      # zero‐based [0..NUM_EXERCISES-1]\n",
    "        vid = r[\"video_id\"]\n",
    "        f0, f1 = int(r[\"window_start\"]), int(r[\"window_end\"])\n",
    "\n",
    "        # load keypoints\n",
    "        arr = np.load(\n",
    "            next((self.keypt_root/f\"Ex{ex+1}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "        )  # shape (F,33,3)\n",
    "\n",
    "        seg = arr[f0:f1]            # (T, 33, 3)\n",
    "        seg = seg.reshape(len(seg), -1)  # (T, 99)\n",
    "        seq = torch.from_numpy(seg).float()\n",
    "\n",
    "        label = torch.tensor(r[\"correctness\"], dtype=torch.long)\n",
    "        err   = torch.tensor([r[f\"err_{j}\"] for j in range(N_ERR)],\n",
    "                             dtype=torch.float32)\n",
    "\n",
    "        return seq, label, err, ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81848f",
   "metadata": {},
   "source": [
    "## 4.4 Model definitions   \n",
    "\n",
    "1. KeypointEncoder Class: Feature Extraction  \n",
    "The KeypointEncoder class is responsible for extracting feature representations from the input keypoint data. It uses two 1D convolutional layers (conv1 and conv2) to process the input sequence of keypoints. The input tensor, which represents keypoints for each frame in a video, is passed through these convolutional layers after being reshaped to fit the 1D convolution. Each convolution layer is followed by a ReLU activation function to introduce non-linearity. The final step of the encoder involves an adaptive average pooling (pool), which reduces the feature map to a single value per feature channel. This results in a compact representation of the keypoint sequence, which is then passed forward for further processing.  \n",
    "\n",
    "2. PoseQualityNetKP Class: Overview  \n",
    "The PoseQualityNetKP class is the main model used for pose quality assessment. It integrates the KeypointEncoder to process the raw keypoint data and extracts meaningful features. The model then uses an LSTM (Long Short-Term Memory) network to learn the temporal dependencies between the keypoint sequences. The LSTM consists of two bidirectional layers, allowing the model to capture information from both past and future frames in the sequence. The LSTM outputs a sequence of hidden states, which are averaged across the time dimension to produce a fixed-size feature vector representing the entire sequence of frames. This vector, along with the exercise embedding, is used to make predictions.  \n",
    "\n",
    "3. Exercise Embedding and Final Layers  \n",
    "In addition to the keypoint features, the model incorporates an exercise-specific embedding to capture the variations between different exercises. The ex_emb layer processes the one-hot encoded exercise ID into a dense representation. This embedding is passed through a small multi-layer perceptron (MLP) that reduces the embedding size, enabling the model to focus on the most important characteristics of each exercise. The concatenation of the temporal features from the LSTM and the exercise embedding forms the final input to the classification and error prediction heads. These final heads, cls_head and err_head, are fully connected layers that output the classification of the exercise and the pose errors, respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a49b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model definitions\n",
    "class KeypointEncoder(nn.Module):\n",
    "    def __init__(self, in_dim:int, embed:int=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, embed, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, D); treat as (B, D, 1) for Conv1d\n",
    "        x = x.unsqueeze(2)                 # → (B, D, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return self.pool(x).squeeze(-1)    # → (B, embed)\n",
    "\n",
    "class PoseQualityNetKP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 num_ex: int,\n",
    "                 hidden: int = 256,\n",
    "                 ex_emb: int = 64):\n",
    "        super().__init__()\n",
    "        # keypoint feature extractor\n",
    "        self.encoder = KeypointEncoder(in_dim)\n",
    "\n",
    "        # sequence model\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        feat_dim = hidden * 2\n",
    "\n",
    "        # exercise embedding MLP\n",
    "        self.ex_emb = nn.Sequential(\n",
    "            nn.Linear(num_ex, ex_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ex_emb, ex_emb)\n",
    "        )\n",
    "\n",
    "        # final heads\n",
    "        self.cls_head = nn.Linear(feat_dim + ex_emb, 2)\n",
    "        self.err_head = nn.Linear(feat_dim + ex_emb, N_ERR)\n",
    "\n",
    "    def forward(self,\n",
    "                seq:     torch.Tensor,  # (B, T, D)\n",
    "                ex_1hot: torch.Tensor   # (B, num_ex)\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # 1) keypoint → sequence feats\n",
    "        # encode each frame\n",
    "        B,T,_ = seq.shape\n",
    "        feats = torch.stack([\n",
    "            self.encoder(seq[:,t]) for t in range(T)\n",
    "        ], dim=1)                                # (B, T, 512)\n",
    "        out, _ = self.lstm(feats)                # (B, T, 2*hidden)\n",
    "        g = out.mean(1)                          # (B, 2*hidden)\n",
    "\n",
    "        # 2) exercise embed\n",
    "        ex_e = self.ex_emb(ex_1hot)              # (B, ex_emb)\n",
    "\n",
    "        # 3) concat and heads\n",
    "        h = torch.cat([g, ex_e], dim=1)          # (B, feat_dim+ex_emb)\n",
    "        return self.cls_head(h), self.err_head(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc70cc",
   "metadata": {},
   "source": [
    "# 5. Model Training  \n",
    "\n",
    "The PoseQualityNetKP model is engineered for pose quality assessment, leveraging keypoint data to evaluate the correctness of exercise movements. It comprises a KeypointEncoder to extract robust features from keypoint sequences, a bidirectional LSTM to model temporal relationships across frames, an exercise-specific embedding layer to account for exercise variations, and two output heads for classification and joint error prediction.\n",
    "\n",
    "### Model Architecture and Processing\n",
    "\n",
    "The model processes input keypoint sequences and one-hot encoded exercise IDs. The KeypointEncoder applies 1D convolutional layers to each frame of the keypoint sequence, generating per-frame feature representations. These features are fed into a bidirectional LSTM, which captures both forward and backward temporal dependencies, producing a sequence of contextualized features. The LSTM outputs are averaged over time to yield a fixed-size feature vector for the entire sequence.\n",
    "\n",
    "Simultaneously, the exercise-specific embedding MLP transforms the one-hot encoded exercise IDs into a dense embedding, capturing exercise-specific characteristics. The temporal features from the LSTM and the exercise embeddings are concatenated and passed to two heads:  \n",
    "- Classification Head (cls_head): Predicts whether the exercise repetition is \"Correct\" (1) or \"Wrong\" (0).\n",
    "- Error Head (err_head): Estimates pose errors for key joints by predicting deviations from ideal joint angles.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The model is trained to optimize two loss functions:\n",
    "- Cross-Entropy Loss (loss_cls): Used for binary classification of exercise correctness. To address the significant class imbalance (70.06% \"Wrong\" vs. 29.94% \"Correct\"), the loss is weighted inversely proportional to class frequencies, assigning higher penalties to misclassifications of the minority \"Correct\" class.\n",
    "- Smooth L1 Loss (loss_err): Minimizes the difference between predicted and ground-truth joint angle errors, weighted by a factor of 0.1 to balance its contribution relative to the classification loss.\n",
    "\n",
    "The training data is processed using the KeypointWindowDataset, which loads keypoint sequences and labels from a CSV file and keypoint directory. To further mitigate class imbalance, the training DataLoader employs a WeightedRandomSampler to oversample the minority \"Correct\" class, ensuring balanced exposure to both classes during training. The validation set remains unsampled to provide unbiased performance metrics.\n",
    "\n",
    "The Adam optimizer, with a learning rate of 1e-4, updates model parameters to minimize the combined loss. Training proceeds over multiple epochs, with each epoch processing batches of data. During validation, the model is evaluated on a separate validation set using metrics such as accuracy, weighted F1-score, per-class precision and recall (for both \"Wrong\" and \"Correct\" classes), and mean absolute error (MAE) for joint error predictions. Per-class metrics help monitor performance on the minority \"Correct\" class, which is critical due to the imbalance.\n",
    "\n",
    "### Model Selection and Saving\n",
    "\n",
    "The model with the highest weighted F1-score on the validation set is saved, along with its state dictionary, ensuring the best-performing model is retained for deployment. This training framework, enhanced with class-weighted loss and oversampling, enables robust pose quality assessment, improving classification accuracy for both classes and providing precise joint error analysis for exercise feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1694810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 629/629 [00:13<00:00, 47.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 2.2208\n",
      "  ↳ val acc 0.740, Precision (0,1) 1.000,0.543, Recall (0,1) 0.621,1.000, F1 0.748, MAE° 17.67\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 629/629 [00:12<00:00, 49.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.9653\n",
      "  ↳ val acc 0.745, Precision (0,1) 1.000,0.547, Recall (0,1) 0.628,1.000, F1 0.753, MAE° 17.13\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 629/629 [00:12<00:00, 48.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.9391\n",
      "  ↳ val acc 0.769, Precision (0,1) 0.994,0.572, Recall (0,1) 0.668,0.989, F1 0.776, MAE° 16.87\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.776)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 629/629 [00:13<00:00, 48.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.8953\n",
      "  ↳ val acc 0.781, Precision (0,1) 0.979,0.585, Recall (0,1) 0.696,0.961, F1 0.789, MAE° 16.48\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.789)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 629/629 [00:13<00:00, 47.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.8416\n",
      "  ↳ val acc 0.790, Precision (0,1) 0.998,0.595, Recall (0,1) 0.697,0.997, F1 0.798, MAE° 15.92\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.798)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 629/629 [00:13<00:00, 47.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.7538\n",
      "  ↳ val acc 0.794, Precision (0,1) 1.000,0.599, Recall (0,1) 0.699,1.000, F1 0.801, MAE° 15.09\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 629/629 [00:12<00:00, 49.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.6621\n",
      "  ↳ val acc 0.783, Precision (0,1) 1.000,0.587, Recall (0,1) 0.683,1.000, F1 0.790, MAE° 14.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 629/629 [00:12<00:00, 49.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.5865\n",
      "  ↳ val acc 0.798, Precision (0,1) 1.000,0.608, Recall (0,1) 0.706,1.000, F1 0.805, MAE° 13.24\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.805)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 629/629 [00:12<00:00, 48.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.4740\n",
      "  ↳ val acc 0.815, Precision (0,1) 0.996,0.628, Recall (0,1) 0.732,0.994, F1 0.821, MAE° 12.34\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.821)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 629/629 [00:12<00:00, 49.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.4107\n",
      "  ↳ val acc 0.826, Precision (0,1) 0.998,0.641, Recall (0,1) 0.748,0.996, F1 0.832, MAE° 11.88\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 629/629 [00:12<00:00, 48.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.3231\n",
      "  ↳ val acc 0.848, Precision (0,1) 0.997,0.671, Recall (0,1) 0.781,0.996, F1 0.854, MAE° 11.10\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.854)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 629/629 [00:12<00:00, 48.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.2539\n",
      "  ↳ val acc 0.855, Precision (0,1) 0.996,0.683, Recall (0,1) 0.790,0.989, F1 0.860, MAE° 10.55\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.860)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 629/629 [00:12<00:00, 48.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.1801\n",
      "  ↳ val acc 0.854, Precision (0,1) 0.995,0.679, Recall (0,1) 0.791,0.990, F1 0.859, MAE° 10.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 629/629 [00:12<00:00, 48.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.1489\n",
      "  ↳ val acc 0.846, Precision (0,1) 0.995,0.669, Recall (0,1) 0.777,0.990, F1 0.851, MAE° 9.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 629/629 [00:12<00:00, 48.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.0958\n",
      "  ↳ val acc 0.873, Precision (0,1) 0.995,0.712, Recall (0,1) 0.817,0.989, F1 0.877, MAE° 9.32\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.877)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 629/629 [04:08<00:00,  2.53it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.0485\n",
      "  ↳ val acc 0.869, Precision (0,1) 0.983,0.712, Recall (0,1) 0.822,0.968, F1 0.873, MAE° 9.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 629/629 [00:12<00:00, 49.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.0329\n",
      "  ↳ val acc 0.882, Precision (0,1) 0.997,0.726, Recall (0,1) 0.828,0.991, F1 0.885, MAE° 8.83\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.885)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 629/629 [00:12<00:00, 49.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.9677\n",
      "  ↳ val acc 0.859, Precision (0,1) 0.989,0.691, Recall (0,1) 0.802,0.977, F1 0.864, MAE° 8.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 629/629 [00:12<00:00, 49.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.9704\n",
      "  ↳ val acc 0.886, Precision (0,1) 0.997,0.727, Recall (0,1) 0.835,0.997, F1 0.889, MAE° 8.42\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 629/629 [00:12<00:00, 49.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.9361\n",
      "  ↳ val acc 0.879, Precision (0,1) 0.997,0.715, Recall (0,1) 0.826,0.992, F1 0.883, MAE° 8.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 629/629 [00:12<00:00, 49.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.9014\n",
      "  ↳ val acc 0.892, Precision (0,1) 0.996,0.742, Recall (0,1) 0.846,0.992, F1 0.895, MAE° 7.99\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.895)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 629/629 [00:13<00:00, 47.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8963\n",
      "  ↳ val acc 0.864, Precision (0,1) 0.997,0.690, Recall (0,1) 0.802,0.997, F1 0.868, MAE° 7.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 629/629 [00:13<00:00, 47.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8753\n",
      "  ↳ val acc 0.903, Precision (0,1) 0.993,0.763, Recall (0,1) 0.861,0.990, F1 0.905, MAE° 7.67\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 629/629 [00:13<00:00, 48.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8426\n",
      "  ↳ val acc 0.898, Precision (0,1) 0.986,0.761, Recall (0,1) 0.864,0.970, F1 0.901, MAE° 7.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 629/629 [00:12<00:00, 48.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8273\n",
      "  ↳ val acc 0.904, Precision (0,1) 0.984,0.772, Recall (0,1) 0.873,0.973, F1 0.907, MAE° 7.46\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.907)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 629/629 [00:13<00:00, 48.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8173\n",
      "  ↳ val acc 0.896, Precision (0,1) 0.998,0.745, Recall (0,1) 0.847,0.997, F1 0.899, MAE° 7.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 629/629 [00:12<00:00, 48.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7942\n",
      "  ↳ val acc 0.888, Precision (0,1) 0.954,0.771, Recall (0,1) 0.877,0.909, F1 0.890, MAE° 7.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 629/629 [00:12<00:00, 48.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7759\n",
      "  ↳ val acc 0.908, Precision (0,1) 0.997,0.772, Recall (0,1) 0.870,0.989, F1 0.911, MAE° 6.81\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 629/629 [00:13<00:00, 48.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7535\n",
      "  ↳ val acc 0.907, Precision (0,1) 0.992,0.771, Recall (0,1) 0.869,0.983, F1 0.909, MAE° 6.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 629/629 [00:12<00:00, 48.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7475\n",
      "  ↳ val acc 0.920, Precision (0,1) 0.995,0.796, Recall (0,1) 0.886,0.990, F1 0.922, MAE° 6.70\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.922)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 629/629 [00:12<00:00, 48.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7286\n",
      "  ↳ val acc 0.911, Precision (0,1) 0.989,0.781, Recall (0,1) 0.879,0.977, F1 0.913, MAE° 6.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 629/629 [00:13<00:00, 48.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7259\n",
      "  ↳ val acc 0.924, Precision (0,1) 0.985,0.815, Recall (0,1) 0.900,0.978, F1 0.925, MAE° 6.41\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.925)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 629/629 [00:12<00:00, 48.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7035\n",
      "  ↳ val acc 0.920, Precision (0,1) 0.991,0.803, Recall (0,1) 0.892,0.980, F1 0.922, MAE° 6.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 629/629 [00:13<00:00, 47.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6906\n",
      "  ↳ val acc 0.895, Precision (0,1) 1.000,0.741, Recall (0,1) 0.845,1.000, F1 0.898, MAE° 6.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 629/629 [00:13<00:00, 47.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6813\n",
      "  ↳ val acc 0.875, Precision (0,1) 0.999,0.709, Recall (0,1) 0.817,0.998, F1 0.879, MAE° 6.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 629/629 [00:13<00:00, 47.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6688\n",
      "  ↳ val acc 0.915, Precision (0,1) 0.991,0.783, Recall (0,1) 0.884,0.983, F1 0.917, MAE° 5.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 629/629 [00:12<00:00, 48.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6476\n",
      "  ↳ val acc 0.918, Precision (0,1) 0.997,0.791, Recall (0,1) 0.883,0.994, F1 0.920, MAE° 5.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 629/629 [00:13<00:00, 47.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6446\n",
      "  ↳ val acc 0.924, Precision (0,1) 0.990,0.808, Recall (0,1) 0.898,0.977, F1 0.926, MAE° 5.72\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.926)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 629/629 [00:13<00:00, 47.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6409\n",
      "  ↳ val acc 0.929, Precision (0,1) 0.992,0.815, Recall (0,1) 0.903,0.981, F1 0.930, MAE° 5.73\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.930)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 629/629 [00:13<00:00, 47.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6150\n",
      "  ↳ val acc 0.931, Precision (0,1) 0.991,0.826, Recall (0,1) 0.908,0.983, F1 0.933, MAE° 5.63\n",
      "  ✓ Saved best model to kp_pose_quality_windows_ex.pt and state_dict to kp_pose_quality_windows_ex.pth (F1 0.933)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_epochs(\n",
    "    csv_file: str = str(WIN_CSV),\n",
    "    keypt_root: str = str(KEYPT_ROOT),\n",
    "    num_ex: int = NUM_EXERCISES,\n",
    "    epochs: int = 30,\n",
    "    batch: int = 16,\n",
    "    lr: float = 1e-4,\n",
    "    ckpt_file: str = CKPT_FILE\n",
    "):\n",
    "    # Build dataset and split\n",
    "    ds = KeypointWindowDataset(Path(csv_file), Path(keypt_root))\n",
    "    N = len(ds)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    c1, c2 = int(0.7 * N), int(0.85 * N)\n",
    "    train_idx, val_idx = idx[:c1], idx[c1:c2]\n",
    "\n",
    "    # --- Oversampling the Minority Class ---\n",
    "    # Get labels for training indices\n",
    "    train_labels = [ds.rows[i][\"correctness\"] for i in train_idx]\n",
    "    # Compute class weights for oversampling\n",
    "    class_counts = np.bincount(train_labels)  # [count_0, count_1]\n",
    "    num_samples = len(train_labels)\n",
    "    weights = np.zeros(num_samples)\n",
    "    for i, label in enumerate(train_labels):\n",
    "        weights[i] = num_samples / (len(class_counts) * class_counts[label])  # Inverse frequency\n",
    "    sampler = WeightedRandomSampler(weights=weights, num_samples=num_samples, replacement=True)\n",
    "\n",
    "    # DataLoaders with oversampling for training\n",
    "    train_dl = DataLoader(\n",
    "        Subset(ds, train_idx),\n",
    "        batch_size=batch,\n",
    "        sampler=sampler  # Use sampler instead of shuffle\n",
    "    )\n",
    "    val_dl = DataLoader(\n",
    "        Subset(ds, val_idx),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Infer input dimension\n",
    "    sample_seq, _, _, _ = ds[0]\n",
    "    in_dim = sample_seq.shape[-1]\n",
    "\n",
    "    # Build model\n",
    "    model = PoseQualityNetKP(in_dim, num_ex).to(DEVICE)\n",
    "    \n",
    "    # --- Class-Weighted Loss ---\n",
    "    # Assign higher weight to minority class (1: Correct)\n",
    "    class_weights = torch.tensor([1.0, class_counts[0] / class_counts[1]], dtype=torch.float32).to(DEVICE)  # Weight for [0, 1]\n",
    "    loss_cls = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    loss_err = nn.SmoothL1Loss()\n",
    "    opt = Adam(model.parameters(), lr)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # -- Train --\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        for seq, y, err, ex in tqdm(train_dl, desc=f\"Epoch {epoch:02d}\"):\n",
    "            seq, y, err, ex = [x.to(DEVICE) for x in (seq, y, err, ex)]\n",
    "            ex_1hot = F.one_hot(ex, num_ex).float()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits, err_hat = model(seq, ex_1hot)\n",
    "            loss = loss_cls(logits, y) + 0.1 * loss_err(err_hat, err)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += loss.item() * y.size(0)\n",
    "        print(f\"  ↳ train loss: {tot_loss/len(train_idx):.4f}\")\n",
    "\n",
    "        # -- Validation --\n",
    "        model.eval()\n",
    "        y_true, y_pred, errs = [], [], []\n",
    "        precision, recall = [], []\n",
    "        with torch.no_grad():\n",
    "            for seq, y, err, ex in val_dl:\n",
    "                seq, y, err, ex = [x.to(DEVICE) for x in (seq, y, err, ex)]\n",
    "                ex_1hot = F.one_hot(ex, num_ex).float()\n",
    "                logits, err_hat = model(seq, ex_1hot)\n",
    "\n",
    "                y_true += y.cpu().tolist()\n",
    "                y_pred += logits.argmax(1).cpu().tolist()\n",
    "                errs += [(err_hat - err.to(DEVICE)).abs().mean(1)]\n",
    "                \n",
    "                # Per-class precision and recall\n",
    "                precision += [precision_score(y.cpu(), logits.argmax(1).cpu(), average=None, labels=[0, 1])]\n",
    "                recall += [recall_score(y.cpu(), logits.argmax(1).cpu(), average=None, labels=[0, 1])]\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        mae = torch.cat(errs).mean().item()\n",
    "        precision_mean = np.mean(precision, axis=0)  # [prec_0, prec_1]\n",
    "        recall_mean = np.mean(recall, axis=0)       # [recall_0, recall_1]\n",
    "\n",
    "        print(f\"  ↳ val acc {acc:.3f}, Precision (0,1) {precision_mean[0]:.3f},{precision_mean[1]:.3f}, \"\n",
    "              f\"Recall (0,1) {recall_mean[0]:.3f},{recall_mean[1]:.3f}, F1 {f1:.3f}, MAE° {mae:.2f}\")\n",
    "\n",
    "        # Save if improved\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model, ckpt_file)\n",
    "            state_dict_path = Path(ckpt_file).with_suffix('.pth')\n",
    "            torch.save(model.state_dict(), state_dict_path)\n",
    "            print(f\"  ✓ Saved best model to {ckpt_file} and state_dict to {state_dict_path} (F1 {f1:.3f})\")\n",
    "\n",
    "train_epochs(epochs=40, batch=16, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b4529",
   "metadata": {},
   "source": [
    "# 6. Inference Tesing - Correctness and feedback on Live Videos / Recorded Videos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cd790",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "\n",
    "This script performs real-time pose quality assessment for physical exercises by integrating MediaPipe's Pose estimation with a custom deep learning model, `PoseQualityNetKP`, implemented in PyTorch. The process begins with capturing video input (either from a file or live webcam) using OpenCV (`cv2`). MediaPipe Pose processes each frame to detect 33 human pose landmarks, extracting their 3D coordinates (x, y, z) if visibility exceeds a threshold (0.7). These keypoints are collected into a buffer of 16 frames (`SEQUENCE_LENGTH`) to form a sequence of shape `(1, 16, 99)` (batch, time, 33 joints × 3 coordinates). This sequence is fed into `PoseQualityNetKP`, which consists of a `KeypointEncoder` (using 1D convolutions to encode per-frame keypoints into 512-dimensional embeddings) followed by a bidirectional LSTM (with 2 layers, 256 hidden units per direction) to capture temporal dynamics, producing a 512-dimensional sequence feature (`2 × hidden`). The model also embeds the exercise type (via one-hot encoding and an MLP) and concatenates it with the sequence feature, yielding a final representation. This is passed through two heads: a classification head (`cls_head`) to predict \"Correct\" or \"Incorrect\" pose (binary classification) and an error head (`err_head`) to estimate angular deviations for 14 predefined joints (e.g., `LEFT_ELBOW`, `RIGHT_KNEE`). The script ensures robust inference by checking the visibility of required landmarks for each exercise (e.g., special handling for push-ups with `PUSHUP_REQUIRED_LANDMARKS`), halting analysis if key joints are not visible (visibility < 0.7), and displaying messages like \"Adjust posture\" or \"No pose detected\" to guide the user.  \n",
    "\n",
    "The feedback logic is designed to provide actionable insights by leveraging the model's outputs in a structured manner. Once the model classifies a pose as \"Incorrect\" (predicted class 0), the script examines the angular deviation predictions from the error head (`err_head`), which outputs a vector of 14 values representing the mean angular deviation for each joint in `ERR_JOINTS`. It identifies the joint with the largest absolute deviation using `np.argmax`, but only flags it for correction if the deviation exceeds a threshold degree (`ERROR_WARNING_THRESHOLD`) (Currently set as 5). The suggestion is then formatted as a string (e.g., \"Check LEFT_ELBOW (Dev: +3.2°)\") and displayed on the video feed for a duration of 90 frames (`SUGGESTION_DURATION_FRAMES = 3 * 30`), ensuring the user has sufficient time to notice and act on the feedback. If the pose is \"Correct\" (predicted class 1), no joint-specific feedback is shown, and the buffer is maintained to continue monitoring. Additional feedback states, such as \"Analysing 5/16\" during buffer filling or \"World landmarks missing\" if 3D coordinates are unavailable, are also displayed to keep the user informed, with all text overlaid on the video using OpenCV in distinct colors (e.g., green for \"Correct\", red for \"Incorrect\", orange for warnings) to enhance clarity.\n",
    "\n",
    "### How to Run (Live or from File)\n",
    "\n",
    "1. Set Up Environment: Ensure Python is installed, then install required packages by running pip install torch torchvision torchaudio mediapipe opencv-python numpy.  \n",
    "2. Prepare Files: Place the script in a directory with the Data-REHAB24-6 folder containing video files (e.g., Videos/Ex1/PM_000-Camera17-30fps.mp4) and the checkpoint file kp_pose_quality_windows_ex.pt in the same directory.  \n",
    "3. Run the Script: Execute the script in a terminal or Jupyter notebook using python pose_inference.py.  \n",
    "4. Select Exercise: When prompted, enter an exercise ID (1-6) to choose from the available exercises (e.g., 1 for \"Arm abduction\").\n",
    "5. Choose Video Source: Enter 0 for file-based inference or 1 for live webcam. If choosing a file, enter the video ID (e.g., 000 for VIDEO_000).\n",
    "6. View Feedback: The script will process the video, display pose landmarks, and provide real-time feedback (\"Correct\", \"Incorrect\", or joint-specific suggestions). Press q to exit.\n",
    "7. Review Output: Check the terminal for logs and ensure resources are released upon completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0404a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► Using device: mps\n",
      "ℹ️ Indices required for error angle visibility check: [0, 11, 12, 13, 14, 15, 16, 19, 20, 23, 24, 25, 26, 27, 28, 31, 32]\n",
      "\n",
      "Available Exercises:\n",
      "  1: Arm abduction\n",
      "  2: Arm VW\n",
      "  3: Push-ups\n",
      "  4: Leg abduction\n",
      "  5: Leg lunge\n",
      "  6: Squats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745192644.032706  612322 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: 1 - Arm abduction\n",
      "⏳ Initializing MediaPipe Pose...\n",
      "✅ MediaPipe Pose initialized.\n",
      "\n",
      "Perform inference from:\n",
      "  0: File\n",
      "  1: Laptop camera (live)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745192644.094704  617701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745192644.115260  617701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using laptop camera for live inference.\n",
      "⏳ Loading model from /Users/jithinkrishnan/Documents/Study/IS06 /MVP/RehabApp/model-training-scripts/kp_pose_quality_windows_ex.pt...\n",
      "✅ Model loaded successfully and set to eval mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_25397/3156858384.py:513: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  infer_model = torch.load(CKPT_FILE, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam FPS: 30.0\n",
      "Frame interval: 1 (processing every 1th frame)\n",
      "🚀 Starting feedback loop using: camera (Press 'q' to quit)\n",
      "\n",
      "'q' pressed. Exiting.\n",
      "✅ Resources released.\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "# --- Configuration ---\n",
    "SCRIPT_DIR = Path().resolve()\n",
    "DATA_ROOT = SCRIPT_DIR / \"Data-REHAB24-6\"\n",
    "CKPT_FILE = SCRIPT_DIR / \"kp_pose_quality_windows_ex.pt\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"► Using device: {DEVICE}\")\n",
    "\n",
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "JOINT_NAMES = [lm.name for lm in PoseLandmark]\n",
    "N_JOINTS = len(JOINT_NAMES)\n",
    "\n",
    "EXERCISE_MAP = {\n",
    "    1: \"Arm abduction\", 2: \"Arm VW\", 3: \"Push-ups\",\n",
    "    4: \"Leg abduction\", 5: \"Leg lunge\", 6: \"Squats\"\n",
    "}\n",
    "NUM_EXERCISES = len(EXERCISE_MAP)\n",
    "\n",
    "JOINT_TRIPLETS = {\n",
    "    \"LEFT_ELBOW\":   (PoseLandmark.LEFT_SHOULDER, PoseLandmark.LEFT_ELBOW, PoseLandmark.LEFT_WRIST),\n",
    "    \"RIGHT_ELBOW\":  (PoseLandmark.RIGHT_SHOULDER, PoseLandmark.RIGHT_ELBOW, PoseLandmark.RIGHT_WRIST),\n",
    "    \"LEFT_SHOULDER\":  (PoseLandmark.LEFT_ELBOW, PoseLandmark.LEFT_SHOULDER, PoseLandmark.LEFT_HIP),\n",
    "    \"RIGHT_SHOULDER\": (PoseLandmark.RIGHT_ELBOW, PoseLandmark.RIGHT_SHOULDER, PoseLandmark.RIGHT_HIP),\n",
    "    \"LEFT_HIP\":   (PoseLandmark.LEFT_SHOULDER, PoseLandmark.LEFT_HIP, PoseLandmark.LEFT_KNEE),\n",
    "    \"RIGHT_HIP\":  (PoseLandmark.RIGHT_SHOULDER, PoseLandmark.RIGHT_HIP, PoseLandmark.RIGHT_KNEE),\n",
    "    \"LEFT_KNEE\":  (PoseLandmark.LEFT_HIP, PoseLandmark.LEFT_KNEE, PoseLandmark.LEFT_ANKLE),\n",
    "    \"RIGHT_KNEE\": (PoseLandmark.RIGHT_HIP, PoseLandmark.RIGHT_KNEE, PoseLandmark.RIGHT_ANKLE),\n",
    "    \"SPINE\":      (PoseLandmark.LEFT_HIP, PoseLandmark.LEFT_SHOULDER, PoseLandmark.RIGHT_SHOULDER),\n",
    "    \"HEAD\":       (PoseLandmark.LEFT_SHOULDER, PoseLandmark.NOSE, PoseLandmark.RIGHT_SHOULDER),\n",
    "    \"LEFT_WRIST\": (PoseLandmark.LEFT_ELBOW, PoseLandmark.LEFT_WRIST, PoseLandmark.LEFT_INDEX),\n",
    "    \"RIGHT_WRIST\":(PoseLandmark.RIGHT_ELBOW, PoseLandmark.RIGHT_WRIST, PoseLandmark.RIGHT_INDEX),\n",
    "    \"LEFT_ANKLE\": (PoseLandmark.LEFT_KNEE, PoseLandmark.LEFT_ANKLE, PoseLandmark.LEFT_FOOT_INDEX),\n",
    "    \"RIGHT_ANKLE\":(PoseLandmark.RIGHT_KNEE, PoseLandmark.RIGHT_ANKLE, PoseLandmark.RIGHT_FOOT_INDEX),\n",
    "}\n",
    "ERR_JOINTS = list(JOINT_TRIPLETS.keys())\n",
    "N_ERR = len(ERR_JOINTS)\n",
    "\n",
    "REQUIRED_LANDMARK_INDICES_FOR_ERRORS = set()\n",
    "for joint_name, landmarks in JOINT_TRIPLETS.items():\n",
    "    for landmark in landmarks:\n",
    "        REQUIRED_LANDMARK_INDICES_FOR_ERRORS.add(landmark.value)\n",
    "print(f\"ℹ️ Indices required for error angle visibility check: {sorted(list(REQUIRED_LANDMARK_INDICES_FOR_ERRORS))}\")\n",
    "N_REQUIRED_LANDMARKS = len(REQUIRED_LANDMARK_INDICES_FOR_ERRORS)\n",
    "\n",
    "PUSHUP_JOINT_TRIPLETS = { \n",
    "    \"RIGHT_ELBOW\": (PoseLandmark.RIGHT_SHOULDER, PoseLandmark.RIGHT_ELBOW, PoseLandmark.RIGHT_WRIST),\n",
    "    \"RIGHT_SHOULDER\": (PoseLandmark.RIGHT_ELBOW, PoseLandmark.RIGHT_SHOULDER, PoseLandmark.RIGHT_HIP),\n",
    "    \"SPINE\": (PoseLandmark.LEFT_HIP, PoseLandmark.LEFT_SHOULDER, PoseLandmark.RIGHT_SHOULDER),\n",
    "}\n",
    "PUSHUP_REQUIRED_LANDMARKS = set()\n",
    "for landmarks in PUSHUP_JOINT_TRIPLETS.values():\n",
    "    for landmark in landmarks:\n",
    "        PUSHUP_REQUIRED_LANDMARKS.add(landmark.value)\n",
    "\n",
    "VISIBILITY_THRESHOLD = 0.8\n",
    "\n",
    "# --- Model Definitions ---\n",
    "# 5. Model definitions\n",
    "class KeypointEncoder(nn.Module):\n",
    "    def __init__(self, in_dim:int, embed:int=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, embed, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, D); treat as (B, D, 1) for Conv1d\n",
    "        x = x.unsqueeze(2)                 # → (B, D, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return self.pool(x).squeeze(-1)    # → (B, embed)\n",
    "\n",
    "class PoseQualityNetKP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 num_ex: int,\n",
    "                 hidden: int = 256,\n",
    "                 ex_emb: int = 64):\n",
    "        super().__init__()\n",
    "        # keypoint feature extractor\n",
    "        self.encoder = KeypointEncoder(in_dim)\n",
    "\n",
    "        # sequence model\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        feat_dim = hidden * 2\n",
    "\n",
    "        # exercise embedding MLP\n",
    "        self.ex_emb = nn.Sequential(\n",
    "            nn.Linear(num_ex, ex_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ex_emb, ex_emb)\n",
    "        )\n",
    "\n",
    "        # final heads\n",
    "        self.cls_head = nn.Linear(feat_dim + ex_emb, 2)\n",
    "        self.err_head = nn.Linear(feat_dim + ex_emb, N_ERR)\n",
    "\n",
    "    def forward(self,\n",
    "                seq:     torch.Tensor,  # (B, T, D)\n",
    "                ex_1hot: torch.Tensor   # (B, num_ex)\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # 1) keypoint → sequence feats\n",
    "        # encode each frame\n",
    "        B,T,_ = seq.shape\n",
    "        feats = torch.stack([\n",
    "            self.encoder(seq[:,t]) for t in range(T)\n",
    "        ], dim=1)                                # (B, T, 512)\n",
    "        out, _ = self.lstm(feats)                # (B, T, 2*hidden)\n",
    "        g = out.mean(1)                          # (B, 2*hidden)\n",
    "\n",
    "        # 2) exercise embed\n",
    "        ex_e = self.ex_emb(ex_1hot)              # (B, ex_emb)\n",
    "\n",
    "        # 3) concat and heads\n",
    "        h = torch.cat([g, ex_e], dim=1)          # (B, feat_dim+ex_emb)\n",
    "        return self.cls_head(h), self.err_head(h)\n",
    "\n",
    "\n",
    "# --- User Input for Exercise ---\n",
    "while True:\n",
    "    print(\"\\nAvailable Exercises:\")\n",
    "    for id, name in EXERCISE_MAP.items(): print(f\"  {id}: {name}\")\n",
    "    try:\n",
    "        exercise_id_str = input(f\"► Enter the exercise ID (1-{NUM_EXERCISES}): \")\n",
    "        exercise_id = int(exercise_id_str)\n",
    "        if 1 <= exercise_id <= NUM_EXERCISES:\n",
    "            exercise_name = EXERCISE_MAP[exercise_id]\n",
    "            print(f\"Selected: {exercise_id} - {exercise_name}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid ID.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input.\")\n",
    "    except EOFError:\n",
    "        print(\"\\nCancelled.\")\n",
    "        exit()\n",
    "\n",
    "# --- MediaPipe Pose Setup ---\n",
    "print(\"⏳ Initializing MediaPipe Pose...\")\n",
    "try:\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=False, model_complexity=2, enable_segmentation=False,\n",
    "        min_detection_confidence=0.8, min_tracking_confidence=0.8\n",
    "    )\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    print(\"✅ MediaPipe Pose initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing MediaPipe Pose: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Helper Function: Extract Keypoints ---\n",
    "def extract_keypoints(frame, exercise_id):\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_rgb.flags.writeable = False\n",
    "    result = pose.process(img_rgb)\n",
    "    img_rgb.flags.writeable = True\n",
    "\n",
    "    world_keypoints = None\n",
    "    image_landmarks_for_drawing = None\n",
    "    all_required_visible = False\n",
    "\n",
    "    required_landmarks = (\n",
    "        PUSHUP_REQUIRED_LANDMARKS if exercise_id == 3 else REQUIRED_LANDMARK_INDICES_FOR_ERRORS\n",
    "    )\n",
    "    n_required = len(required_landmarks)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        image_landmarks_for_drawing = result.pose_landmarks\n",
    "        num_visible_required = sum(\n",
    "            1 for index in required_landmarks\n",
    "            if index < len(image_landmarks_for_drawing.landmark)\n",
    "            and image_landmarks_for_drawing.landmark[index].visibility >= VISIBILITY_THRESHOLD\n",
    "        )\n",
    "        all_required_visible = num_visible_required == n_required\n",
    "        if all_required_visible and result.pose_world_landmarks:\n",
    "            world_landmarks = result.pose_world_landmarks.landmark\n",
    "            world_keypoints = np.array([(lm.x, lm.y, lm.z) for lm in world_landmarks], dtype=np.float32)\n",
    "\n",
    "    return world_keypoints, image_landmarks_for_drawing, all_required_visible\n",
    "\n",
    "# --- Inference Parameters ---\n",
    "SEQUENCE_LENGTH = 16\n",
    "IN_DIM = N_JOINTS * 3\n",
    "\n",
    "# --- Core Inference and Feedback Function ---\n",
    "def infer_and_feedback(model, video_source, selected_ex_id, selected_ex_name):\n",
    "    if isinstance(video_source, str) and not Path(video_source).exists():\n",
    "        print(f\"❌ Error: Video file not found: {video_source}\")\n",
    "        return\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_source)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"❌ Error: Could not open video source '{video_source}'\")\n",
    "            return\n",
    "        # --- New: Set and verify webcam frame rate ---\n",
    "        if isinstance(video_source, int):  # Webcam\n",
    "            cap.set(cv2.CAP_PROP_FPS, 30)  # Attempt to set to 30 FPS\n",
    "            webcam_fps = cap.get(cv2.CAP_PROP_FPS) or 30  # Fallback to 30 if not available\n",
    "            print(f\"Webcam FPS: {webcam_fps}\")\n",
    "            target_fps = 30\n",
    "            frame_interval = max(1, round(webcam_fps / target_fps))  # Frames to skip\n",
    "            print(f\"Frame interval: {frame_interval} (processing every {frame_interval}th frame)\")\n",
    "        else:\n",
    "            frame_interval = 1  # No skipping for video files (assumed 30 FPS)\n",
    "        frame_count = 0  # Track frame count for skipping\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error opening video source: {e}\")\n",
    "        return\n",
    "\n",
    "    source_name = \"camera\" if isinstance(video_source, int) else Path(video_source).name\n",
    "    print(f\"🚀 Starting feedback loop using: {source_name} (Press 'q' to quit)\")\n",
    "\n",
    "    keypoints_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    feedback = \"Initializing...\"\n",
    "    err_values = np.zeros(N_ERR)\n",
    "    predicted_class = 0\n",
    "    suggestion = \"\"\n",
    "    suggestion_time = 0\n",
    "    SUGGESTION_DURATION_FRAMES = 3 * 30\n",
    "\n",
    "    ERROR_WARNING_THRESHOLD = 10 #Minimum error angle for joint warning display\n",
    "\n",
    "    FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    FONT_SCALE_INFO = 1\n",
    "    FONT_SCALE_FEEDBACK = 1\n",
    "    FONT_SCALE_SUGGESTION = 1\n",
    "    FONT_SCALE_WARNING = 1\n",
    "    FONT_THICKNESS = 2\n",
    "    JOINT_WARNING_SPACING = 30\n",
    "\n",
    "    window_name = 'Pose Estimation Feedback'\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"\\nℹ️ End of video stream.\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_interval != 0:\n",
    "            continue  # Skip this frame\n",
    "        \n",
    "        world_keypoints, image_landmarks_for_drawing, all_required_visible = extract_keypoints(frame, selected_ex_id)\n",
    "\n",
    "        if image_landmarks_for_drawing:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, image_landmarks_for_drawing, mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        current_feedback_state = \"OK\"\n",
    "        if image_landmarks_for_drawing is None:\n",
    "            current_feedback_state = \"NO_POSE\"\n",
    "        elif not all_required_visible:\n",
    "            current_feedback_state = \"ADJUST_POSTURE\"\n",
    "        elif world_keypoints is None and all_required_visible:\n",
    "            current_feedback_state = \"NO_WORLD_LANDMARKS\"\n",
    "\n",
    "        if current_feedback_state == \"NO_POSE\":\n",
    "            feedback = \"No pose detected\"\n",
    "            keypoints_buffer.clear()\n",
    "            predicted_class = 0\n",
    "            suggestion = \"\"\n",
    "            suggestion_time = 0\n",
    "        elif current_feedback_state == \"ADJUST_POSTURE\":\n",
    "            feedback = \"Adjust posture\"\n",
    "            keypoints_buffer.clear()\n",
    "            predicted_class = 0\n",
    "            suggestion = \"\"\n",
    "            suggestion_time = 0\n",
    "        elif current_feedback_state == \"NO_WORLD_LANDMARKS\":\n",
    "            feedback = \"World landmarks missing\"\n",
    "            keypoints_buffer.clear()\n",
    "            predicted_class = 0\n",
    "            suggestion = \"\"\n",
    "            suggestion_time = 0\n",
    "        else:\n",
    "            keypoints_flat_frame = world_keypoints.flatten()\n",
    "            keypoints_buffer.append(keypoints_flat_frame)\n",
    "\n",
    "            if len(keypoints_buffer) == SEQUENCE_LENGTH:\n",
    "                seq_np = np.array(keypoints_buffer, dtype=np.float32)\n",
    "                seq = torch.tensor(seq_np, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "                ex_tensor = torch.tensor([selected_ex_id - 1], device=DEVICE)\n",
    "                ex_1hot = F.one_hot(ex_tensor, num_classes=NUM_EXERCISES).float()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    logits, err_hat = model(seq, ex_1hot)\n",
    "                    predicted_class = logits.argmax(1).item()\n",
    "                    err_values = err_hat.squeeze().cpu().numpy()\n",
    "\n",
    "                feedback = \"Correct\" if predicted_class == 1 else \"Incorrect\"\n",
    "\n",
    "                if predicted_class == 0:\n",
    "                    if np.any(np.abs(err_values) > 1e-3):\n",
    "                        max_error_idx = np.argmax(np.abs(err_values))\n",
    "                        joint_with_error = ERR_JOINTS[max_error_idx]\n",
    "                        max_error = err_values[max_error_idx]\n",
    "                        suggestion = f\"Check {joint_with_error.replace('_', ' ')} (Dev: {max_error:+.1f}°)\"\n",
    "                        suggestion_time = SUGGESTION_DURATION_FRAMES\n",
    "                    else:\n",
    "                        suggestion = \"Check Form\"\n",
    "                        suggestion_time = SUGGESTION_DURATION_FRAMES\n",
    "                else:\n",
    "                    suggestion = \"\"\n",
    "                    suggestion_time = 0\n",
    "            else:\n",
    "                feedback = f\"Analysing {len(keypoints_buffer)}/{SEQUENCE_LENGTH}\"\n",
    "                suggestion = \"\"\n",
    "                suggestion_time = 0\n",
    "                predicted_class = 0\n",
    "\n",
    "        info_y = 40\n",
    "        cv2.putText(frame, f\"{selected_ex_name}\", (15, info_y), FONT_FACE,\n",
    "                    FONT_SCALE_INFO, (255, 255, 255), FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "        feedback_y = info_y + 50\n",
    "        if current_feedback_state == \"OK\" and len(keypoints_buffer) == SEQUENCE_LENGTH:\n",
    "            feedback_color = (0, 255, 0) if predicted_class == 1 else (0, 0, 255)\n",
    "        elif current_feedback_state == \"ADJUST_POSTURE\" or current_feedback_state == \"NO_WORLD_LANDMARKS\":\n",
    "            feedback_color = (0, 165, 255)\n",
    "        else:\n",
    "            feedback_color = (0, 0, 255)\n",
    "\n",
    "        cv2.putText(frame, f\"{feedback}\", (15, feedback_y), FONT_FACE,\n",
    "                    FONT_SCALE_FEEDBACK, feedback_color, FONT_THICKNESS + 1, cv2.LINE_AA)\n",
    "\n",
    "        suggestion_y = feedback_y + 50\n",
    "        if suggestion_time > 0:\n",
    "            cv2.putText(frame, suggestion, (15, suggestion_y), FONT_FACE,\n",
    "                        FONT_SCALE_SUGGESTION, (0, 255, 255), FONT_THICKNESS, cv2.LINE_AA)\n",
    "            suggestion_time -= 1\n",
    "            feedback_y_start = suggestion_y + 50\n",
    "        else:\n",
    "            feedback_y_start = feedback_y + 50\n",
    "\n",
    "        joint_warning_count = 0\n",
    "        if current_feedback_state == \"OK\" and len(keypoints_buffer) == SEQUENCE_LENGTH:\n",
    "            sorted_error_indices = np.argsort(np.abs(err_values))[::-1]\n",
    "            for idx in sorted_error_indices:\n",
    "                warning_y = feedback_y_start + joint_warning_count * JOINT_WARNING_SPACING\n",
    "                if joint_warning_count >= 4:\n",
    "                    cv2.putText(frame, \"...\", (15, warning_y), FONT_FACE, FONT_SCALE_WARNING, (0, 165, 255), FONT_THICKNESS, cv2.LINE_AA)\n",
    "                    break\n",
    "                joint_name = ERR_JOINTS[idx]\n",
    "                err_val = err_values[idx]\n",
    "                if abs(err_val) > ERROR_WARNING_THRESHOLD:\n",
    "                    color = (0, 165, 255)\n",
    "                    text = f\"{joint_name}: {err_val:+.0f}°\"\n",
    "                    cv2.putText(frame, text, (15, warning_y), FONT_FACE,\n",
    "                                FONT_SCALE_WARNING, color, FONT_THICKNESS, cv2.LINE_AA)\n",
    "                    joint_warning_count += 1\n",
    "\n",
    "        cv2.imshow(window_name, frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"\\n'q' pressed. Exiting.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pose.close()\n",
    "    print(\"✅ Resources released.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ex1 videos\n",
    "    VIDEO_000 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_000-Camera17-30fps.mp4\"\n",
    "    VIDEO_001 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_001-Camera17-30fps.mp4\"\n",
    "    VIDEO_002 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_002-Camera17-30fps.mp4\"\n",
    "    VIDEO_012 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_012-Camera17-30fps.mp4\"\n",
    "    VIDEO_016 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_016-Camera17-30fps.mp4\"\n",
    "    VIDEO_023 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_023-Camera17-30fps.mp4\"\n",
    "    VIDEO_024 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_024-Camera17-30fps.mp4\"\n",
    "    VIDEO_032 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_032-Camera17-30fps.mp4\"\n",
    "    VIDEO_039 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_039-Camera17-30fps.mp4\"\n",
    "    VIDEO_100 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_100-Camera17-30fps.mp4\"\n",
    "    VIDEO_109 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_109-Camera17-30fps.mp4\"\n",
    "    VIDEO_114 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_114-Camera17-30fps.mp4\"\n",
    "    VIDEO_122 = DATA_ROOT / \"Videos\" / \"Ex1\" / \"PM_122-Camera17-30fps.mp4\"\n",
    "\n",
    "    # Ex2 videos\n",
    "    VIDEO_003 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_003-Camera17-30fps.mp4\"\n",
    "    VIDEO_004 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_004-Camera17-30fps.mp4\"\n",
    "    VIDEO_013 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_013-Camera17-30fps.mp4\"\n",
    "    VIDEO_014 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_014-Camera17-30fps.mp4\"\n",
    "    VIDEO_025 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_025-Camera17-30fps.mp4\"\n",
    "    VIDEO_026 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_026-Camera17-30fps.mp4\"\n",
    "    VIDEO_033 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_033-Camera17-30fps.mp4\"\n",
    "    VIDEO_040 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_040-Camera17-30fps.mp4\"\n",
    "    VIDEO_102 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_102-Camera17-30fps.mp4\"\n",
    "    VIDEO_110 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_110-Camera17-30fps.mp4\"\n",
    "    VIDEO_115 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_115-Camera17-30fps.mp4\"\n",
    "    VIDEO_123 = DATA_ROOT / \"Videos\" / \"Ex2\" / \"PM_123-Camera17-30fps.mp4\"\n",
    "\n",
    "    # Ex3 videos\n",
    "    VIDEO_010 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_010-Camera17-30fps.mp4\"\n",
    "    VIDEO_011 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_011-Camera17-30fps.mp4\"\n",
    "    VIDEO_030 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_030-Camera17-30fps.mp4\"\n",
    "    VIDEO_031 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_031-Camera17-30fps.mp4\"\n",
    "    VIDEO_044 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_044-Camera17-30fps.mp4\"\n",
    "    VIDEO_045 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_045-Camera17-30fps.mp4\"\n",
    "    VIDEO_107 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_107-Camera17-30fps.mp4\"\n",
    "    VIDEO_108 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_108-Camera17-30fps.mp4\"\n",
    "    VIDEO_119 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_119-Camera17-30fps.mp4\"\n",
    "    VIDEO_121 = DATA_ROOT / \"Videos\" / \"Ex3\" / \"PM_121-Camera17-30fps.mp4\"\n",
    "\n",
    "    # Ex4 videos\n",
    "    VIDEO_005 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_005-Camera17-30fps.mp4\"\n",
    "    VIDEO_006 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_006-Camera17-30fps.mp4\"\n",
    "    VIDEO_018 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_018-Camera17-30fps.mp4\"\n",
    "    VIDEO_020 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_020-Camera17-30fps.mp4\"\n",
    "    VIDEO_027 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_027-Camera17-30fps.mp4\"\n",
    "    VIDEO_034 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_034-Camera17-30fps.mp4\"\n",
    "    VIDEO_035 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_035-Camera17-30fps.mp4\"\n",
    "    VIDEO_041 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_041-Camera17-30fps.mp4\"\n",
    "    VIDEO_103 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_103-Camera17-30fps.mp4\"\n",
    "    VIDEO_111 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_111-Camera17-30fps.mp4\"\n",
    "    VIDEO_116 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_116-Camera17-30fps.mp4\"\n",
    "    VIDEO_124 = DATA_ROOT / \"Videos\" / \"Ex4\" / \"PM_124-Camera17-30fps.mp4\"\n",
    "\n",
    "    # Ex5 videos\n",
    "    VIDEO_021 = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_021-Camera17-30fps.mp4\"\n",
    "    VIDEO_028 = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_028-Camera17-30fps.mp4\"\n",
    "    VIDEO_037 = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_037-Camera17-30fps.mp4\"\n",
    "    VIDEO_042 = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_042-Camera17-30fps.mp4\"\n",
    "    VIDEO_104 = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_104-Camera17-30fps.mp4\"\n",
    "    VIDEO_112 = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_112-Camera17-30fps.mp4\"\n",
    "    VIDEO_117a = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_117a-Camera17-30fps.mp4\"\n",
    "    VIDEO_117b = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_117b-Camera17-30fps.mp4\"\n",
    "    VIDEO_125 = DATA_ROOT / \"Videos\" / \"Ex5\" / \"PM_125-Camera17-30fps.mp4\"\n",
    "\n",
    "    # Ex6 videos\n",
    "    VIDEO_008 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_008-Camera17-30fps.mp4\"\n",
    "    VIDEO_022 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_022-Camera17-30fps.mp4\"\n",
    "    VIDEO_029 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_029-Camera17-30fps.mp4\"\n",
    "    VIDEO_038 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_038-Camera17-30fps.mp4\"\n",
    "    VIDEO_043 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_043-Camera17-30fps.mp4\"\n",
    "    VIDEO_105 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_105-Camera17-30fps.mp4\"\n",
    "    VIDEO_113 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_113-Camera17-30fps.mp4\"\n",
    "    VIDEO_118 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_118-Camera17-30fps.mp4\"\n",
    "    VIDEO_126 = DATA_ROOT / \"Videos\" / \"Ex6\" / \"PM_126-Camera17-30fps.mp4\"\n",
    "\n",
    "    # Prompt for video source selection\n",
    "    while True:\n",
    "        print(\"\\nPerform inference from:\")\n",
    "        print(\"  0: File\")\n",
    "        print(\"  1: Laptop camera (live)\")\n",
    "        try:\n",
    "            source_choice = input(\"► Enter choice (0 for file, 1 for laptop camera): \")\n",
    "            source_choice = int(source_choice)\n",
    "            if source_choice in [0, 1]:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter 0 or 1.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter 0 or 1.\")\n",
    "        except EOFError:\n",
    "            print(\"\\nCancelled.\")\n",
    "            exit()\n",
    "\n",
    "    # Handle video source based on choice\n",
    "    if source_choice == 0:  # File-based inference\n",
    "        while True:\n",
    "            try:\n",
    "                video_id = input(\"► Enter video ID (e.g., '000'): \")\n",
    "                video_var_name = f\"VIDEO_{video_id}\"\n",
    "                if video_var_name in globals():\n",
    "                    VIDEO_PATH = globals()[video_var_name]\n",
    "                    if VIDEO_PATH.exists():\n",
    "                        VIDEO_SOURCE = str(VIDEO_PATH)\n",
    "                        print(f\"Selected video: {VIDEO_PATH.name}\")\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Video file not found: {VIDEO_PATH}\")\n",
    "                else:\n",
    "                    print(f\"Video ID '{video_id}' not found. Please enter a valid ID.\")\n",
    "            except EOFError:\n",
    "                print(\"\\nCancelled.\")\n",
    "                exit()\n",
    "    else:  # Live camera inference\n",
    "        VIDEO_SOURCE = 0  # Webcam\n",
    "        print(\"Using laptop camera for live inference.\")\n",
    "\n",
    "    # --- Model Loading ---\n",
    "    if not CKPT_FILE.exists():\n",
    "        print(f\"❌ Error: Checkpoint file not found at {CKPT_FILE}\")\n",
    "        exit()\n",
    "    print(f\"⏳ Loading model from {CKPT_FILE}...\")\n",
    "    try:\n",
    "        infer_model = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "        if isinstance(infer_model, dict):\n",
    "            print(\"Loaded state_dict, instantiating model...\")\n",
    "            model_state_dict = infer_model\n",
    "            IN_DIM = N_JOINTS * 3\n",
    "            infer_model = PoseQualityNetKP(in_dim=IN_DIM, num_ex=NUM_EXERCISES).to(DEVICE)\n",
    "            infer_model.load_state_dict(model_state_dict)\n",
    "            print(\"Model instantiated and state_dict loaded.\")\n",
    "        elif not isinstance(infer_model, nn.Module):\n",
    "            raise TypeError(f\"Loaded object is not a nn.Module or state_dict: {type(infer_model)}\")\n",
    "        infer_model.eval()\n",
    "        print(\"✅ Model loaded successfully and set to eval mode.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        infer_and_feedback(infer_model, VIDEO_SOURCE, exercise_id, exercise_name)\n",
    "    except NameError as e:\n",
    "        print(f\"❌ NameError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred: {e}\")\n",
    "    print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rehabtrainingpy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
