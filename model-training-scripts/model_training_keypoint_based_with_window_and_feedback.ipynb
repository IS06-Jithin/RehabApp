{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab585d8f",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a70e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0b559",
   "metadata": {},
   "source": [
    "# 2. Keypoint Extraction from Training Videos with MediaPipe  \n",
    "Before training a model for tasks like pose estimation, exercise recognition, or movement analysis, the first step is to extract relevant features from the data. In this case, the data consists of training videos, and the relevant features are pose keypoints—3D coordinates representing specific body parts (like shoulders, elbows, knees, etc.).  \n",
    "\n",
    "This code uses MediaPipe Pose to extract keypoints from training videos. These keypoints are critical for model training as they represent the underlying body movements that the model will need to learn. Below is a breakdown of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae42d0f",
   "metadata": {},
   "source": [
    "## 2.1 Keypoint extraction and saving as numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fe5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keypoint extraction from Training Videos With Mediapipe\n",
    "\n",
    "# # 0. Quiet TensorFlow/absl\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# # 1. MediaPipe Pose setup\n",
    "# pose = mp.solutions.pose.Pose(\n",
    "#     static_image_mode=False,\n",
    "#     model_complexity=2,\n",
    "#     enable_segmentation=False,\n",
    "#     min_detection_confidence=0.5,\n",
    "#     min_tracking_confidence=0.5\n",
    "# )\n",
    "\n",
    "# # 2. Paths\n",
    "# VIDEO_ROOT = Path(\"Data-REHAB24-6/videos\")\n",
    "# OUT_ROOT   = Path(\"Data-REHAB24-6/mp_keypoints\")\n",
    "# OUT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# # 3. Worker\n",
    "# def process_video(vid_path: Path):\n",
    "#     rel     = vid_path.parent.name            # e.g. \"Ex1\"\n",
    "#     out_dir = OUT_ROOT / rel\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     out_file = out_dir / f\"{vid_path.stem}-mp.npy\"\n",
    "\n",
    "#     print(f\"\\n→ Processing: {vid_path.name}\")\n",
    "#     print(f\"   From:      {vid_path.parent}\")\n",
    "#     print(f\"   To folder: {out_dir}\")\n",
    "\n",
    "#     cap    = cv2.VideoCapture(str(vid_path))\n",
    "#     frames = []\n",
    "#     count  = 0\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         count += 1\n",
    "\n",
    "#         img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         res = pose.process(img)\n",
    "#         lm  = res.pose_world_landmarks.landmark if res.pose_world_landmarks else []\n",
    "\n",
    "#         if lm:\n",
    "#             pts = [(p.x, p.y, p.z) for p in lm]\n",
    "#         else:\n",
    "#             pts = [(0.0, 0.0, 0.0)] * 33\n",
    "\n",
    "#         frames.append(pts)\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "#     arr = np.array(frames, dtype=np.float32)\n",
    "#     np.save(out_file, arr)\n",
    "\n",
    "#     print(f\"✔ Saved: {out_file}  (frames={count}, shape={arr.shape})\")\n",
    "\n",
    "# # 4. Run — only Ex1 through Ex5\n",
    "# for i in range(1, 6):\n",
    "#     ex_dir = VIDEO_ROOT / f\"Ex{i}\"\n",
    "#     if not ex_dir.is_dir():\n",
    "#         print(f\"⚠️  Skipping missing folder: {ex_dir}\")\n",
    "#         continue\n",
    "\n",
    "#     for vid in sorted(ex_dir.glob(\"*.mp4\")):\n",
    "#         try:\n",
    "#             process_video(vid)\n",
    "#         except Exception as e:\n",
    "#             print(f\"✘ Failed processing {vid.name}: {e}\")\n",
    "\n",
    "# print(\"\\nAll requested videos processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390d6ed",
   "metadata": {},
   "source": [
    "## 2.2 Inspecting at a numpy file containing 3D keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86099ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Data-REHAB24-6/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\n",
      " dtype: float32\n",
      " shape: (5191, 33, 3)  (frames × landmarks × coords)\n",
      "\n",
      "Frame #000 (33×3):\n",
      "[[-0.03379065 -0.6112996  -0.22619084]\n",
      " [-0.02455677 -0.628223   -0.2275483 ]\n",
      " [-0.0257254  -0.6304051  -0.21732828]\n",
      " [-0.02547118 -0.6302204  -0.21820049]\n",
      " [-0.02778288 -0.6363151  -0.2358914 ]\n",
      " [-0.02677054 -0.63463634 -0.24706481]\n",
      " [-0.02263773 -0.61979294 -0.2281486 ]\n",
      " [ 0.02648694 -0.6184615  -0.16859515]\n",
      " [-0.03559308 -0.56201506 -0.14808732]\n",
      " [ 0.0030987  -0.59678274 -0.19447449]\n",
      " [-0.01712019 -0.55977213 -0.21580447]\n",
      " [ 0.1250833  -0.49333623 -0.08702794]\n",
      " [-0.05722423 -0.53108674 -0.02576461]\n",
      " [ 0.15178505 -0.51440114 -0.09251688]\n",
      " [-0.17247145 -0.4994753  -0.05377672]\n",
      " [ 0.14430666 -0.54461473 -0.03090633]\n",
      " [-0.32217076 -0.5845331  -0.08351779]\n",
      " [ 0.1352469  -0.5789426  -0.02217976]\n",
      " [-0.33411983 -0.62724733 -0.13406767]\n",
      " [ 0.11857966 -0.58959824 -0.04421883]\n",
      " [-0.30619952 -0.65674794 -0.14033641]\n",
      " [ 0.13962431 -0.533594   -0.04002995]\n",
      " [-0.32007053 -0.60172904 -0.09692235]\n",
      " [ 0.05049127 -0.0018556   0.00790542]\n",
      " [-0.0485956  -0.00113658 -0.00433628]\n",
      " [ 0.11502459  0.2840918   0.04747445]\n",
      " [-0.05322852  0.24029574  0.08932951]\n",
      " [ 0.14734772  0.5199347   0.23229995]\n",
      " [-0.07118332  0.5179724   0.2826942 ]\n",
      " [ 0.13367084  0.54242045  0.2601168 ]\n",
      " [-0.07182921  0.541233    0.24959905]\n",
      " [ 0.12196927  0.57154816  0.2570243 ]\n",
      " [-0.07216683  0.572454    0.23920183]]\n",
      "  → first landmark: (-0.033790648, -0.6112996, -0.22619084)\n",
      "\n",
      "Frame #001 (33×3):\n",
      "[[-0.01134037 -0.5697082  -0.339561  ]\n",
      " [ 0.00353809 -0.5859319  -0.34309617]\n",
      " [ 0.00270209 -0.5884564  -0.33378896]\n",
      " [ 0.00201175 -0.5885314  -0.33455902]\n",
      " [-0.01271034 -0.59487945 -0.34993634]\n",
      " [-0.01165643 -0.593975   -0.36170712]\n",
      " [-0.01075809 -0.57910186 -0.3406161 ]\n",
      " [ 0.07479228 -0.5783957  -0.28210232]\n",
      " [-0.02705817 -0.537724   -0.2474346 ]\n",
      " [ 0.03136193 -0.5524706  -0.30781525]\n",
      " [-0.00508686 -0.5230155  -0.32598764]\n",
      " [ 0.1856992  -0.48319736 -0.18188453]\n",
      " [-0.05337023 -0.5305265  -0.10725395]\n",
      " [ 0.30533957 -0.5076612  -0.24926643]\n",
      " [-0.18747927 -0.47846928 -0.1686121 ]\n",
      " [ 0.40025973 -0.51844925 -0.28818202]\n",
      " [-0.3567456  -0.5400336  -0.23759778]\n",
      " [ 0.4252214  -0.5436983  -0.29650414]\n",
      " [-0.37466687 -0.5707025  -0.28731832]\n",
      " [ 0.4044413  -0.5539673  -0.32024327]\n",
      " [-0.34815052 -0.60103    -0.30733863]\n",
      " [ 0.39781106 -0.5031106  -0.30399245]\n",
      " [-0.35498336 -0.55496377 -0.2580629 ]\n",
      " [ 0.07222402 -0.00160784  0.0128601 ]\n",
      " [-0.06938923 -0.00155623 -0.00999647]\n",
      " [ 0.08928943  0.31117028  0.06024915]\n",
      " [-0.10052951  0.25218678  0.05140337]\n",
      " [ 0.1335875   0.57052726  0.22892243]\n",
      " [-0.15915288  0.5619467   0.21901147]\n",
      " [ 0.12346359  0.590925    0.25226566]\n",
      " [-0.16072083  0.5869104   0.19796352]\n",
      " [ 0.13690576  0.60883754  0.23850642]\n",
      " [-0.17555799  0.603654    0.1471466 ]]\n",
      "  → first landmark: (-0.0113403695, -0.5697082, -0.339561)\n",
      "\n",
      "Frame #002 (33×3):\n",
      "[[-0.0168165  -0.57375824 -0.36536366]\n",
      " [-0.00349796 -0.58968115 -0.36685658]\n",
      " [-0.00404231 -0.5920712  -0.3569905 ]\n",
      " [-0.00467073 -0.5923622  -0.35750297]\n",
      " [-0.01410367 -0.5981765  -0.3782469 ]\n",
      " [-0.01414973 -0.5977909  -0.3914376 ]\n",
      " [-0.01312186 -0.5821221  -0.37337747]\n",
      " [ 0.05679294 -0.572806   -0.29834253]\n",
      " [-0.0476275  -0.5310251  -0.2819756 ]\n",
      " [ 0.02337127 -0.55599713 -0.3263316 ]\n",
      " [-0.0054874  -0.5257816  -0.3534069 ]\n",
      " [ 0.17624158 -0.4816474  -0.20698948]\n",
      " [-0.09889796 -0.52954847 -0.11612497]\n",
      " [ 0.32004458 -0.51815015 -0.27747568]\n",
      " [-0.2538115  -0.42914388 -0.22348595]\n",
      " [ 0.479071   -0.5206693  -0.31443927]\n",
      " [-0.4472741  -0.45803884 -0.3754344 ]\n",
      " [ 0.5139305  -0.5360094  -0.3153151 ]\n",
      " [-0.47330606 -0.47658482 -0.43610725]\n",
      " [ 0.490114   -0.54833627 -0.3450243 ]\n",
      " [-0.44490942 -0.51193917 -0.46239042]\n",
      " [ 0.4791889  -0.50126463 -0.33063355]\n",
      " [-0.44496435 -0.465993   -0.40077347]\n",
      " [ 0.08651342 -0.00139352  0.02232434]\n",
      " [-0.08341713 -0.00188619 -0.01804559]\n",
      " [ 0.14964634  0.36578354  0.05097246]\n",
      " [-0.11824293  0.26294428 -0.02509112]\n",
      " [ 0.20363103  0.61495334  0.19673198]\n",
      " [-0.19085191  0.5981499   0.14407939]\n",
      " [ 0.19493678  0.6042932   0.21519588]\n",
      " [-0.19582     0.62702686  0.12640719]\n",
      " [ 0.23405242  0.69470036  0.18596604]\n",
      " [-0.22132283  0.64795923  0.03823205]]\n",
      "  → first landmark: (-0.0168165, -0.57375824, -0.36536366)\n",
      "\n",
      "Overall coordinate stats:\n",
      "  x: min=-0.633, max=0.785, mean=0.061\n",
      "  y: min=-0.686, max=0.848, mean=-0.109\n",
      "  z: min=-0.742, max=0.672, mean=-0.146\n"
     ]
    }
   ],
   "source": [
    "# ─── edit this to your target file ────────────────────────────────────────────\n",
    "file_path = Path(\"Data-REHAB24-6/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# load\n",
    "arr = np.load(file_path)\n",
    "\n",
    "# basic info\n",
    "print(f\"Loaded: {file_path}\")\n",
    "print(f\" dtype: {arr.dtype}\")\n",
    "print(f\" shape: {arr.shape}  (frames × landmarks × coords)\\n\")\n",
    "\n",
    "# show first few frames\n",
    "n_show = min(3, arr.shape[0])\n",
    "for i in range(n_show):\n",
    "    print(f\"Frame #{i:03d} (33×3):\")\n",
    "    print(arr[i])\n",
    "    print(f\"  → first landmark: {tuple(arr[i,0])}\\n\")\n",
    "\n",
    "# overall statistics\n",
    "print(\"Overall coordinate stats:\")\n",
    "for idx, name in enumerate((\"x\", \"y\", \"z\")):\n",
    "    data = arr[..., idx]\n",
    "    print(f\"  {name}: min={data.min():.3f}, max={data.max():.3f}, mean={data.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a70d4",
   "metadata": {},
   "source": [
    "# 3. Data Preparation for Pose Error Analysis  (Sliding Window-Based Pose Error Calculation for Video segments)\n",
    "\n",
    "This script processes keypoint data from videos to compute errors in body joint angles relative to \"ideal\" angles, and then generates sliding windows of these errors. This is done to prepare features that will later be used for model training in tasks like exercise recognition or pose correction.\n",
    "\n",
    "What is done: For each exercise in the dataset, the script calculates the \"ideal\" joint angles by selecting the middle frame of each correct repetition (based on the correctness label). It calculates the angles between the joints defined in JOINT_TRIPLETS for each of these frames.  \n",
    "\n",
    "The median angle is then computed for each joint across the correct repetitions of the exercise. These median values represent the \"ideal\" angles the model should aim for in perfect executions of the exercise.  \n",
    "\n",
    "In the next step, the script generates sliding windows of angular errors, calculated as the difference between the observed angles in the video and the ideal angles. These windows contain temporal sequences of error data that are used as features for model training. The windowed data is then saved in a CSV file, which includes additional metadata such as exercise ID, repetition number, and frame indices.  \n",
    "\n",
    "Why it’s done: Calculating the ideal angles for each exercise provides a reference for identifying errors during subsequent video frames. These ideal angles will serve as the baseline for assessing whether a movement is performed correctly or incorrectly. By generating and saving the sliding window data, the script prepares the dataset for supervised learning, allowing the model to analyze temporal error patterns over a series of frames. This windowed approach is crucial for the model to learn dynamic movements and classify whether exercises are performed correctly based on the computed joint angles. The segmentation data in the CSV file offers a structured and labeled dataset, which helps in efficient training and evaluation of the model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fa0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. helpers --------------------------------------------------\n",
    "# def angle_between(a,b,c):\n",
    "#     BA = a-b; BC = c-b\n",
    "#     cosθ = np.dot(BA,BC)/(np.linalg.norm(BA)*np.linalg.norm(BC))\n",
    "#     return math.degrees(math.acos(np.clip(cosθ,-1,1)))\n",
    "\n",
    "# PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "# JOINT_TRIPLETS = {\n",
    "#     \"LEFT_ELBOW\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "#                      PoseLandmark.LEFT_ELBOW.value,\n",
    "#                      PoseLandmark.LEFT_WRIST.value),\n",
    "#     \"RIGHT_ELBOW\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "#                      PoseLandmark.RIGHT_ELBOW.value,\n",
    "#                      PoseLandmark.RIGHT_WRIST.value),\n",
    "#     \"LEFT_SHOULDER\":  (PoseLandmark.LEFT_ELBOW.value,\n",
    "#                        PoseLandmark.LEFT_SHOULDER.value,\n",
    "#                        PoseLandmark.LEFT_HIP.value),\n",
    "#     \"RIGHT_SHOULDER\": (PoseLandmark.RIGHT_ELBOW.value,\n",
    "#                        PoseLandmark.RIGHT_SHOULDER.value,\n",
    "#                        PoseLandmark.RIGHT_HIP.value),\n",
    "#     \"LEFT_HIP\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "#                    PoseLandmark.LEFT_HIP.value,\n",
    "#                    PoseLandmark.LEFT_KNEE.value),\n",
    "#     \"RIGHT_HIP\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "#                    PoseLandmark.RIGHT_HIP.value,\n",
    "#                    PoseLandmark.RIGHT_KNEE.value),\n",
    "#     \"LEFT_KNEE\":  (PoseLandmark.LEFT_HIP.value,\n",
    "#                   PoseLandmark.LEFT_KNEE.value,\n",
    "#                   PoseLandmark.LEFT_ANKLE.value),\n",
    "#     \"RIGHT_KNEE\": (PoseLandmark.RIGHT_HIP.value,\n",
    "#                   PoseLandmark.RIGHT_KNEE.value,\n",
    "#                   PoseLandmark.RIGHT_ANKLE.value),\n",
    "#     \"SPINE\": (\n",
    "#        PoseLandmark.LEFT_HIP.value,       \n",
    "#        PoseLandmark.LEFT_SHOULDER.value,   \n",
    "#        PoseLandmark.RIGHT_SHOULDER.value   \n",
    "#     ),\n",
    "#     \"HEAD\": (\n",
    "#        PoseLandmark.LEFT_SHOULDER.value,\n",
    "#        PoseLandmark.NOSE.value,\n",
    "#        PoseLandmark.RIGHT_SHOULDER.value\n",
    "#     ),\n",
    "# }\n",
    "# ERR_JOINTS = list(JOINT_TRIPLETS.keys())\n",
    "# N_ERR = len(ERR_JOINTS)  # 10\n",
    "\n",
    "# # 2. load original metadata & keypoints -----------------------\n",
    "# DATA_ROOT    = Path(\"Data-REHAB24-6\")\n",
    "# KEYPT_ROOT   = DATA_ROOT/\"mp_keypoints\"\n",
    "# META_ORIG    = DATA_ROOT/\"Segmentation.xlsx\"\n",
    "# df           = pd.read_excel(META_ORIG, engine=\"openpyxl\")\n",
    "# df.columns   = df.columns.str.strip()\n",
    "\n",
    "# # 3. compute ideal_angles on correct reps ----------------------\n",
    "# ideal_angles = {}\n",
    "# correct = df[df.correctness==1]\n",
    "# for ex in correct.exercise_id.unique():\n",
    "#     all_ang = {jn:[] for jn in ERR_JOINTS}\n",
    "#     for _,r in correct[correct.exercise_id==ex].iterrows():\n",
    "#         vid, f0, f1 = r.video_id, int(r.first_frame), int(r.last_frame)\n",
    "#         files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "#         if not files: continue\n",
    "#         arr = np.load(files[0])\n",
    "#         seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "#         if len(seg)==0: continue\n",
    "#         mid = len(seg)//2\n",
    "#         frm = seg[mid]\n",
    "#         for jn in ERR_JOINTS:\n",
    "#             ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "#             ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "#             all_ang[jn].append(ang)\n",
    "#     # median\n",
    "#     ideal_angles[ex] = {jn:float(np.median(all_ang[jn])) for jn in all_ang if all_ang[jn]}\n",
    "\n",
    "# # 4. slide windows & write rows --------------------------------\n",
    "# WINDOW, STRIDE = 16, 8\n",
    "# rows = []\n",
    "# for _,r in df.iterrows():\n",
    "#     vid, ex, f0, f1 = r.video_id, int(r.exercise_id), int(r.first_frame), int(r.last_frame)\n",
    "#     files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "#     if not files: continue\n",
    "#     arr = np.load(files[0])                # (F,33,3)\n",
    "#     seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "#     if len(seg)<WINDOW: continue\n",
    "\n",
    "#     # per-frame errors\n",
    "#     pf_err = {jn:[] for jn in ERR_JOINTS}\n",
    "#     for frm in seg:\n",
    "#         for jn in ERR_JOINTS:\n",
    "#             ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "#             ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "#             pf_err[jn].append(ang - ideal_angles[ex].get(jn,ang))\n",
    "\n",
    "#     # slide\n",
    "#     for start in range(0, len(seg)-WINDOW+1, STRIDE):\n",
    "#         w = np.array([ pf_err[jn][start:start+WINDOW] for jn in ERR_JOINTS ])  # (10,WINDOW)\n",
    "#         mean_err = w.mean(axis=1)\n",
    "#         row = {\n",
    "#             \"video_id\":vid,\n",
    "#             \"exercise_id\":ex,\n",
    "#             \"repetition_number\":r.repetition_number,\n",
    "#             \"window_start\": f0+start,\n",
    "#             \"window_end\":   f0+start+WINDOW,\n",
    "#             \"correctness\":  r.correctness\n",
    "#         }\n",
    "#         for i,jn in enumerate(ERR_JOINTS):\n",
    "#             row[f\"err_{i}\"] = float(mean_err[i])\n",
    "#         rows.append(row)\n",
    "\n",
    "# win_df = pd.DataFrame(rows)\n",
    "# win_df.to_csv(DATA_ROOT/\"Segmentation_windows.csv\", index=False)\n",
    "# print(\"Wrote\", len(win_df), \"windows to Segmentation_windows.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ea133",
   "metadata": {},
   "source": [
    "# 4. Traning Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa56f5d",
   "metadata": {},
   "source": [
    "## 4.1 Paths & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463f3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SCRIPT_DIR    = Path().resolve()\n",
    "DATA_ROOT     = SCRIPT_DIR/\"Data-REHAB24-6\"\n",
    "WIN_CSV       = DATA_ROOT/\"Segmentation_windows.csv\"\n",
    "KEYPT_ROOT    = DATA_ROOT/\"mp_keypoints\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"► Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bfd1e",
   "metadata": {},
   "source": [
    "## 4.2 Joint names & count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97531384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOINT_NAMES: ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
      "N_JOINTS: 33\n"
     ]
    }
   ],
   "source": [
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "\n",
    "# Then:\n",
    "JOINT_NAMES = [lm.name for lm in PoseLandmark]\n",
    "N_JOINTS    = len(JOINT_NAMES)  # should be 33\n",
    "\n",
    "print(f\"JOINT_NAMES: {JOINT_NAMES}\")\n",
    "print(f\"N_JOINTS: {N_JOINTS}\")\n",
    "\n",
    "#  Exerciseses (Ex1…Ex6)\n",
    "NUM_EXERCISES = 6\n",
    "CKPT_FILE     = \"kp_pose_quality_windows_ex.pt\"  \n",
    "\n",
    "ERR_JOINTS   = [\n",
    "  \"LEFT_ELBOW\",\"RIGHT_ELBOW\",\n",
    "  \"LEFT_SHOULDER\",\"RIGHT_SHOULDER\",\n",
    "  \"LEFT_HIP\",\"RIGHT_HIP\",\n",
    "  \"LEFT_KNEE\",\"RIGHT_KNEE\",\n",
    "  \"SPINE\",\"HEAD\",\n",
    "]\n",
    "N_ERR = len(ERR_JOINTS)   # 10\n",
    "ERR_COLS = [f\"err_{i}\" for i in range(N_ERR)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583456b",
   "metadata": {},
   "source": [
    "## 4.3 Dataset class definition\n",
    "\n",
    "The KeypointWindowDataset class loads and processes pose keypoint data from videos for model training. It reads a CSV file containing metadata, including video IDs, exercise IDs, frame indices, and pre-calculated joint angle errors (ranging from 0 to 9). The data is sorted based on video ID, repetition number, and window start. For each sample, it loads the corresponding keypoint data (in .npy format), extracts a segment of frames based on the start and end indices, reshapes the keypoints into a 2D array, and converts them into a PyTorch tensor. It also retrieves the correctness label and the pre-calculated error values, which are stored in tensors. This class efficiently loads and processes the data in batches for training tasks like exercise recognition, where both pose keypoints and error features are used for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a911cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KeypointWindowDataset(Dataset):\n",
    "    def __init__(self, csv_file: Path, keypt_root: Path):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df.sort_values([\"video_id\",\"repetition_number\",\"window_start\"])\n",
    "        self.rows = df.to_dict(\"records\")\n",
    "        self.keypt_root = keypt_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        r   = self.rows[i]\n",
    "        ex  = int(r[\"exercise_id\"]) - 1      # zero‐based [0..NUM_EXERCISES-1]\n",
    "        vid = r[\"video_id\"]\n",
    "        f0, f1 = int(r[\"window_start\"]), int(r[\"window_end\"])\n",
    "\n",
    "        # load keypoints\n",
    "        arr = np.load(\n",
    "            next((self.keypt_root/f\"Ex{ex+1}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "        )  # shape (F,33,3)\n",
    "\n",
    "        seg = arr[f0:f1]            # (T, 33, 3)\n",
    "        seg = seg.reshape(len(seg), -1)  # (T, 99)\n",
    "        seq = torch.from_numpy(seg).float()\n",
    "\n",
    "        label = torch.tensor(r[\"correctness\"], dtype=torch.long)\n",
    "        err   = torch.tensor([r[f\"err_{j}\"] for j in range(N_ERR)],\n",
    "                             dtype=torch.float32)\n",
    "\n",
    "        return seq, label, err, ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81848f",
   "metadata": {},
   "source": [
    "## 4.4 Model definitions   \n",
    "\n",
    "1. KeypointEncoder Class: Feature Extraction  \n",
    "The KeypointEncoder class is responsible for extracting feature representations from the input keypoint data. It uses two 1D convolutional layers (conv1 and conv2) to process the input sequence of keypoints. The input tensor, which represents keypoints for each frame in a video, is passed through these convolutional layers after being reshaped to fit the 1D convolution. Each convolution layer is followed by a ReLU activation function to introduce non-linearity. The final step of the encoder involves an adaptive average pooling (pool), which reduces the feature map to a single value per feature channel. This results in a compact representation of the keypoint sequence, which is then passed forward for further processing.  \n",
    "\n",
    "2. PoseQualityNetKP Class: Overview  \n",
    "The PoseQualityNetKP class is the main model used for pose quality assessment. It integrates the KeypointEncoder to process the raw keypoint data and extracts meaningful features. The model then uses an LSTM (Long Short-Term Memory) network to learn the temporal dependencies between the keypoint sequences. The LSTM consists of two bidirectional layers, allowing the model to capture information from both past and future frames in the sequence. The LSTM outputs a sequence of hidden states, which are averaged across the time dimension to produce a fixed-size feature vector representing the entire sequence of frames. This vector, along with the exercise embedding, is used to make predictions.  \n",
    "\n",
    "3. Exercise Embedding and Final Layers  \n",
    "In addition to the keypoint features, the model incorporates an exercise-specific embedding to capture the variations between different exercises. The ex_emb layer processes the one-hot encoded exercise ID into a dense representation. This embedding is passed through a small multi-layer perceptron (MLP) that reduces the embedding size, enabling the model to focus on the most important characteristics of each exercise. The concatenation of the temporal features from the LSTM and the exercise embedding forms the final input to the classification and error prediction heads. These final heads, cls_head and err_head, are fully connected layers that output the classification of the exercise and the pose errors, respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a49b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model definitions\n",
    "class KeypointEncoder(nn.Module):\n",
    "    def __init__(self, in_dim:int, embed:int=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, embed, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, D); treat as (B, D, 1) for Conv1d\n",
    "        x = x.unsqueeze(2)                 # → (B, D, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return self.pool(x).squeeze(-1)    # → (B, embed)\n",
    "\n",
    "class PoseQualityNetKP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 num_ex: int,\n",
    "                 hidden: int = 256,\n",
    "                 ex_emb: int = 64):\n",
    "        super().__init__()\n",
    "        # keypoint feature extractor\n",
    "        self.encoder = KeypointEncoder(in_dim)\n",
    "\n",
    "        # sequence model\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        feat_dim = hidden * 2\n",
    "\n",
    "        # exercise embedding MLP\n",
    "        self.ex_emb = nn.Sequential(\n",
    "            nn.Linear(num_ex, ex_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ex_emb, ex_emb)\n",
    "        )\n",
    "\n",
    "        # final heads\n",
    "        self.cls_head = nn.Linear(feat_dim + ex_emb, 2)\n",
    "        self.err_head = nn.Linear(feat_dim + ex_emb, N_ERR)\n",
    "\n",
    "    def forward(self,\n",
    "                seq:     torch.Tensor,  # (B, T, D)\n",
    "                ex_1hot: torch.Tensor   # (B, num_ex)\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # 1) keypoint → sequence feats\n",
    "        # encode each frame\n",
    "        B,T,_ = seq.shape\n",
    "        feats = torch.stack([\n",
    "            self.encoder(seq[:,t]) for t in range(T)\n",
    "        ], dim=1)                                # (B, T, 512)\n",
    "        out, _ = self.lstm(feats)                # (B, T, 2*hidden)\n",
    "        g = out.mean(1)                          # (B, 2*hidden)\n",
    "\n",
    "        # 2) exercise embed\n",
    "        ex_e = self.ex_emb(ex_1hot)              # (B, ex_emb)\n",
    "\n",
    "        # 3) concat and heads\n",
    "        h = torch.cat([g, ex_e], dim=1)          # (B, feat_dim+ex_emb)\n",
    "        return self.cls_head(h), self.err_head(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc70cc",
   "metadata": {},
   "source": [
    "# 5. Model Training  \n",
    "\n",
    "The PoseQualityNetKP model is designed for pose quality assessment, utilizing keypoint data to evaluate exercise movements. The model consists of a Keypoint Encoder to extract meaningful features from raw keypoint sequences, followed by an LSTM layer for learning temporal dependencies across frames. Additionally, the model incorporates an exercise-specific embedding layer to capture the variations between different exercises, and two final output heads for classification and error prediction.  \n",
    "\n",
    "During the training process, the model receives keypoint data sequences and exercise one-hot encodings as inputs. The keypoint sequences are first processed by the KeypointEncoder, which uses 1D convolutions to extract feature representations from each frame in the sequence. These frame-level features are then passed through a bidirectional LSTM to capture both past and future context in the sequence. The resulting features are aggregated by averaging the LSTM output over time, producing a fixed-size feature vector for each input sequence.  \n",
    "\n",
    "The exercise information is encoded through the exercise embedding MLP, which maps the one-hot encoded exercise IDs to a dense embedding representation. The temporal features from the LSTM and the exercise embeddings are concatenated together and passed through two separate heads: the classification head (cls_head) to predict the correctness of the exercise repetition (correct/incorrect), and the error head (err_head) to predict the pose errors for each joint, based on the ideal joint angles.  \n",
    "\n",
    "During training, the model optimizes two loss functions:\n",
    "\n",
    "- Cross-entropy loss (loss_cls): This loss is used to train the model to classify the exercise as correct or incorrect.  \n",
    "\n",
    "- Smooth L1 loss (loss_err): This loss is used to predict the joint angle errors, aiming to minimize the difference between predicted and actual errors.  \n",
    "\n",
    "The model is trained using the Adam optimizer with a learning rate of 1e-4. The training process iterates over batches of data, updating the model parameters to minimize the combined loss. During each epoch, the model's performance is validated using a validation set, where metrics such as accuracy, F1 score, precision, recall, and mean absolute error (MAE) are calculated. The model with the best F1 score is saved as the final trained model.  \n",
    "\n",
    "This model training framework prepares the system for accurate pose quality assessment and error prediction, allowing for fine-tuned classification of exercise correctness and detailed joint error analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 629/629 [00:13<00:00, 47.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 2.7361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ val acc 0.585, Precision 0.617, Recall 0.585, F1 0.559, MAE° 19.60\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 629/629 [00:12<00:00, 49.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 2.4140\n",
      "  ↳ val acc 0.633, Precision 0.674, Recall 0.633, F1 0.633, MAE° 17.05\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.633)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 629/629 [00:12<00:00, 49.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 2.1987\n",
      "  ↳ val acc 0.637, Precision 0.670, Recall 0.637, F1 0.635, MAE° 15.78\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 629/629 [00:12<00:00, 48.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 2.0483\n",
      "  ↳ val acc 0.695, Precision 0.725, Recall 0.695, F1 0.693, MAE° 14.72\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 629/629 [00:12<00:00, 48.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.9148\n",
      "  ↳ val acc 0.725, Precision 0.765, Recall 0.725, F1 0.717, MAE° 13.80\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.717)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 629/629 [00:13<00:00, 48.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.7716\n",
      "  ↳ val acc 0.733, Precision 0.772, Recall 0.733, F1 0.725, MAE° 12.92\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.725)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 629/629 [00:12<00:00, 48.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.6460\n",
      "  ↳ val acc 0.748, Precision 0.782, Recall 0.748, F1 0.743, MAE° 11.81\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.743)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 629/629 [00:12<00:00, 48.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.5368\n",
      "  ↳ val acc 0.776, Precision 0.801, Recall 0.776, F1 0.777, MAE° 10.89\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.777)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 629/629 [00:13<00:00, 48.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.4314\n",
      "  ↳ val acc 0.786, Precision 0.830, Recall 0.786, F1 0.779, MAE° 10.18\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 629/629 [00:13<00:00, 47.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.3509\n",
      "  ↳ val acc 0.802, Precision 0.823, Recall 0.802, F1 0.801, MAE° 9.57\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 629/629 [07:16<00:00,  1.44it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.2768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ val acc 0.740, Precision 0.806, Recall 0.740, F1 0.723, MAE° 9.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 629/629 [00:12<00:00, 50.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.2156\n",
      "  ↳ val acc 0.809, Precision 0.840, Recall 0.809, F1 0.807, MAE° 8.57\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 629/629 [00:12<00:00, 49.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.1549\n",
      "  ↳ val acc 0.800, Precision 0.821, Recall 0.800, F1 0.799, MAE° 8.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 629/629 [00:12<00:00, 49.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.1053\n",
      "  ↳ val acc 0.830, Precision 0.851, Recall 0.830, F1 0.829, MAE° 7.74\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 629/629 [00:12<00:00, 49.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.0451\n",
      "  ↳ val acc 0.843, Precision 0.862, Recall 0.843, F1 0.843, MAE° 7.32\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.843)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 629/629 [00:12<00:00, 49.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 1.0001\n",
      "  ↳ val acc 0.831, Precision 0.849, Recall 0.831, F1 0.831, MAE° 7.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 629/629 [00:12<00:00, 49.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.9678\n",
      "  ↳ val acc 0.833, Precision 0.854, Recall 0.832, F1 0.831, MAE° 6.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 629/629 [00:12<00:00, 49.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.9310\n",
      "  ↳ val acc 0.830, Precision 0.846, Recall 0.829, F1 0.830, MAE° 6.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 629/629 [00:12<00:00, 49.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8960\n",
      "  ↳ val acc 0.793, Precision 0.851, Recall 0.792, F1 0.783, MAE° 6.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 629/629 [00:12<00:00, 48.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8619\n",
      "  ↳ val acc 0.828, Precision 0.854, Recall 0.828, F1 0.826, MAE° 6.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 629/629 [00:12<00:00, 48.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8339\n",
      "  ↳ val acc 0.873, Precision 0.887, Recall 0.873, F1 0.873, MAE° 5.86\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.873)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 629/629 [00:13<00:00, 48.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.8148\n",
      "  ↳ val acc 0.856, Precision 0.871, Recall 0.855, F1 0.855, MAE° 5.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 629/629 [00:13<00:00, 48.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7936\n",
      "  ↳ val acc 0.873, Precision 0.887, Recall 0.873, F1 0.873, MAE° 5.67\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.873)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 629/629 [00:13<00:00, 48.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7775\n",
      "  ↳ val acc 0.864, Precision 0.878, Recall 0.864, F1 0.864, MAE° 5.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 629/629 [00:13<00:00, 48.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7534\n",
      "  ↳ val acc 0.875, Precision 0.890, Recall 0.874, F1 0.874, MAE° 5.34\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.874)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 629/629 [00:13<00:00, 47.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7243\n",
      "  ↳ val acc 0.861, Precision 0.876, Recall 0.861, F1 0.861, MAE° 5.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 629/629 [00:12<00:00, 48.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7258\n",
      "  ↳ val acc 0.852, Precision 0.871, Recall 0.851, F1 0.850, MAE° 5.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 629/629 [00:13<00:00, 48.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.7086\n",
      "  ↳ val acc 0.872, Precision 0.894, Recall 0.872, F1 0.871, MAE° 5.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 629/629 [00:13<00:00, 47.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6898\n",
      "  ↳ val acc 0.830, Precision 0.871, Recall 0.829, F1 0.824, MAE° 5.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 629/629 [00:13<00:00, 48.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6800\n",
      "  ↳ val acc 0.854, Precision 0.882, Recall 0.854, F1 0.852, MAE° 4.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 629/629 [00:13<00:00, 47.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6585\n",
      "  ↳ val acc 0.874, Precision 0.885, Recall 0.874, F1 0.874, MAE° 4.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 629/629 [00:13<00:00, 48.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6433\n",
      "  ↳ val acc 0.895, Precision 0.904, Recall 0.895, F1 0.895, MAE° 4.74\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.895)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 629/629 [00:13<00:00, 48.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6344\n",
      "  ↳ val acc 0.881, Precision 0.894, Recall 0.880, F1 0.880, MAE° 4.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 629/629 [00:12<00:00, 48.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6187\n",
      "  ↳ val acc 0.888, Precision 0.902, Recall 0.887, F1 0.887, MAE° 4.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 629/629 [00:13<00:00, 48.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.6199\n",
      "  ↳ val acc 0.872, Precision 0.888, Recall 0.872, F1 0.873, MAE° 4.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 629/629 [00:13<00:00, 48.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5904\n",
      "  ↳ val acc 0.880, Precision 0.899, Recall 0.879, F1 0.879, MAE° 4.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 629/629 [00:13<00:00, 48.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5880\n",
      "  ↳ val acc 0.882, Precision 0.896, Recall 0.882, F1 0.882, MAE° 4.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 629/629 [00:12<00:00, 48.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5849\n",
      "  ↳ val acc 0.900, Precision 0.910, Recall 0.899, F1 0.900, MAE° 4.44\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 629/629 [00:13<00:00, 48.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5647\n",
      "  ↳ val acc 0.871, Precision 0.895, Recall 0.871, F1 0.869, MAE° 4.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 629/629 [00:12<00:00, 48.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5531\n",
      "  ↳ val acc 0.874, Precision 0.888, Recall 0.873, F1 0.874, MAE° 4.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 629/629 [00:13<00:00, 48.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5498\n",
      "  ↳ val acc 0.892, Precision 0.903, Recall 0.892, F1 0.892, MAE° 4.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 629/629 [00:13<00:00, 48.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5352\n",
      "  ↳ val acc 0.894, Precision 0.906, Recall 0.894, F1 0.894, MAE° 4.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 629/629 [23:23<00:00,  2.23s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5291\n",
      "  ↳ val acc 0.906, Precision 0.915, Recall 0.905, F1 0.906, MAE° 3.91\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.906)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 629/629 [00:12<00:00, 50.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5164\n",
      "  ↳ val acc 0.906, Precision 0.916, Recall 0.905, F1 0.906, MAE° 4.05\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.906)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 629/629 [00:12<00:00, 50.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5228\n",
      "  ↳ val acc 0.891, Precision 0.903, Recall 0.890, F1 0.891, MAE° 4.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 629/629 [00:12<00:00, 49.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5060\n",
      "  ↳ val acc 0.894, Precision 0.910, Recall 0.894, F1 0.893, MAE° 3.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 629/629 [00:12<00:00, 49.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.5018\n",
      "  ↳ val acc 0.911, Precision 0.923, Recall 0.911, F1 0.911, MAE° 3.74\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 629/629 [00:12<00:00, 49.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4978\n",
      "  ↳ val acc 0.898, Precision 0.911, Recall 0.898, F1 0.897, MAE° 3.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 629/629 [00:12<00:00, 49.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4728\n",
      "  ↳ val acc 0.914, Precision 0.924, Recall 0.914, F1 0.914, MAE° 3.66\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.914)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 629/629 [00:12<00:00, 49.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4796\n",
      "  ↳ val acc 0.903, Precision 0.916, Recall 0.903, F1 0.903, MAE° 3.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 629/629 [00:12<00:00, 49.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4860\n",
      "  ↳ val acc 0.904, Precision 0.917, Recall 0.904, F1 0.904, MAE° 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 629/629 [00:12<00:00, 49.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4629\n",
      "  ↳ val acc 0.900, Precision 0.913, Recall 0.900, F1 0.899, MAE° 3.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 629/629 [00:12<00:00, 48.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4489\n",
      "  ↳ val acc 0.914, Precision 0.924, Recall 0.913, F1 0.914, MAE° 3.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 629/629 [00:12<00:00, 48.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4368\n",
      "  ↳ val acc 0.896, Precision 0.914, Recall 0.896, F1 0.895, MAE° 3.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 629/629 [00:12<00:00, 49.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4656\n",
      "  ↳ val acc 0.900, Precision 0.910, Recall 0.900, F1 0.900, MAE° 3.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 629/629 [00:12<00:00, 49.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rehabtrainingpy312/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ val acc 0.891, Precision 0.909, Recall 0.891, F1 0.890, MAE° 3.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 629/629 [00:12<00:00, 48.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4448\n",
      "  ↳ val acc 0.907, Precision 0.919, Recall 0.906, F1 0.906, MAE° 3.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 629/629 [00:12<00:00, 48.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4340\n",
      "  ↳ val acc 0.915, Precision 0.925, Recall 0.915, F1 0.915, MAE° 3.54\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 629/629 [00:12<00:00, 48.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4173\n",
      "  ↳ val acc 0.905, Precision 0.922, Recall 0.905, F1 0.905, MAE° 3.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 629/629 [00:12<00:00, 48.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4160\n",
      "  ↳ val acc 0.894, Precision 0.910, Recall 0.894, F1 0.893, MAE° 3.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 629/629 [00:13<00:00, 47.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4161\n",
      "  ↳ val acc 0.909, Precision 0.919, Recall 0.909, F1 0.909, MAE° 3.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 629/629 [00:13<00:00, 47.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4166\n",
      "  ↳ val acc 0.909, Precision 0.922, Recall 0.908, F1 0.908, MAE° 3.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 629/629 [00:14<00:00, 44.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3986\n",
      "  ↳ val acc 0.873, Precision 0.893, Recall 0.873, F1 0.872, MAE° 3.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 629/629 [00:13<00:00, 47.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.4125\n",
      "  ↳ val acc 0.919, Precision 0.928, Recall 0.919, F1 0.919, MAE° 3.32\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 629/629 [00:13<00:00, 45.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3953\n",
      "  ↳ val acc 0.898, Precision 0.913, Recall 0.898, F1 0.899, MAE° 3.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 629/629 [00:13<00:00, 45.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3854\n",
      "  ↳ val acc 0.910, Precision 0.922, Recall 0.910, F1 0.910, MAE° 3.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 629/629 [00:13<00:00, 47.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3750\n",
      "  ↳ val acc 0.918, Precision 0.926, Recall 0.918, F1 0.918, MAE° 3.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 629/629 [00:13<00:00, 48.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3737\n",
      "  ↳ val acc 0.906, Precision 0.918, Recall 0.906, F1 0.906, MAE° 3.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 629/629 [00:13<00:00, 47.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3784\n",
      "  ↳ val acc 0.910, Precision 0.920, Recall 0.910, F1 0.910, MAE° 3.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 629/629 [00:13<00:00, 48.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3718\n",
      "  ↳ val acc 0.895, Precision 0.909, Recall 0.895, F1 0.895, MAE° 3.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 629/629 [00:12<00:00, 48.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3616\n",
      "  ↳ val acc 0.913, Precision 0.926, Recall 0.913, F1 0.913, MAE° 3.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 629/629 [00:13<00:00, 46.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3624\n",
      "  ↳ val acc 0.922, Precision 0.927, Recall 0.921, F1 0.922, MAE° 2.99\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.922)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 629/629 [00:13<00:00, 47.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3632\n",
      "  ↳ val acc 0.918, Precision 0.926, Recall 0.918, F1 0.918, MAE° 3.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 629/629 [00:13<00:00, 47.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3699\n",
      "  ↳ val acc 0.919, Precision 0.926, Recall 0.919, F1 0.919, MAE° 3.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 629/629 [00:13<00:00, 47.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3454\n",
      "  ↳ val acc 0.924, Precision 0.934, Recall 0.924, F1 0.924, MAE° 3.06\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.924)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 629/629 [00:13<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3536\n",
      "  ↳ val acc 0.920, Precision 0.932, Recall 0.920, F1 0.919, MAE° 3.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 629/629 [00:13<00:00, 47.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3404\n",
      "  ↳ val acc 0.918, Precision 0.927, Recall 0.918, F1 0.918, MAE° 3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 629/629 [00:13<00:00, 47.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3417\n",
      "  ↳ val acc 0.928, Precision 0.936, Recall 0.928, F1 0.928, MAE° 2.97\n",
      "  ✓ saved new best model to kp_pose_quality_windows_ex.pt  (F1 0.928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 629/629 [00:13<00:00, 47.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3464\n",
      "  ↳ val acc 0.924, Precision 0.932, Recall 0.924, F1 0.924, MAE° 3.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 629/629 [00:13<00:00, 47.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↳ train loss: 0.3417\n",
      "  ↳ val acc 0.917, Precision 0.926, Recall 0.917, F1 0.916, MAE° 2.94\n"
     ]
    }
   ],
   "source": [
    "def train_epochs(\n",
    "    csv_file:  str   = str(WIN_CSV),\n",
    "    keypt_root:str   = str(KEYPT_ROOT),\n",
    "    num_ex:    int   = NUM_EXERCISES,\n",
    "    epochs:    int   = 30,\n",
    "    batch:     int   = 16,\n",
    "    lr:        float = 1e-4,\n",
    "    ckpt_file: str   = CKPT_FILE\n",
    "):\n",
    "    # Build dataset and split\n",
    "    ds  = KeypointWindowDataset(Path(csv_file), Path(keypt_root))\n",
    "    N   = len(ds)\n",
    "    idx = np.arange(N); np.random.shuffle(idx)\n",
    "    c1, c2 = int(0.7*N), int(0.85*N)\n",
    "    train_idx, val_idx = idx[:c1], idx[c1:c2]\n",
    "\n",
    "    train_dl = DataLoader(Subset(ds, train_idx), batch_size=batch, shuffle=True)\n",
    "    val_dl   = DataLoader(Subset(ds, val_idx),   batch_size=batch, shuffle=False)\n",
    "\n",
    "    # Infer input dimension\n",
    "    sample_seq, _, _, _ = ds[0]\n",
    "    in_dim = sample_seq.shape[-1]\n",
    "\n",
    "    # Build model\n",
    "    model    = PoseQualityNetKP(in_dim, num_ex).to(DEVICE)\n",
    "    loss_cls = nn.CrossEntropyLoss()\n",
    "    loss_err = nn.SmoothL1Loss()\n",
    "    opt      = Adam(model.parameters(), lr)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # -- train --\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        for seq, y, err, ex in tqdm(train_dl, desc=f\"Epoch {epoch:02d}\"):\n",
    "            seq, y, err, ex = [x.to(DEVICE) for x in (seq, y, err, ex)]\n",
    "            # Build one-hot encoding for exercise\n",
    "            ex_1hot = F.one_hot(ex, num_ex).float()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits, err_hat = model(seq, ex_1hot)\n",
    "            loss = loss_cls(logits, y) + 0.1 * loss_err(err_hat, err)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += loss.item() * y.size(0)\n",
    "        print(f\"  ↳ train loss: {tot_loss/len(train_idx):.4f}\")\n",
    "\n",
    "        # -- validation --\n",
    "        model.eval()\n",
    "        y_true, y_pred, errs, precision, recall = [], [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for seq, y, err, ex in val_dl:\n",
    "                seq, y, err, ex = [x.to(DEVICE) for x in (seq, y, err, ex)]\n",
    "                ex_1hot = F.one_hot(ex, num_ex).float()\n",
    "                logits, err_hat = model(seq, ex_1hot)\n",
    "\n",
    "                y_true += y.cpu().tolist()\n",
    "                y_pred += logits.argmax(1).cpu().tolist()\n",
    "                errs    += [(err_hat - err.to(DEVICE)).abs().mean(1)]\n",
    "                \n",
    "                # Precision and Recall\n",
    "                precision += [precision_score(y.cpu(), logits.argmax(1).cpu(), average='weighted')]\n",
    "                recall += [recall_score(y.cpu(), logits.argmax(1).cpu(), average='weighted')]\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1  = f1_score(y_true, y_pred, average='weighted')\n",
    "        mae = torch.cat(errs).mean().item()\n",
    "        precision_mean = np.mean(precision)\n",
    "        recall_mean = np.mean(recall)\n",
    "\n",
    "        print(f\"  ↳ val acc {acc:.3f}, Precision {precision_mean:.3f}, Recall {recall_mean:.3f}, F1 {f1:.3f}, MAE° {mae:.2f}\")\n",
    "\n",
    "         # Save if improved\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Save full model\n",
    "            torch.save(model, ckpt_file)\n",
    "            # Save state_dict separately\n",
    "            state_dict_path = Path(ckpt_file).with_suffix('.pth')\n",
    "            torch.save(model.state_dict(), state_dict_path)\n",
    "            print(f\"  ✓ Saved best model to {ckpt_file} and state_dict to {state_dict_path} (F1 {f1:.3f})\")\n",
    "\n",
    "train_epochs(epochs=80, batch=16, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b4529",
   "metadata": {},
   "source": [
    "# 6. Inference Tesing - Correctness and feedback on Live Videos / Recorded Videos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c92f0",
   "metadata": {},
   "source": [
    "This script provides a real-time system for exercise pose analysis and feedback, utilizing keypoint data from recorded videos or live camera streams. The system is built around a pre-trained model (PoseQualityNetKP) that classifies the correctness of an exercise and provides detailed feedback on joint angle errors for each frame of the video.\n",
    "\n",
    "- Model Setup and Loading: The model is first loaded from a checkpoint file (kp_pose_quality_windows_ex1.pt). The model consists of a keypoint encoder, a sequence model (LSTM), and two output heads for classification (correct/incorrect) and error prediction (joint angle errors).\n",
    "\n",
    "- Exercise Selection: The user is prompted to select an exercise from a predefined list, and the corresponding exercise ID is used to guide the feedback generation.\n",
    "\n",
    "- MediaPipe Pose Estimation: The script uses MediaPipe to extract world keypoints (3D pose landmarks) from each frame of the video or camera feed. These keypoints are passed to the model for pose evaluation.\n",
    "\n",
    "- Pose Inference and Feedback:\n",
    "    - The keypoints from each frame are processed and buffered to maintain a sequence of frames, which are then fed into the model.\n",
    "    - The model predicts the correctness of the exercise and calculates the joint angle errors for each frame.\n",
    "    - If the exercise is classified as incorrect, the model suggests corrections based on the joint with the largest error. The suggestion is displayed on the video feed for 3 seconds.\n",
    "    - Wrong exercise detection: If the model detects significant errors across multiple joints, it flags a wrong exercise and displays a warning message.\n",
    "\n",
    "- Visual Feedback:\n",
    "    - The system draws pose landmarks on the video feed using MediaPipe's drawing utilities for visual feedback.\n",
    "    - Feedback about the exercise's correctness and suggestions for improvement are overlaid on the video.\n",
    "    - If joint errors exceed a certain threshold, the model displays detailed feedback on specific joints, highlighting areas where the user should focus on improving.\n",
    "\n",
    "- User Interaction: The video feed (from a file or camera) is processed frame by frame, and real-time feedback is provided to the user, allowing them to see their performance and improve their exercise form.\n",
    "\n",
    "This system serves as a real-time feedback tool for users performing exercises, enabling them to receive immediate corrective suggestions based on pose analysis. It is useful for applications like personal training, physical therapy, and fitness monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e242ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_26887/1446849506.py:126: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  infer_model = torch.load(CKPT_FILE, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► Using device: mps\n",
      "Loading model from kp_pose_quality_windows_ex1.pt...\n",
      "✅ Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745045005.890300 1145256 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Attention: Re-run necessary imports and definitions above if running standalone\n",
    "\n",
    "# 1. Paths & device (adjust if needed)\n",
    "SCRIPT_DIR    = Path().resolve()\n",
    "DATA_ROOT     = SCRIPT_DIR/\"Data-REHAB24-6\" # Make sure this path is correct\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"► Using device:\", DEVICE)\n",
    "\n",
    "# 2. Joint names & count\n",
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "JOINT_NAMES = [lm.name for lm in PoseLandmark]\n",
    "N_JOINTS    = len(JOINT_NAMES)  # should be 33\n",
    "\n",
    "#  Exerciseses (Ex1…Ex6)\n",
    "NUM_EXERCISES = 6\n",
    "CKPT_FILE     = \"kp_pose_quality_windows_ex1.pt\" # Check if this file exists\n",
    "\n",
    "ERR_JOINTS   = [\n",
    "  \"LEFT_ELBOW\",\"RIGHT_ELBOW\",\n",
    "  \"LEFT_SHOULDER\",\"RIGHT_SHOULDER\",\n",
    "  \"LEFT_HIP\",\"RIGHT_HIP\",\n",
    "  \"LEFT_KNEE\",\"RIGHT_KNEE\",\n",
    "  \"SPINE\",\"HEAD\", # Make sure these match the training order\n",
    "]\n",
    "N_ERR = len(ERR_JOINTS)   # 10\n",
    "ERR_COLS = [f\"err_{i}\" for i in range(N_ERR)]\n",
    "\n",
    "# 5. Model definitions (Using the ORIGINAL definition from training)\n",
    "class KeypointEncoder(nn.Module):\n",
    "    # --- Restored ORIGINAL definition ---\n",
    "    def __init__(self, in_dim:int, embed:int=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, embed, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, D); treat as (B, D, 1) for Conv1d\n",
    "        # This encoder is designed to process features of a SINGLE frame (B, D)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(2)                 # → (B, D, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.pool(x).squeeze(-1)    # → (B, embed)\n",
    "\n",
    "\n",
    "class PoseQualityNetKP(nn.Module):\n",
    "    # --- Keep the PoseQualityNetKP class definition as in the original code ---\n",
    "    def __init__(self,\n",
    "                 in_dim: int, # Should be 99 (33*3)\n",
    "                 num_ex: int,\n",
    "                 hidden: int = 256,\n",
    "                 ex_emb: int = 64,\n",
    "                 embed: int = 512): # Added embed dim to match encoder\n",
    "        super().__init__()\n",
    "        # keypoint feature extractor (Uses the restored original encoder)\n",
    "        self.encoder = KeypointEncoder(in_dim, embed=embed)\n",
    "\n",
    "        # sequence model\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed, # Use embed dim here\n",
    "            hidden_size=hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        feat_dim = hidden * 2\n",
    "\n",
    "        # exercise embedding MLP\n",
    "        self.ex_emb = nn.Sequential(\n",
    "            nn.Linear(num_ex, ex_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ex_emb, ex_emb)\n",
    "        )\n",
    "\n",
    "        # final heads\n",
    "        self.cls_head = nn.Linear(feat_dim + ex_emb, 2) # 2 classes: incorrect, correct\n",
    "        self.err_head = nn.Linear(feat_dim + ex_emb, N_ERR)\n",
    "\n",
    "    def forward(self,\n",
    "                seq:     torch.Tensor,  # (B, T, D) where D=99\n",
    "                ex_1hot: torch.Tensor   # (B, num_ex)\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # 1) keypoint → sequence feats\n",
    "        # encode each frame\n",
    "        B, T, D = seq.shape # Now this should work\n",
    "        # Process sequence frame by frame using the encoder\n",
    "        frame_embeddings = []\n",
    "        for t in range(T):\n",
    "            frame_data = seq[:, t, :] # Get data for frame t: (B, D)\n",
    "            frame_embedding = self.encoder(frame_data) # Output: (B, embed)\n",
    "            frame_embeddings.append(frame_embedding)\n",
    "\n",
    "        feats = torch.stack(frame_embeddings, dim=1) # (B, T, embed)\n",
    "\n",
    "        # 2) sequence model (LSTM)\n",
    "        out, _ = self.lstm(feats)                # (B, T, 2*hidden)\n",
    "        # Aggregate LSTM outputs (e.g., mean pooling over time)\n",
    "        g = out.mean(dim=1)                      # (B, 2*hidden)\n",
    "\n",
    "        # 3) exercise embed\n",
    "        ex_e = self.ex_emb(ex_1hot)              # (B, ex_emb)\n",
    "\n",
    "        # 4) concat and heads\n",
    "        h = torch.cat([g, ex_e], dim=1)          # (B, feat_dim + ex_emb)\n",
    "        logits = self.cls_head(h)                # (B, 2)\n",
    "        err_hat = self.err_head(h)               # (B, N_ERR)\n",
    "\n",
    "        return logits, err_hat\n",
    "# --- End of re-included definitions ---\n",
    "\n",
    "\n",
    "# Load model\n",
    "if not Path(CKPT_FILE).exists():\n",
    "    print(f\"Error: Checkpoint file not found at {CKPT_FILE}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading model from {CKPT_FILE}...\")\n",
    "# Load the state dict if you have the class defined, or the whole model if saved that way\n",
    "# Assuming the whole model was saved with torch.save(model, ckpt_file)\n",
    "infer_model = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "infer_model.eval()\n",
    "print(\"✅ Model loaded.\")\n",
    "\n",
    "# Exercise map\n",
    "EXERCISE_MAP = {\n",
    "    1: \"Arm abduction\",\n",
    "    2: \"Arm VW\",\n",
    "    3: \"Push-ups\",\n",
    "    4: \"Leg abduction\",\n",
    "    5: \"Leg lunge\",\n",
    "    6: \"Squats\"\n",
    "}\n",
    "NUM_EXERCISES = len(EXERCISE_MAP) # Ensure consistency\n",
    "\n",
    "# Ask user for exercise ID\n",
    "while True:\n",
    "    try:\n",
    "        exercise_id_str = input(f\"Enter the exercise ID you're performing (1-{len(EXERCISE_MAP)}): \")\n",
    "        exercise_id = int(exercise_id_str)\n",
    "        if 1 <= exercise_id <= len(EXERCISE_MAP):\n",
    "            exercise_name = EXERCISE_MAP[exercise_id]\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid ID. Please enter a number between 1 and {len(EXERCISE_MAP)}.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# MediaPipe Pose Setup --- MODIFIED ---\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2, # <<< MATCHED training complexity\n",
    "    enable_segmentation=False, # Keep false if not used\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Keypoints extraction function --- MODIFIED ---\n",
    "def extract_keypoints(frame):\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_rgb.flags.writeable = False\n",
    "    result = pose.process(img_rgb)\n",
    "    img_rgb.flags.writeable = True\n",
    "\n",
    "    keypoints = None\n",
    "    landmarks_for_drawing = None # Still useful to draw image landmarks\n",
    "\n",
    "    # <<< USE pose_world_landmarks >>>\n",
    "    if result.pose_world_landmarks:\n",
    "        world_landmarks = result.pose_world_landmarks.landmark\n",
    "        # Store x, y, z world coordinates\n",
    "        keypoints = np.array([(lm.x, lm.y, lm.z) for lm in world_landmarks], dtype=np.float32)\n",
    "\n",
    "    # Get image landmarks separately for drawing (optional but helpful)\n",
    "    if result.pose_landmarks:\n",
    "         landmarks_for_drawing = result.pose_landmarks\n",
    "\n",
    "    # Return world keypoints (for model) and image landmarks (for drawing)\n",
    "    return keypoints, landmarks_for_drawing\n",
    "\n",
    "# Inference parameters\n",
    "SEQUENCE_LENGTH = 16 # Match the buffer size used in inference\n",
    "IN_DIM = N_JOINTS * 3 # 33 * 3 = 99\n",
    "\n",
    "# Inference for correctness (model prediction and feedback)\n",
    "def infer_and_feedback(model, video_path, selected_ex_id, selected_ex_name):\n",
    "    # Check if a video path is provided or use the camera\n",
    "    if video_path is None:\n",
    "        cap = cv2.VideoCapture(0)  # 0 refers to the default camera (webcam)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not access the camera.\")\n",
    "            return\n",
    "        print(\"Using camera feed...\")\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(video_path)  # Use the provided video path\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video file {video_path}\")\n",
    "            return\n",
    "        print(f\"Using video: {video_path}\")\n",
    "\n",
    "    keypoints_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    feedback = \"Initializing...\"\n",
    "    err_values = np.zeros(N_ERR)  # Store last error values\n",
    "    predicted_class = 0  # Default to incorrect initially\n",
    "    suggestion = \"\"  # Variable to store the suggestion\n",
    "    suggestion_time = 0  # Timer to track suggestion display duration (in frames)\n",
    "    SUGGESTION_DURATION = 3 * 30  # 3 seconds (assuming 30 fps, so 3 * 30 frames)\n",
    "    \n",
    "    wrong_exercise_detected = False  # Flag to track if a wrong exercise is detected\n",
    "    wrong_exercise_time = 0  # Timer to track the duration of wrong exercise detection\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or cannot read frame.\")\n",
    "            break\n",
    "\n",
    "        # --- Get WORLD keypoints for model, IMAGE landmarks for drawing ---\n",
    "        world_keypoints, image_landmarks_for_drawing = extract_keypoints(frame)\n",
    "\n",
    "        # --- Draw IMAGE landmarks if detected ---\n",
    "        if image_landmarks_for_drawing:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                image_landmarks_for_drawing,  # Use image landmarks here\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # --- Use WORLD keypoints for the model ---\n",
    "        if world_keypoints is not None:\n",
    "            keypoints_buffer.append(world_keypoints)  # Add WORLD keypoints to buffer\n",
    "\n",
    "            # Check if buffer is full\n",
    "            if len(keypoints_buffer) == SEQUENCE_LENGTH:\n",
    "                # Prepare sequence for model using world keypoints\n",
    "                keypoints_array = np.array(keypoints_buffer, dtype=np.float32)  # (16, 33, 3) world coords\n",
    "                keypoints_flat = keypoints_array.reshape(SEQUENCE_LENGTH, -1)  # (16, 99) world coords\n",
    "                seq = torch.tensor(keypoints_flat, dtype=torch.float32).unsqueeze(0).to(DEVICE)  # (1, 16, 99)\n",
    "\n",
    "                ex_tensor = torch.tensor([selected_ex_id - 1], device=DEVICE)\n",
    "                ex_1hot = F.one_hot(ex_tensor, num_classes=NUM_EXERCISES).float()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    logits, err_hat = model(seq, ex_1hot)\n",
    "                    predicted_class = logits.argmax(1).item()  # 0: incorrect, 1: correct\n",
    "                    err_values = err_hat.squeeze().cpu().numpy()\n",
    "\n",
    "                feedback = \"Correct\" if predicted_class == 1 else \"Incorrect\"\n",
    "\n",
    "                # If the posture is incorrect, find the joint with the largest error and suggest correction\n",
    "                if predicted_class == 0:\n",
    "                    # Find the joint with the maximum error\n",
    "                    max_error_idx = np.argmax(np.abs(err_values))  # Get index of the joint with the largest error\n",
    "                    joint_with_error = ERR_JOINTS[max_error_idx]\n",
    "                    max_error = err_values[max_error_idx]\n",
    "                    suggestion = f\"Correct your {joint_with_error.replace('_', ' ')} (Error: {max_error:+.2f}°)\"\n",
    "                    suggestion_time = SUGGESTION_DURATION  # Reset suggestion timer for 3 seconds\n",
    "\n",
    "                # Detect wrong exercise based on high error across multiple joints for a sustained time\n",
    "                error_threshold = 10  # Adjust as needed (threshold for detecting wrong exercise)\n",
    "                wrong_joints = np.sum(np.abs(err_values) > error_threshold)  # Count joints with high error\n",
    "\n",
    "                if wrong_joints >= 4:  # For example, detect wrong exercise if 4 or more joints are off\n",
    "                    wrong_exercise_detected = True\n",
    "                    wrong_exercise_time += 1\n",
    "                    if wrong_exercise_time > 90:  # 3 seconds of high error across multiple joints (3*30 fps)\n",
    "                        # Show a large warning on the center of the screen\n",
    "                        frame_height, frame_width = frame.shape[:2]  # Get the frame dimensions\n",
    "                        text = \"WRONG EXERCISE DETECTED!\"\n",
    "                        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 2, 3)[0]\n",
    "                        text_x = (frame_width - text_size[0]) // 2  # Calculate X position to center text\n",
    "                        text_y = (frame_height + text_size[1]) // 2  # Calculate Y position to center text\n",
    "                        cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                else:\n",
    "                    wrong_exercise_detected = False\n",
    "                    wrong_exercise_time = 0  # Reset if exercise is correct\n",
    "\n",
    "            else:\n",
    "                feedback = f\"Collecting frames... {len(keypoints_buffer)}/{SEQUENCE_LENGTH}\"\n",
    "\n",
    "        else:\n",
    "            # Handle case where no world keypoints are detected\n",
    "            feedback = \"No pose detected\"\n",
    "            keypoints_buffer.clear()  # Clear buffer if detection lost\n",
    "            predicted_class = 0  # Reset prediction if pose lost\n",
    "            suggestion = \"\"  # Reset suggestion when no pose detected\n",
    "            suggestion_time = 0  # Reset suggestion timer\n",
    "\n",
    "        # Display feedback on frame\n",
    "        cv2.putText(frame, f\"Exercise: {selected_ex_name}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        feedback_color = (0, 255, 0) if predicted_class == 1 and len(keypoints_buffer) == SEQUENCE_LENGTH else (0, 0, 255)\n",
    "        cv2.putText(frame, f\"Feedback: {feedback}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, feedback_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the suggestion for 3 seconds (timer logic)\n",
    "        if suggestion_time > 0:\n",
    "            cv2.putText(frame, suggestion, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            suggestion_time -= 1  # Decrease timer on each frame\n",
    "\n",
    "        # Display specific joint feedback based on error values\n",
    "        feedback_y_start = 130\n",
    "        error_threshold = 0.15  # Adjust as needed\n",
    "\n",
    "        if len(keypoints_buffer) == SEQUENCE_LENGTH:  # Only show errors if inference ran\n",
    "            for i, joint_name in enumerate(ERR_JOINTS):\n",
    "                err_val = err_values[i]\n",
    "                if abs(err_val) > error_threshold:  # Use abs() as error can be +/-\n",
    "                    color = (0, 165, 255)  # Orange/Yellow for warning\n",
    "                    text = f\"{joint_name}: Check ({err_val:+.2f})\"\n",
    "                    cv2.putText(frame, text, (10, feedback_y_start + i * 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        cv2.imshow('Pose Estimation Feedback', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pose.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7576e91",
   "metadata": {},
   "source": [
    "Attention: Set Vedio path =None to do live inferencewith laptop camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745045005.949570 1211346 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745045005.990816 1211346 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using camera feed...\n",
      "Using camera feed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745045006.586156 1211345 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Main execution part ---\n",
    "# VIDEO_PATH = \"Data-REHAB24-6/Videos/Ex1/PM_001-Camera17-30fps.mp4\"\n",
    "#VIDEO_PATH = \"Data-REHAB24-6/Videos/Ex3/PM_010-Camera17-30fps.mp4\"\n",
    "# --- Main execution part ---\n",
    "VIDEO_PATH = None  # This is where you set the video path (None for camera feed)\n",
    "\n",
    "# Check if VIDEO_PATH is None or invalid\n",
    "if VIDEO_PATH is None:\n",
    "    print(\"Using camera feed...\")\n",
    "    cap = cv2.VideoCapture(0)  # 0 refers to the default camera (webcam)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not access the camera.\")\n",
    "        exit()\n",
    "else:\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)  # Use the provided video path\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {VIDEO_PATH}\")\n",
    "        exit()\n",
    "\n",
    "# Run the inference and feedback function\n",
    "infer_and_feedback(infer_model, VIDEO_PATH, exercise_id, exercise_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rehabtrainingpy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
