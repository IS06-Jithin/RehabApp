{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3370fa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Data-REHAB24-6/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\n",
      " dtype: float32\n",
      " shape: (5191, 33, 3)  (frames × landmarks × coords)\n",
      "\n",
      "Frame #000 (33×3):\n",
      "[[-0.03379065 -0.6112996  -0.22619084]\n",
      " [-0.02455677 -0.628223   -0.2275483 ]\n",
      " [-0.0257254  -0.6304051  -0.21732828]\n",
      " [-0.02547118 -0.6302204  -0.21820049]\n",
      " [-0.02778288 -0.6363151  -0.2358914 ]\n",
      " [-0.02677054 -0.63463634 -0.24706481]\n",
      " [-0.02263773 -0.61979294 -0.2281486 ]\n",
      " [ 0.02648694 -0.6184615  -0.16859515]\n",
      " [-0.03559308 -0.56201506 -0.14808732]\n",
      " [ 0.0030987  -0.59678274 -0.19447449]\n",
      " [-0.01712019 -0.55977213 -0.21580447]\n",
      " [ 0.1250833  -0.49333623 -0.08702794]\n",
      " [-0.05722423 -0.53108674 -0.02576461]\n",
      " [ 0.15178505 -0.51440114 -0.09251688]\n",
      " [-0.17247145 -0.4994753  -0.05377672]\n",
      " [ 0.14430666 -0.54461473 -0.03090633]\n",
      " [-0.32217076 -0.5845331  -0.08351779]\n",
      " [ 0.1352469  -0.5789426  -0.02217976]\n",
      " [-0.33411983 -0.62724733 -0.13406767]\n",
      " [ 0.11857966 -0.58959824 -0.04421883]\n",
      " [-0.30619952 -0.65674794 -0.14033641]\n",
      " [ 0.13962431 -0.533594   -0.04002995]\n",
      " [-0.32007053 -0.60172904 -0.09692235]\n",
      " [ 0.05049127 -0.0018556   0.00790542]\n",
      " [-0.0485956  -0.00113658 -0.00433628]\n",
      " [ 0.11502459  0.2840918   0.04747445]\n",
      " [-0.05322852  0.24029574  0.08932951]\n",
      " [ 0.14734772  0.5199347   0.23229995]\n",
      " [-0.07118332  0.5179724   0.2826942 ]\n",
      " [ 0.13367084  0.54242045  0.2601168 ]\n",
      " [-0.07182921  0.541233    0.24959905]\n",
      " [ 0.12196927  0.57154816  0.2570243 ]\n",
      " [-0.07216683  0.572454    0.23920183]]\n",
      "  → first landmark: (-0.033790648, -0.6112996, -0.22619084)\n",
      "\n",
      "Frame #001 (33×3):\n",
      "[[-0.01134037 -0.5697082  -0.339561  ]\n",
      " [ 0.00353809 -0.5859319  -0.34309617]\n",
      " [ 0.00270209 -0.5884564  -0.33378896]\n",
      " [ 0.00201175 -0.5885314  -0.33455902]\n",
      " [-0.01271034 -0.59487945 -0.34993634]\n",
      " [-0.01165643 -0.593975   -0.36170712]\n",
      " [-0.01075809 -0.57910186 -0.3406161 ]\n",
      " [ 0.07479228 -0.5783957  -0.28210232]\n",
      " [-0.02705817 -0.537724   -0.2474346 ]\n",
      " [ 0.03136193 -0.5524706  -0.30781525]\n",
      " [-0.00508686 -0.5230155  -0.32598764]\n",
      " [ 0.1856992  -0.48319736 -0.18188453]\n",
      " [-0.05337023 -0.5305265  -0.10725395]\n",
      " [ 0.30533957 -0.5076612  -0.24926643]\n",
      " [-0.18747927 -0.47846928 -0.1686121 ]\n",
      " [ 0.40025973 -0.51844925 -0.28818202]\n",
      " [-0.3567456  -0.5400336  -0.23759778]\n",
      " [ 0.4252214  -0.5436983  -0.29650414]\n",
      " [-0.37466687 -0.5707025  -0.28731832]\n",
      " [ 0.4044413  -0.5539673  -0.32024327]\n",
      " [-0.34815052 -0.60103    -0.30733863]\n",
      " [ 0.39781106 -0.5031106  -0.30399245]\n",
      " [-0.35498336 -0.55496377 -0.2580629 ]\n",
      " [ 0.07222402 -0.00160784  0.0128601 ]\n",
      " [-0.06938923 -0.00155623 -0.00999647]\n",
      " [ 0.08928943  0.31117028  0.06024915]\n",
      " [-0.10052951  0.25218678  0.05140337]\n",
      " [ 0.1335875   0.57052726  0.22892243]\n",
      " [-0.15915288  0.5619467   0.21901147]\n",
      " [ 0.12346359  0.590925    0.25226566]\n",
      " [-0.16072083  0.5869104   0.19796352]\n",
      " [ 0.13690576  0.60883754  0.23850642]\n",
      " [-0.17555799  0.603654    0.1471466 ]]\n",
      "  → first landmark: (-0.0113403695, -0.5697082, -0.339561)\n",
      "\n",
      "Frame #002 (33×3):\n",
      "[[-0.0168165  -0.57375824 -0.36536366]\n",
      " [-0.00349796 -0.58968115 -0.36685658]\n",
      " [-0.00404231 -0.5920712  -0.3569905 ]\n",
      " [-0.00467073 -0.5923622  -0.35750297]\n",
      " [-0.01410367 -0.5981765  -0.3782469 ]\n",
      " [-0.01414973 -0.5977909  -0.3914376 ]\n",
      " [-0.01312186 -0.5821221  -0.37337747]\n",
      " [ 0.05679294 -0.572806   -0.29834253]\n",
      " [-0.0476275  -0.5310251  -0.2819756 ]\n",
      " [ 0.02337127 -0.55599713 -0.3263316 ]\n",
      " [-0.0054874  -0.5257816  -0.3534069 ]\n",
      " [ 0.17624158 -0.4816474  -0.20698948]\n",
      " [-0.09889796 -0.52954847 -0.11612497]\n",
      " [ 0.32004458 -0.51815015 -0.27747568]\n",
      " [-0.2538115  -0.42914388 -0.22348595]\n",
      " [ 0.479071   -0.5206693  -0.31443927]\n",
      " [-0.4472741  -0.45803884 -0.3754344 ]\n",
      " [ 0.5139305  -0.5360094  -0.3153151 ]\n",
      " [-0.47330606 -0.47658482 -0.43610725]\n",
      " [ 0.490114   -0.54833627 -0.3450243 ]\n",
      " [-0.44490942 -0.51193917 -0.46239042]\n",
      " [ 0.4791889  -0.50126463 -0.33063355]\n",
      " [-0.44496435 -0.465993   -0.40077347]\n",
      " [ 0.08651342 -0.00139352  0.02232434]\n",
      " [-0.08341713 -0.00188619 -0.01804559]\n",
      " [ 0.14964634  0.36578354  0.05097246]\n",
      " [-0.11824293  0.26294428 -0.02509112]\n",
      " [ 0.20363103  0.61495334  0.19673198]\n",
      " [-0.19085191  0.5981499   0.14407939]\n",
      " [ 0.19493678  0.6042932   0.21519588]\n",
      " [-0.19582     0.62702686  0.12640719]\n",
      " [ 0.23405242  0.69470036  0.18596604]\n",
      " [-0.22132283  0.64795923  0.03823205]]\n",
      "  → first landmark: (-0.0168165, -0.57375824, -0.36536366)\n",
      "\n",
      "Overall coordinate stats:\n",
      "  x: min=-0.633, max=0.785, mean=0.061\n",
      "  y: min=-0.686, max=0.848, mean=-0.109\n",
      "  z: min=-0.742, max=0.672, mean=-0.146\n"
     ]
    }
   ],
   "source": [
    "# Look at a numpy file containing 3D keypoints\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ─── edit this to your target file ────────────────────────────────────────────\n",
    "file_path = Path(\"Data-REHAB24-6/mp_keypoints/Ex6/PM_008-Camera17-30fps-mp.npy\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# load\n",
    "arr = np.load(file_path)\n",
    "\n",
    "# basic info\n",
    "print(f\"Loaded: {file_path}\")\n",
    "print(f\" dtype: {arr.dtype}\")\n",
    "print(f\" shape: {arr.shape}  (frames × landmarks × coords)\\n\")\n",
    "\n",
    "# show first few frames\n",
    "n_show = min(3, arr.shape[0])\n",
    "for i in range(n_show):\n",
    "    print(f\"Frame #{i:03d} (33×3):\")\n",
    "    print(arr[i])\n",
    "    print(f\"  → first landmark: {tuple(arr[i,0])}\\n\")\n",
    "\n",
    "# overall statistics\n",
    "print(\"Overall coordinate stats:\")\n",
    "for idx, name in enumerate((\"x\", \"y\", \"z\")):\n",
    "    data = arr[..., idx]\n",
    "    print(f\"  {name}: min={data.min():.3f}, max={data.max():.3f}, mean={data.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d83788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 14375 windows to Segmentation_windows.csv\n"
     ]
    }
   ],
   "source": [
    "# generate_windows.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. helpers --------------------------------------------------\n",
    "def angle_between(a,b,c):\n",
    "    BA = a-b; BC = c-b\n",
    "    cosθ = np.dot(BA,BC)/(np.linalg.norm(BA)*np.linalg.norm(BC))\n",
    "    return math.degrees(math.acos(np.clip(cosθ,-1,1)))\n",
    "\n",
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "JOINT_TRIPLETS = {\n",
    "    \"LEFT_ELBOW\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "                     PoseLandmark.LEFT_ELBOW.value,\n",
    "                     PoseLandmark.LEFT_WRIST.value),\n",
    "    \"RIGHT_ELBOW\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                     PoseLandmark.RIGHT_ELBOW.value,\n",
    "                     PoseLandmark.RIGHT_WRIST.value),\n",
    "    \"LEFT_SHOULDER\":  (PoseLandmark.LEFT_ELBOW.value,\n",
    "                       PoseLandmark.LEFT_SHOULDER.value,\n",
    "                       PoseLandmark.LEFT_HIP.value),\n",
    "    \"RIGHT_SHOULDER\": (PoseLandmark.RIGHT_ELBOW.value,\n",
    "                       PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                       PoseLandmark.RIGHT_HIP.value),\n",
    "    \"LEFT_HIP\":   (PoseLandmark.LEFT_SHOULDER.value,\n",
    "                   PoseLandmark.LEFT_HIP.value,\n",
    "                   PoseLandmark.LEFT_KNEE.value),\n",
    "    \"RIGHT_HIP\":  (PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                   PoseLandmark.RIGHT_HIP.value,\n",
    "                   PoseLandmark.RIGHT_KNEE.value),\n",
    "    \"LEFT_KNEE\":  (PoseLandmark.LEFT_HIP.value,\n",
    "                  PoseLandmark.LEFT_KNEE.value,\n",
    "                  PoseLandmark.LEFT_ANKLE.value),\n",
    "    \"RIGHT_KNEE\": (PoseLandmark.RIGHT_HIP.value,\n",
    "                  PoseLandmark.RIGHT_KNEE.value,\n",
    "                  PoseLandmark.RIGHT_ANKLE.value),\n",
    "    \"SPINE\": (\n",
    "       PoseLandmark.LEFT_HIP.value,       \n",
    "       PoseLandmark.LEFT_SHOULDER.value,   \n",
    "       PoseLandmark.RIGHT_SHOULDER.value   \n",
    "    ),\n",
    "    \"HEAD\": (\n",
    "       PoseLandmark.LEFT_SHOULDER.value,\n",
    "       PoseLandmark.NOSE.value,\n",
    "       PoseLandmark.RIGHT_SHOULDER.value\n",
    "    ),\n",
    "}\n",
    "ERR_JOINTS = list(JOINT_TRIPLETS.keys())\n",
    "N_ERR = len(ERR_JOINTS)  # 10\n",
    "\n",
    "# 2. load original metadata & keypoints -----------------------\n",
    "DATA_ROOT    = Path(\"Data-REHAB24-6\")\n",
    "KEYPT_ROOT   = DATA_ROOT/\"mp_keypoints\"\n",
    "META_ORIG    = DATA_ROOT/\"Segmentation.xlsx\"\n",
    "df           = pd.read_excel(META_ORIG, engine=\"openpyxl\")\n",
    "df.columns   = df.columns.str.strip()\n",
    "\n",
    "# 3. compute ideal_angles on correct reps ----------------------\n",
    "ideal_angles = {}\n",
    "correct = df[df.correctness==1]\n",
    "for ex in correct.exercise_id.unique():\n",
    "    all_ang = {jn:[] for jn in ERR_JOINTS}\n",
    "    for _,r in correct[correct.exercise_id==ex].iterrows():\n",
    "        vid, f0, f1 = r.video_id, int(r.first_frame), int(r.last_frame)\n",
    "        files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "        if not files: continue\n",
    "        arr = np.load(files[0])\n",
    "        seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "        if len(seg)==0: continue\n",
    "        mid = len(seg)//2\n",
    "        frm = seg[mid]\n",
    "        for jn in ERR_JOINTS:\n",
    "            ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "            ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "            all_ang[jn].append(ang)\n",
    "    # median\n",
    "    ideal_angles[ex] = {jn:float(np.median(all_ang[jn])) for jn in all_ang if all_ang[jn]}\n",
    "\n",
    "# 4. slide windows & write rows --------------------------------\n",
    "WINDOW, STRIDE = 16, 8\n",
    "rows = []\n",
    "for _,r in df.iterrows():\n",
    "    vid, ex, f0, f1 = r.video_id, int(r.exercise_id), int(r.first_frame), int(r.last_frame)\n",
    "    files = list((KEYPT_ROOT/f\"Ex{ex}\").glob(f\"{vid}-Camera17*-mp.npy\"))\n",
    "    if not files: continue\n",
    "    arr = np.load(files[0])                # (F,33,3)\n",
    "    seg = arr[f0:f1] if f1>f0 else arr[f0:]\n",
    "    if len(seg)<WINDOW: continue\n",
    "\n",
    "    # per-frame errors\n",
    "    pf_err = {jn:[] for jn in ERR_JOINTS}\n",
    "    for frm in seg:\n",
    "        for jn in ERR_JOINTS:\n",
    "            ia,ib,ic = JOINT_TRIPLETS[jn]\n",
    "            ang = angle_between(frm[ia,:2],frm[ib,:2],frm[ic,:2])\n",
    "            pf_err[jn].append(ang - ideal_angles[ex].get(jn,ang))\n",
    "\n",
    "    # slide\n",
    "    for start in range(0, len(seg)-WINDOW+1, STRIDE):\n",
    "        w = np.array([ pf_err[jn][start:start+WINDOW] for jn in ERR_JOINTS ])  # (10,WINDOW)\n",
    "        mean_err = w.mean(axis=1)\n",
    "        row = {\n",
    "            \"video_id\":vid,\n",
    "            \"exercise_id\":ex,\n",
    "            \"repetition_number\":r.repetition_number,\n",
    "            \"window_start\": f0+start,\n",
    "            \"window_end\":   f0+start+WINDOW,\n",
    "            \"correctness\":  r.correctness\n",
    "        }\n",
    "        for i,jn in enumerate(ERR_JOINTS):\n",
    "            row[f\"err_{i}\"] = float(mean_err[i])\n",
    "        rows.append(row)\n",
    "\n",
    "win_df = pd.DataFrame(rows)\n",
    "win_df.to_csv(DATA_ROOT/\"Segmentation_windows.csv\", index=False)\n",
    "print(\"Wrote\", len(win_df), \"windows to Segmentation_windows.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d58dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/99z70cjs7t1dx16_0r1c33hw0000gn/T/ipykernel_25621/594032138.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  infer_model = torch.load(CKPT_FILE, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► Using device: mps\n",
      "Loading model from kp_pose_quality_windows_ex.pt...\n",
      "✅ Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745030852.291959  946755 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Max\n",
      "W0000 00:00:1745030852.353390 1014573 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745030852.375076 1014585 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Imports (assuming previous imports are still valid)\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "from collections import deque # Use deque for efficient buffer\n",
    "from pathlib import Path\n",
    "import pandas as pd # Needed for KeypointWindowDataset if re-defining\n",
    "import torch.nn as nn # Needed for model classes if re-defining\n",
    "\n",
    "# --- Re-include necessary definitions if running standalone ---\n",
    "# 1. Paths & device (adjust if needed)\n",
    "SCRIPT_DIR    = Path().resolve()\n",
    "DATA_ROOT     = SCRIPT_DIR/\"Data-REHAB24-6\" # Make sure this path is correct\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"► Using device:\", DEVICE)\n",
    "\n",
    "# 2. Joint names & count\n",
    "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
    "JOINT_NAMES = [lm.name for lm in PoseLandmark]\n",
    "N_JOINTS    = len(JOINT_NAMES)  # should be 33\n",
    "\n",
    "# 3. Exercises (Ex1…Ex6)\n",
    "NUM_EXERCISES = 6\n",
    "CKPT_FILE     = \"kp_pose_quality_windows_ex.pt\" # Check if this file exists\n",
    "\n",
    "# --- MODIFIED: Define the list of joints the error head predicts ---\n",
    "# Make sure this EXACT order matches how the model was trained (generate_windows.py)\n",
    "ERR_JOINTS   = [\n",
    "  \"LEFT_ELBOW\",\"RIGHT_ELBOW\",\n",
    "  \"LEFT_SHOULDER\",\"RIGHT_SHOULDER\",\n",
    "  \"LEFT_HIP\",\"RIGHT_HIP\",\n",
    "  \"LEFT_KNEE\",\"RIGHT_KNEE\",\n",
    "  \"SPINE\",\"HEAD\",\n",
    "]\n",
    "N_ERR = len(ERR_JOINTS)\n",
    "if N_ERR != 10:\n",
    "    print(f\"Warning: N_ERR is {N_ERR}, but expected 10 based on comment. Ensure ERR_JOINTS is correct.\")\n",
    "ERR_COLS = [f\"err_{i}\" for i in range(N_ERR)] # Used in training data prep, not directly here\n",
    "\n",
    "# 5. Model definitions (Using the ORIGINAL definition from training)\n",
    "class KeypointEncoder(nn.Module):\n",
    "    # --- Restored ORIGINAL definition ---\n",
    "    def __init__(self, in_dim:int, embed:int=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, embed, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, D); treat as (B, D, 1) for Conv1d\n",
    "        # This encoder is designed to process features of a SINGLE frame (B, D)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(2)                 # → (B, D, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.pool(x).squeeze(-1)    # → (B, embed)\n",
    "\n",
    "\n",
    "class PoseQualityNetKP(nn.Module):\n",
    "    # --- Keep the PoseQualityNetKP class definition as in the original code ---\n",
    "    def __init__(self,\n",
    "                 in_dim: int, # Should be 99 (33*3)\n",
    "                 num_ex: int,\n",
    "                 hidden: int = 256,\n",
    "                 ex_emb: int = 64,\n",
    "                 embed: int = 512): # Added embed dim to match encoder\n",
    "        super().__init__()\n",
    "        # keypoint feature extractor (Uses the restored original encoder)\n",
    "        self.encoder = KeypointEncoder(in_dim, embed=embed)\n",
    "\n",
    "        # sequence model\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed, # Use embed dim here\n",
    "            hidden_size=hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        feat_dim = hidden * 2\n",
    "\n",
    "        # exercise embedding MLP\n",
    "        self.ex_emb = nn.Sequential(\n",
    "            nn.Linear(num_ex, ex_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ex_emb, ex_emb)\n",
    "        )\n",
    "\n",
    "        # final heads\n",
    "        self.cls_head = nn.Linear(feat_dim + ex_emb, 2) # 2 classes: incorrect, correct\n",
    "        self.err_head = nn.Linear(feat_dim + ex_emb, N_ERR) # Predicts N_ERR error values\n",
    "\n",
    "    def forward(self,\n",
    "                seq:     torch.Tensor,  # (B, T, D) where D=99\n",
    "                ex_1hot: torch.Tensor   # (B, num_ex)\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # 1) keypoint → sequence feats\n",
    "        # encode each frame\n",
    "        B, T, D = seq.shape # Now this should work\n",
    "        # Process sequence frame by frame using the encoder\n",
    "        frame_embeddings = []\n",
    "        for t in range(T):\n",
    "            frame_data = seq[:, t, :] # Get data for frame t: (B, D)\n",
    "            frame_embedding = self.encoder(frame_data) # Output: (B, embed)\n",
    "            frame_embeddings.append(frame_embedding)\n",
    "\n",
    "        feats = torch.stack(frame_embeddings, dim=1) # (B, T, embed)\n",
    "\n",
    "        # 2) sequence model (LSTM)\n",
    "        out, _ = self.lstm(feats)                # (B, T, 2*hidden)\n",
    "        # Aggregate LSTM outputs (e.g., mean pooling over time)\n",
    "        g = out.mean(dim=1)                      # (B, 2*hidden)\n",
    "\n",
    "        # 3) exercise embed\n",
    "        ex_e = self.ex_emb(ex_1hot)              # (B, ex_emb)\n",
    "\n",
    "        # 4) concat and heads\n",
    "        h = torch.cat([g, ex_e], dim=1)          # (B, feat_dim + ex_emb)\n",
    "        logits = self.cls_head(h)                # (B, 2)\n",
    "        err_hat = self.err_head(h)               # (B, N_ERR)\n",
    "\n",
    "        return logits, err_hat\n",
    "# --- End of re-included definitions ---\n",
    "\n",
    "\n",
    "# Load model\n",
    "if not Path(CKPT_FILE).exists():\n",
    "    print(f\"Error: Checkpoint file not found at {CKPT_FILE}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading model from {CKPT_FILE}...\")\n",
    "infer_model = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "infer_model.eval()\n",
    "print(\"✅ Model loaded.\")\n",
    "\n",
    "# Exercise map\n",
    "EXERCISE_MAP = {\n",
    "    1: \"Arm abduction\",\n",
    "    2: \"Arm VW\",\n",
    "    3: \"Push-ups\",\n",
    "    4: \"Leg abduction\",\n",
    "    5: \"Leg lunge\",\n",
    "    6: \"Squats\"\n",
    "}\n",
    "NUM_EXERCISES = len(EXERCISE_MAP) # Ensure consistency\n",
    "\n",
    "# Ask user for exercise ID\n",
    "while True:\n",
    "    try:\n",
    "        exercise_id_str = input(f\"Enter the exercise ID you're performing (1-{len(EXERCISE_MAP)}): \")\n",
    "        exercise_id = int(exercise_id_str)\n",
    "        if 1 <= exercise_id <= len(EXERCISE_MAP):\n",
    "            exercise_name = EXERCISE_MAP[exercise_id]\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid ID. Please enter a number between 1 and {len(EXERCISE_MAP)}.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# MediaPipe Pose Setup\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2, # Match training\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Keypoints extraction function (using world landmarks)\n",
    "def extract_keypoints(frame):\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_rgb.flags.writeable = False\n",
    "    result = pose.process(img_rgb)\n",
    "    img_rgb.flags.writeable = True\n",
    "\n",
    "    keypoints = None\n",
    "    landmarks_for_drawing = None\n",
    "\n",
    "    if result.pose_world_landmarks:\n",
    "        world_landmarks = result.pose_world_landmarks.landmark\n",
    "        # Using world landmarks for model input - consistent with potential training setup\n",
    "        keypoints = np.array([(lm.x, lm.y, lm.z) for lm in world_landmarks], dtype=np.float32)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "         # Using image landmarks just for drawing\n",
    "         landmarks_for_drawing = result.pose_landmarks\n",
    "\n",
    "    return keypoints, landmarks_for_drawing\n",
    "\n",
    "# Inference parameters\n",
    "SEQUENCE_LENGTH = 16 # Match training window size\n",
    "IN_DIM = N_JOINTS * 3 # 33 * 3 = 99\n",
    "\n",
    "# --- MODIFIED Inference Function ---\n",
    "def infer_and_feedback(model, video_path, selected_ex_id, selected_ex_name):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    keypoints_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    feedback_status = \"Initializing...\" # Overall status (Correct/Incorrect/Collecting...)\n",
    "    joint_feedback = [] # <-- MODIFIED: Store specific joint feedback messages\n",
    "    predicted_class = -1 # -1: Initializing, 0: Incorrect, 1: Correct\n",
    "    err_values = np.zeros(N_ERR) # Store last raw error values for analysis\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or cannot read frame.\")\n",
    "            break\n",
    "\n",
    "        world_keypoints, image_landmarks_for_drawing = extract_keypoints(frame)\n",
    "\n",
    "        # Draw Image Landmarks if detected\n",
    "        if image_landmarks_for_drawing:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                image_landmarks_for_drawing,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # --- Model Inference Logic ---\n",
    "        if world_keypoints is not None:\n",
    "            keypoints_buffer.append(world_keypoints)\n",
    "\n",
    "            # Clear previous specific feedback before potentially generating new one\n",
    "            joint_feedback = []\n",
    "\n",
    "            if len(keypoints_buffer) == SEQUENCE_LENGTH:\n",
    "                # Prepare sequence\n",
    "                keypoints_array = np.array(keypoints_buffer, dtype=np.float32) # (16, 33, 3)\n",
    "                keypoints_flat = keypoints_array.reshape(SEQUENCE_LENGTH, -1) # (16, 99)\n",
    "                seq = torch.tensor(keypoints_flat, dtype=torch.float32).unsqueeze(0).to(DEVICE) # (1, 16, 99)\n",
    "\n",
    "                # Prepare exercise ID\n",
    "                ex_tensor = torch.tensor([selected_ex_id - 1], device=DEVICE)\n",
    "                ex_1hot = F.one_hot(ex_tensor, num_classes=NUM_EXERCISES).float() # (1, NUM_EXERCISES)\n",
    "\n",
    "                # --- Run Inference ---\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    logits, err_hat = model(seq, ex_1hot)\n",
    "                    # Class prediction (0: incorrect, 1: correct)\n",
    "                    predicted_class = logits.argmax(1).item()\n",
    "                    # Predicted errors (raw values from the model)\n",
    "                    err_values = err_hat.squeeze().cpu().numpy() # Shape: (N_ERR,)\n",
    "\n",
    "                # --- Generate Feedback based on prediction ---\n",
    "                if predicted_class == 1:\n",
    "                    feedback_status = \"Correct\"\n",
    "                    # No specific joint feedback needed if correct\n",
    "                else: # predicted_class == 0 (Incorrect)\n",
    "                    feedback_status = \"Incorrect\"\n",
    "                    # --- MODIFICATION START: Analyze errors ---\n",
    "                    # Calculate absolute errors to find the largest deviation\n",
    "                    abs_errors = np.abs(err_values)\n",
    "\n",
    "                    # Find the indices of the top N largest absolute errors\n",
    "                    # argsort gives indices from smallest to largest\n",
    "                    sorted_indices = np.argsort(abs_errors)\n",
    "\n",
    "                    # Get the indices of the top 3 largest errors (last 3 in sorted list)\n",
    "                    # You can change 3 to 1 if you only want the single most erroneous joint\n",
    "                    top_n = 3\n",
    "                    top_indices = sorted_indices[-top_n:]\n",
    "\n",
    "                    # Create feedback messages for the top errors\n",
    "                    for idx in reversed(top_indices): # Show most prominent error first\n",
    "                        # Ensure the index is valid (although it should be)\n",
    "                        if 0 <= idx < len(ERR_JOINTS):\n",
    "                           joint_name = ERR_JOINTS[idx]\n",
    "                           # Add the specific joint feedback message\n",
    "                           joint_feedback.append(f\"Correct your {joint_name}\")\n",
    "                        else:\n",
    "                            print(f\"Warning: Invalid error index {idx} encountered.\")\n",
    "                    # --- MODIFICATION END ---\n",
    "\n",
    "            else: # Buffer not full yet\n",
    "                feedback_status = f\"Collecting frames... {len(keypoints_buffer)}/{SEQUENCE_LENGTH}\"\n",
    "                predicted_class = -1 # Reset prediction state while collecting\n",
    "                # joint_feedback is already cleared at the start of the loop iteration\n",
    "\n",
    "        else: # No pose detected\n",
    "            feedback_status = \"No pose detected\"\n",
    "            keypoints_buffer.clear() # Clear buffer if pose is lost\n",
    "            predicted_class = -1 # Reset prediction state\n",
    "            joint_feedback = []  # Clear specific feedback\n",
    "            err_values = np.zeros(N_ERR) # Reset errors\n",
    "\n",
    "        # --- Display Feedback on Frame ---\n",
    "        # Display Exercise Name\n",
    "        cv2.putText(frame, f\"Exercise: {selected_ex_name}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display Overall Status (Correct/Incorrect/Collecting...)\n",
    "        status_color = (0, 255, 0) if predicted_class == 1 else ((0, 0, 255) if predicted_class == 0 else (255, 150, 0)) # Green/Red/Orange\n",
    "        cv2.putText(frame, f\"Status: {feedback_status}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # --- MODIFIED: Display Specific Joint Feedback if Incorrect ---\n",
    "        feedback_y_start = 90 # Starting Y position for joint feedback lines\n",
    "        # Only display if the class is incorrect AND there are feedback messages\n",
    "        if predicted_class == 0 and joint_feedback:\n",
    "            for i, msg in enumerate(joint_feedback):\n",
    "                # Display each specific correction suggestion\n",
    "                cv2.putText(frame, msg, (10, feedback_y_start + i * 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2) # Orange/Yellow text\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow('Pose Estimation Feedback', frame)\n",
    "\n",
    "        # Exit condition\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pose.close()\n",
    "\n",
    "\n",
    "# --- Main execution part ---\n",
    "# Path to the video file (Make sure this exists)\n",
    "# Example: Use Exercise 1 video for testing\n",
    "VIDEO_PATH = \"Data-REHAB24-6/Videos/Ex1/PM_001-Camera17-30fps.mp4\"\n",
    "# Or uncomment another exercise video\n",
    "# VIDEO_PATH = \"Data-REHAB24-6/Videos/Ex2/PM_003-Camera17-30fps.mp4\"\n",
    "\n",
    "if not Path(VIDEO_PATH).exists():\n",
    "     print(f\"Error: Video file not found at {VIDEO_PATH}\")\n",
    "else:\n",
    "    # Run the inference and feedback function\n",
    "    infer_and_feedback(infer_model, VIDEO_PATH, exercise_id, exercise_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rehabtrainingpy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
